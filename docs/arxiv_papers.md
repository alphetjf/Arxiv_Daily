# Arxiv Papers

| 标题  | 摘要 | 作者 | PDF链接 | 代码仓库 | Title | Abstract | 
|-------|---------|----------|-----------|------------------|--------------------|---------|
| 用于乳腺摄影的对比语言-图像预训练的多视角与多尺度对齐 | 对比语言-图像预训练（CLIP）在医学图像分析中显示出潜力，但需要大量数据和计算资源。由于这些限制，现有的CLIP在医学影像中的应用主要集中在胸部X光等拥有丰富图像报告数据的模态上，许多其他重要的模态尚未得到充分探索。在此，我们首次将完整的CLIP模型应用于乳腺摄影，这一领域由于标签数据稀缺、高分辨率图像中感兴趣区域小以及数据不平衡而面临显著挑战。我们首先为乳腺摄影开发了一个专门的监督框架，利用其多视角特性。此外，我们设计了一个对称局部对齐模块，以更好地关注高分辨率图像中的细节特征。最后，我们采用了一种参数高效的微调方法，用于预先训练了医学知识的大型语言模型，以应对数据限制。我们的多视角和多尺度对齐（MaMA）方法在两个大型真实世界乳腺摄影数据集EMBED和RSNA-Mammo上，针对三种不同任务的表现优于最先进的基线方法，且模型大小仅为最大基线的52%。 | Yuexi Du | [PDF](http://arxiv.org/pdf/2409.18119v1) | N/A | Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography | Contrastive Language-Image Pre-training (CLIP) shows promise in medical image analysis but requires substantial data and computational resources. Due to these restrictions, existing CLIP applications in medical imaging focus mainly on modalities like chest X-rays that have abundant image-report data available, leaving many other important modalities under-explored. Here, we propose the first adaptation of the full CLIP model to mammography, which presents significant challenges due to labeled data scarcity, high-resolution images with small regions of interest, and data imbalance. We first develop a specialized supervision framework for mammography that leverages its multi-view nature. Furthermore, we design a symmetric local alignment module to better focus on detailed features in high-resolution images. Lastly, we incorporate a parameter-efficient fine-tuning approach for large language models pre-trained with medical knowledge to address data limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms state-of-the-art baselines for three different tasks on two large real-world mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared with the largest baseline. |
| 寻找犀牛而不寻找犀牛：利用南非犀牛栖息地的多模态影像进行主动学习 | 地球上许多具有魅力的巨型动物正因人类活动而濒临灭绝，尤其是犀牛，由于非洲的盗猎危机，它们正面临灭绝的风险。监测犀牛的移动对其保护至关重要，但遗憾的是，由于犀牛行踪隐秘，这一任务一直难以实现。因此，我们提出了一种新颖的方法，即绘制群体排泄地点（称为粪堆）的地图，这些地点提供了关于犀牛空间行为的信息，对反盗猎、管理和重新引入工作具有重要价值。本文首次通过构建分类器，利用遥感的热成像、RGB和LiDAR图像，在被动和主动学习环境中检测并绘制了犀牛粪堆的位置。由于现有主动学习方法在我们的数据集中因极端类别不平衡而表现不佳，我们设计了MultimodAL，这是一个采用排序技术和多模态的主动学习系统，以实现与被动学习模型相竞争的性能，且仅需94%的标签。因此，我们的方法在处理类似规模的数据集时，可以节省超过76小时的标注时间。出乎意料的是，我们的粪堆地图揭示了犀牛粪堆并非随机分布在整个景观中，而是呈现出聚集分布。因此，护林员应集中在粪堆密度高的区域，以加强反盗猎工作，这与联合国目标15.7相一致。 | Lucia Gordon | [PDF](http://arxiv.org/pdf/2409.18104v1) | N/A | Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats | Much of Earth's charismatic megafauna is endangered by human activities, particularly the rhino, which is at risk of extinction due to the poaching crisis in Africa. Monitoring rhinos' movement is crucial to their protection but has unfortunately proven difficult because rhinos are elusive. Therefore, instead of tracking rhinos, we propose the novel approach of mapping communal defecation sites, called middens, which give information about rhinos' spatial behavior valuable to anti-poaching, management, and reintroduction efforts. This paper provides the first-ever mapping of rhino midden locations by building classifiers to detect them using remotely sensed thermal, RGB, and LiDAR imagery in passive and active learning settings. As existing active learning methods perform poorly due to the extreme class imbalance in our dataset, we design MultimodAL, an active learning system employing a ranking technique and multimodality to achieve competitive performance with passive learning models with 94% fewer labels. Our methods could therefore save over 76 hours in labeling time when used on a similarly-sized dataset. Unexpectedly, our midden map reveals that rhino middens are not randomly distributed throughout the landscape; rather, they are clustered. Consequently, rangers should be targeted at areas with high midden densities to strengthen anti-poaching efforts, in line with UN Target 15.7. |
| MALPOLON：深度物种分布建模框架 | 本文介绍了一个名为MALPOLON的深度物种分布模型（deep-SDM）框架。该框架采用Python编写，基于PyTorch库构建，旨在为仅具备一般Python语言技能的用户（如生态建模学者）提供便利，使他们能够轻松训练和推断深度物种分布模型，并进行分享。这些用户对利用深度学习方法构建新的物种分布模型感兴趣。此外，更高级的用户也可以利用该框架的模块化特性，通过重写现有类来运行更具体的实验，同时借助一键式示例，利用自定义或提供的原始及预处理数据集，在多个分类任务上训练神经网络。该框架已在GitHub和PyPi上开源，并附有详尽的文档和在各种场景下的使用示例。MALPOLON提供了简便的安装方式、基于YAML的配置、并行计算、多GPU利用、基准和基础模型用于性能评估，以及丰富的教程和文档，旨在提升生态学者和研究人员的可访问性和性能扩展性。 | Theo Larcher | [PDF](http://arxiv.org/pdf/2409.18102v1) | N/A | MALPOLON: A Framework for Deep Species Distribution Modeling | This paper describes a deep-SDM framework, MALPOLON. Written in Python and built upon the PyTorch library, this framework aims to facilitate training and inferences of deep species distribution models (deep-SDM) and sharing for users with only general Python language skills (e.g., modeling ecologists) who are interested in testing deep learning approaches to build new SDMs. More advanced users can also benefit from the framework's modularity to run more specific experiments by overriding existing classes while taking advantage of press-button examples to train neural networks on multiple classification tasks using custom or provided raw and pre-processed datasets. The framework is open-sourced on GitHub and PyPi along with extensive documentation and examples of use in various scenarios. MALPOLON offers straightforward installation, YAML-based configuration, parallel computing, multi-GPU utilization, baseline and foundational models for benchmarking, and extensive tutorials/documentation, aiming to enhance accessibility and performance scalability for ecologists and researchers. |
| 基于人工智能的增强现实技术在卫星装配、集成与测试中的应用 | 人工智能（AI）与增强现实（AR）的融合有望通过提高精度、减少人为错误以及在洁净室环境中提升操作效率，彻底改变卫星装配、集成和测试（AIT）流程。本文介绍了欧洲航天局（ESA）的“AI for AR in Satellite AIT”项目，该项目结合实时计算机视觉和AR系统，以协助技术人员进行卫星装配。利用微软HoloLens 2作为AR接口，该系统提供情境感知的指导和实时反馈，解决了AIT工作流程中物体识别和6D姿态估计的复杂性。所有AI模型的准确率均超过70%，其中检测模型的准确率超过95%，显示出高水平的性能和可靠性。本研究的一个重要贡献在于有效利用合成数据进行AR应用中的AI模型训练，解决了在高度动态的卫星环境中获取真实世界数据集的重大挑战，并创建了用于自动标注的分割一切模型（SAMAL），该模型实现了对真实数据的自动标注，速度比人工标注快20倍。研究结果表明，AI驱动的AR系统在自动化关键卫星装配任务方面的有效性，为航天工业未来的创新奠定了基础。 | Alvaro Patricio | [PDF](http://arxiv.org/pdf/2409.18101v1) | N/A | AI-Powered Augmented Reality for Satellite Assembly, Integration and Test | The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is set to transform satellite Assembly, Integration, and Testing (AIT) processes by enhancing precision, minimizing human error, and improving operational efficiency in cleanroom environments. This paper presents a technical description of the European Space Agency's (ESA) project "AI for AR in Satellite AIT," which combines real-time computer vision and AR systems to assist technicians during satellite assembly. Leveraging Microsoft HoloLens 2 as the AR interface, the system delivers context-aware instructions and real-time feedback, tackling the complexities of object recognition and 6D pose estimation in AIT workflows. All AI models demonstrated over 70% accuracy, with the detection model exceeding 95% accuracy, indicating a high level of performance and reliability. A key contribution of this work lies in the effective use of synthetic data for training AI models in AR applications, addressing the significant challenges of obtaining real-world datasets in highly dynamic satellite environments, as well as the creation of the Segmented Anything Model for Automatic Labelling (SAMAL), which facilitates the automatic annotation of real data, achieving speeds up to 20 times faster than manual human annotation. The findings demonstrate the efficacy of AI-driven AR systems in automating critical satellite assembly tasks, setting a foundation for future innovations in the space industry. |
| 自监督预训练用于心血管磁共振电影分割 | 自监督预训练（SSP）在从大量未标注数据集中学习方面显示出有希望的结果，因此可能对自动心血管磁共振（CMR）短轴电影分割有用。然而，关于SSP对分割益处的不一致报告使得将其应用于CMR变得困难。因此，本研究旨在评估SSP方法在CMR电影分割中的效果。为此，使用了296名受试者（90618张2D切片）的短轴电影堆栈进行未标注预训练，采用了四种SSP方法：SimCLR、位置对比学习、DINO和掩码图像建模（MIM）。对每种SSP方法，使用不同数量的受试者子集进行2D模型的有监督微调，并从头开始训练一个2D基线模型。微调后的模型与基线模型在140名受试者的测试数据集中使用3D Dice相似系数（DSC）进行比较。SSP方法在最大有监督微调子集上与基线相比没有性能提升（DSC = 0.89）。当仅有10名受试者（231张2D切片）可用于有监督训练时，使用MIM的SSP（DSC = 0.86）优于从头开始训练（DSC = 0.82）。本研究发现，当标注训练数据稀缺时，SSP对CMR电影分割具有价值，但在有充足标注数据时，对最先进的深度学习方法没有帮助。此外，SSP方法的选择很重要。代码公开在：https://github.com/q-cardIA/ssp-cmr-cine-segmentation。 | Rob A. J. de Mooij | [PDF](http://arxiv.org/pdf/2409.18100v1) | N/A | Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation | Self-supervised pretraining (SSP) has shown promising results in learning from large unlabeled datasets and, thus, could be useful for automated cardiovascular magnetic resonance (CMR) short-axis cine segmentation. However, inconsistent reports of the benefits of SSP for segmentation have made it difficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP methods for CMR cine segmentation.   To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were used for unlabeled pretraining with four SSP methods; SimCLR, positional contrastive learning, DINO, and masked image modeling (MIM). Subsets of varying numbers of subjects were used for supervised fine-tuning of 2D models for each SSP method, as well as to train a 2D baseline model from scratch. The fine-tuned models were compared to the baseline using the 3D Dice similarity coefficient (DSC) in a test dataset of 140 subjects.   The SSP methods showed no performance gains with the largest supervised fine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects (231 2D slices) are available for supervised training, SSP using MIM (DSC = 0.86) improves over training from scratch (DSC = 0.82).   This study found that SSP is valuable for CMR cine segmentation when labeled training data is scarce, but does not aid state-of-the-art deep learning methods when ample labeled data is available. Moreover, the choice of SSP method is important. The code is publicly available at: https://github.com/q-cardIA/ssp-cmr-cine-segmentation |
| EfficientCrackNet：一种用于裂缝分割的轻量级模型 | 裂缝检测，尤其是从路面图像中进行检测，由于存在强度不均匀、复杂拓扑结构、低对比度和噪声背景等固有复杂性，在计算机视觉领域构成了巨大的挑战。自动化裂缝检测对于维护建筑物、路面和桥梁等关键基础设施的结构完整性至关重要。现有的轻量级方法常常面临计算效率低下、裂缝模式复杂和背景困难等挑战，导致检测不准确且不适用于实际应用。为了解决这些限制，我们提出了EfficientCrackNet，这是一种轻量级混合模型，结合了卷积神经网络（CNN）和变压器（transformers），用于精确的裂缝分割。EfficientCrackNet集成了深度可分离卷积（DSC）层和MobileViT块，以捕捉全局和局部特征。该模型采用了边缘提取方法（EEM），无需预训练即可实现高效的裂缝边缘检测，并使用超轻量级子空间注意力模块（ULSAM）来增强特征提取。在三个基准数据集Crack500、DeepCrack和GAPs384上的广泛实验表明，EfficientCrackNet在性能上优于现有的轻量级模型，同时仅需0.26M参数和0.483 GFLOPs。所提出的模型在准确性和计算效率之间提供了最佳平衡，优于最先进的轻量级模型，并为实际裂缝分割提供了强大且适应性强的解决方案。 | Abid Hasan Zim | [PDF](http://arxiv.org/pdf/2409.18099v1) | N/A | EfficientCrackNet: A Lightweight Model for Crack Segmentation | Crack detection, particularly from pavement images, presents a formidable challenge in the domain of computer vision due to several inherent complexities such as intensity inhomogeneity, intricate topologies, low contrast, and noisy backgrounds. Automated crack detection is crucial for maintaining the structural integrity of essential infrastructures, including buildings, pavements, and bridges. Existing lightweight methods often face challenges including computational inefficiency, complex crack patterns, and difficult backgrounds, leading to inaccurate detection and impracticality for real-world applications. To address these limitations, we propose EfficientCrackNet, a lightweight hybrid model combining Convolutional Neural Networks (CNNs) and transformers for precise crack segmentation. EfficientCrackNet integrates depthwise separable convolutions (DSC) layers and MobileViT block to capture both global and local features. The model employs an Edge Extraction Method (EEM) and for efficient crack edge detection without pretraining, and Ultra-Lightweight Subspace Attention Module (ULSAM) to enhance feature extraction. Extensive experiments on three benchmark datasets Crack500, DeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superior performance compared to existing lightweight models, while requiring only 0.26M parameters, and 0.483 FLOPs (G). The proposed model offers an optimal balance between accuracy and computational efficiency, outperforming state-of-the-art lightweight models, and providing a robust and adaptable solution for real-world crack segmentation. |
| DiffSSC：使用去噪扩散概率模型实现语义激光雷达扫描完成 | 感知系统在自动驾驶中发挥着至关重要的作用，集成了多种传感器和相应的计算机视觉算法。三维激光雷达传感器广泛用于捕捉车辆周围环境的稀疏点云。然而，由于这些点云的稀疏性和缺乏语义信息，此类系统难以感知被遮挡区域和场景中的间隙。为了应对这些挑战，语义场景补全（SSC）结合了未观察到的几何形状和场景中的语义信息，旨在提供更完整的场景表示。基于扩散模型在图像生成和超分辨率任务中取得的显著成果，我们提出将其扩展到SSC任务中，通过分别在点和语义空间中实现噪声化和去噪扩散过程。为了控制生成过程，我们采用语义激光雷达点云作为条件输入，并设计局部和全局正则化损失以稳定去噪过程。我们在自动驾驶数据集上评估了我们的方法，结果表明我们的方法在SSC任务上优于当前最先进的技术。 | Helin Cao | [PDF](http://arxiv.org/pdf/2409.18092v1) | N/A | DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models | Perception systems play a crucial role in autonomous driving, incorporating multiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors are widely used to capture sparse point clouds of the vehicle's surroundings. However, such systems struggle to perceive occluded areas and gaps in the scene due to the sparsity of these point clouds and their lack of semantics. To address these challenges, Semantic Scene Completion (SSC) jointly predicts unobserved geometry and semantics in the scene given raw LiDAR measurements, aiming for a more complete scene representation. Building on promising results of diffusion models in image generation and super-resolution tasks, we propose their extension to SSC by implementing the noising and denoising diffusion processes in the point and semantic spaces individually. To control the generation, we employ semantic LiDAR point clouds as conditional input and design local and global regularization losses to stabilize the denoising process. We evaluate our approach on autonomous driving datasets and our approach outperforms the state-of-the-art for SSC. |
| GSON：一种基于群体的社会导航框架，采用大型多模态模型 | 随着以人为中心的环境中服务机器人和自动驾驶车辆的数量不断增加，它们的需求已不仅仅局限于导航至目的地。它们还必须考虑动态的社会情境，确保在共享空间中对他人表现出尊重和舒适，这对感知和规划提出了重大挑战。本文中，我们提出了一个基于群体的社会导航框架GSON，通过提升大型多模态模型（LMM）的视觉推理能力，使移动机器人能够感知并利用周围的社会群体。在感知方面，我们应用视觉提示技术，以零样本方式提取行人之间的社会关系，并将结果与鲁棒的行人检测和跟踪流程结合，以缓解LMM推理速度低的问题。在获得感知结果后，规划系统被设计为避免破坏当前的社会结构。我们采用基于社会结构的中间层规划器作为全局路径规划和局部运动规划之间的桥梁，以保持全局上下文和反应性响应。所提出的方法在涉及复杂社会结构理解和推理的真实世界移动机器人导航任务中得到了验证。实验结果表明，与多个基线相比，该系统在这些场景中的有效性。 | Shangyi Luo | [PDF](http://arxiv.org/pdf/2409.18084v1) | N/A | GSON: A Group-based Social Navigation Framework with Large Multimodal Model | As the number of service robots and autonomous vehicles in human-centered environments grows, their requirements go beyond simply navigating to a destination. They must also take into account dynamic social contexts and ensure respect and comfort for others in shared spaces, which poses significant challenges for perception and planning. In this paper, we present a group-based social navigation framework GSON to enable mobile robots to perceive and exploit the social group of their surroundings by leveling the visual reasoning capability of the Large Multimodal Model (LMM). For perception, we apply visual prompting techniques to zero-shot extract the social relationship among pedestrians and combine the result with a robust pedestrian detection and tracking pipeline to alleviate the problem of low inference speed of the LMM. Given the perception result, the planning system is designed to avoid disrupting the current social structure. We adopt a social structure-based mid-level planner as a bridge between global path planning and local motion planning to preserve the global context and reactive response. The proposed method is validated on real-world mobile robot navigation tasks involving complex social structure understanding and reasoning. Experimental results demonstrate the effectiveness of the system in these scenarios compared with several baselines. |
| SKT：将状态感知的关键点轨迹与视觉-语言模型结合，用于机器人服装操作 | 自动化处理服装对辅助机器人来说是一个重大挑战，因为服装具有多样性和可变形性。传统方法通常需要为每种服装类型单独建模，这限制了扩展性和适应性。相比之下，本文提出了一种利用视觉语言模型（VLM）的统一方法，以提高跨不同服装类别的关键点预测。通过解读视觉和语义信息，我们的模型使机器人能够使用单一模型管理不同的服装状态。我们利用先进的模拟技术创建了一个大规模的合成数据集，使得无需大量真实世界数据即可进行可扩展的训练。实验结果表明，基于VLM的方法显著提高了关键点检测的准确性和任务成功率，为机器人服装处理提供了更灵活和通用的解决方案。此外，这项研究还强调了VLM在单一框架内统一各种服装处理任务的潜力，为未来在家居自动化和辅助机器人领域的更广泛应用铺平了道路。 | Xin Li | [PDF](http://arxiv.org/pdf/2409.18082v1) | N/A | SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation | Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and adaptability. In contrast, this paper presents a unified approach using vision-language models (VLMs) to improve keypoint prediction across various garment categories. By interpreting both visual and semantic information, our model enables robots to manage different garment states with a single model. We created a large-scale synthetic dataset using advanced simulation techniques, allowing scalable training without extensive real-world data. Experimental results indicate that the VLM-based method significantly enhances keypoint detection accuracy and task success rates, providing a more flexible and general solution for robotic garment manipulation. In addition, this research also underscores the potential of VLMs to unify various garment manipulation tasks within a single framework, paving the way for broader applications in home automation and assistive robotics for future. |
| 在遵循自然语言指令之前推断人类的意图 | 为了让AI代理能够对人类有所帮助，它们应当能够遵循自然语言指令，在人类环境中完成日常的合作任务。然而，真实的人类指令本身就具有模糊性，因为说话者假设听者对其隐含的目标和意图有足够的先验知识。标准的语言理解和规划方法无法解决这种模糊性，因为它们没有将人类的内在目标建模为环境中的额外部分可观察因素。我们提出了一种新的框架，名为“遵循指令并进行社会和具身推理”（Follow Instructions with Social and Embodied Reasoning, FISER），旨在更好地遵循协作具身任务中的自然语言指令。我们的框架明确地将人类目标和意图作为中间推理步骤进行推断。我们实现了一系列基于Transformer的模型，并在一个具有挑战性的基准测试——HandMeThat上进行了评估。实证结果表明，在制定行动计划之前，使用社会推理来明确推断人类意图的方法优于纯粹的端到端方法。我们还将其与强大的基线方法进行了比较，包括在最大可用预训练语言模型上进行的“思维链”提示，发现FISER在所研究的具身社会推理任务中表现更佳，达到了HandMeThat上的最新技术水平。 | Yanming Wan | [PDF](http://arxiv.org/pdf/2409.18073v1) | N/A | Infer Human's Intentions Before Following Natural Language Instructions | For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-the-art on HandMeThat. |
