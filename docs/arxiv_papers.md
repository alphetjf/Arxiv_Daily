# Arxiv Papers

| 标题  | 摘要 | 作者 | PDF链接 | 代码仓库 | Title | Abstract | 
|-------|---------|----------|-----------|------------------|--------------------|---------|
| Molmo 和 PixMo：为最先进的跨模态模型提供开放权重和开放数据 | 当今最先进的多模态模型仍然是专有的。最强大的开源权重模型严重依赖于从专有的VLMs中提取的合成数据以实现良好性能，实际上是将这些封闭模型提炼为开源模型。因此，社区仍然缺乏关于如何从头构建高性能VLMs的基础知识。我们提出了Molmo，这是一个新的VLMs家族，在其开放性类别中处于最先进水平。我们的关键创新是一个全新、高度详细的全人类标注者基于语音描述收集的图像描述数据集。为了支持广泛的用户交互，我们还引入了一个多样化的微调数据集混合，包括自然情境下的问答和创新的二维指向数据。我们方法的成功依赖于对模型架构细节的精心选择、精心调优的训练流程，以及最关键的，我们新收集数据集的质量，所有这些都将被发布。Molmo家族中最好的72B模型不仅在开源权重和数据模型类别中表现优于其他模型，而且在学术基准和人类评估中与GPT-4o、Claude 3.5和Gemini 1.5等专有系统相比也表现出色。我们将在不久的将来发布所有模型权重、标注和微调数据以及源代码。部分模型权重、推理代码和演示可在https://molmo.allenai.org获取。 | Matt Deitke | [PDF](http://arxiv.org/pdf/2409.17146v1) | N/A | Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models | Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.   We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org. |
| DreamWaltz-G：从骨架引导的2D扩散中生成富有表现力的3D高斯头像 | 利用预训练的二维扩散模型和分数蒸馏采样（SDS），最近的方法在文本到三维头像生成方面展示了有前景的结果。然而，生成能够进行表达性动画的高质量三维头像仍然具有挑战性。在这项工作中，我们提出了DreamWaltz-G，一种从文本生成可动画三维头像的新型学习框架。该框架的核心在于骨架引导的分数蒸馏和混合三维高斯头像表示。具体而言，所提出的骨架引导分数蒸馏将三维人体模板中的骨架控制融入二维扩散模型，增强了SDS监督在视角和人体姿态方面的一致性。这有助于生成高质量的头像，缓解了多面、多余肢体和模糊等问题。所提出的混合三维高斯头像表示基于高效的三维高斯分布，结合了神经隐式场和参数化三维网格，实现了实时渲染、稳定的SDS优化和表达性动画。广泛的实验表明，DreamWaltz-G在生成和动画三维头像方面非常有效，在视觉质量和动画表达性方面均优于现有方法。我们的框架还支持多种应用，包括人体视频重演和多主体场景合成。 | Yukun Huang | [PDF](http://arxiv.org/pdf/2409.17145v1) | N/A | DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion | Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition. |
| 差分隐私正则化：通过损失函数正则化保护训练数据 | 基于神经网络的机器学习模型训练需要大量数据集，这些数据集可能包含敏感信息。然而，这些模型不应暴露这些数据集中的私人信息。差分隐私随机梯度下降（DP-SGD）要求对标准随机梯度下降（SGD）算法进行修改，以训练新模型。在这篇短文中，提出了一种新颖的正则化策略，以更高效的方式实现相同的目标。 | Francisco Aguilera-Martínez | [PDF](http://arxiv.org/pdf/2409.17144v1) | N/A | Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization | Training machine learning models based on neural networks requires large datasets, which may contain sensitive information. The models, however, should not expose private information from these datasets. Differentially private SGD [DP-SGD] requires the modification of the standard stochastic gradient descent [SGD] algorithm for training new models. In this short paper, a novel regularization strategy is proposed to achieve the same goal in a more efficient manner. |
| 图像上注意力提示用于大型视觉-语言模型 | 与大型语言模型（LLMs）相比，大型视觉-语言模型（LVLMs）同样能够接受图像作为输入，因此展现出更多有趣的涌现能力，并在各种视觉-语言任务中表现出令人印象深刻的性能。受到LLMs中文本提示的启发，视觉提示已被探索用于增强LVLMs感知视觉信息的能力。然而，先前的视觉提示技术仅处理视觉输入，而未考虑文本查询，这限制了模型根据文本指令完成任务的能力。为了填补这一空白，我们在本文中提出了一种新的提示技术，名为图像上的注意力提示（Attention Prompting on Image），它仅通过在原始输入图像上叠加一个文本查询引导的注意力热图，就能有效提升LVLM在各种任务中的表现。具体而言，我们使用类似CLIP的辅助模型为输入图像生成一个依赖于文本查询的注意力热图。然后，该热图简单地与原始图像的像素值相乘，以获得LVLM的实际输入图像。在各种视觉-语言基准上的广泛实验验证了我们技术的有效性。例如，图像上的注意力提示在MM-Vet和LLaVA-Wild基准上分别将LLaVA-1.5的性能提升了3.8%和2.9%。 | Runpeng Yu | [PDF](http://arxiv.org/pdf/2409.17143v1) | N/A | Attention Prompting on Image for Large Vision-Language Models | Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs' capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models' ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose a new prompting technique named Attention Prompting on Image, which just simply overlays a text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, Attention Prompting on Image improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively. |
| FineZip：推动大型语言模型在实际无损文本压缩中的极限 | 尽管语言建模目标已被证明与压缩密切相关，但令人惊讶的是，现代大型语言模型（LLMs）并未被应用于实际的文本压缩系统中。本文深入分析了基于神经网络和变压器（transformer）的压缩技术，以解答这一问题。我们对比了传统的文本压缩系统与基于神经网络和LLM的文本压缩方法。尽管基于LLM的系统在性能上显著优于传统压缩方法，但它们的实用性极低。具体来说，最近的一个基于Llama3-8B的文本压缩系统LLMZip，尽管在压缩比上有了巨大提升，但压缩10 MB的文本需要9.5天的时间。为了克服这一问题，我们提出了FineZip——一种新型基于LLM的文本压缩系统，结合了在线记忆和动态上下文的思想，极大地减少了压缩时间。与LLMZip的9.5天相比，FineZip可以在大约4小时内完成上述语料的压缩，性能提升54倍且表现相当。FineZip在压缩比上大幅超越传统的算法压缩方法，提升了约50%。通过这项工作，我们迈出了使基于LLM的无损文本压缩成为现实的第一步。尽管FineZip在这方面迈出了重要一步，但LLMs仍未成为大规模文本压缩的可行解决方案。我们希望我们的工作能为未来的研究和创新铺平道路，以解决这一问题。 | Fazal Mittu | [PDF](http://arxiv.org/pdf/2409.17141v1) | N/A | FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression | While the language modeling objective has been shown to be deeply connected with compression, it is surprising that modern LLMs are not employed in practical text compression systems. In this paper, we provide an in-depth analysis of neural network and transformer-based compression techniques to answer this question. We compare traditional text compression systems with neural network and LLM-based text compression methods. Although LLM-based systems significantly outperform conventional compression methods, they are highly impractical. Specifically, LLMZip, a recent text compression system using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with huge improvements in compression ratios. To overcome this, we present FineZip - a novel LLM-based text compression system that combines ideas of online memorization and dynamic context to reduce the compression time immensely. FineZip can compress the above corpus in approximately 4 hours compared to 9.5 days, a 54 times improvement over LLMZip and comparable performance. FineZip outperforms traditional algorithmic compression methods with a large margin, improving compression ratios by approximately 50\%. With this work, we take the first step towards making lossless text compression with LLMs a reality. While FineZip presents a significant step in that direction, LLMs are still not a viable solution for large-scale text compression. We hope our work paves the way for future research and innovation to solve this problem. |
| 将每个应用程序转变为智能代理：基于API优先的大型语言模型代理，助力高效的人机交互 | 多模态大型语言模型（MLLMs）使得基于LLM的代理能够直接与应用程序用户界面（UI）进行交互，从而在复杂任务中提升代理的性能。然而，这些代理由于广泛的顺序UI交互，常常面临高延迟和低可靠性的问题。为解决这一问题，我们提出了AXIS，这是一种新颖的基于LLM的代理框架，优先通过应用程序编程接口（APIs）而非UI动作来执行操作。该框架还通过自动探索应用程序来促进API的创建和扩展。我们在Office Word上的实验表明，与人类相比，AXIS将任务完成时间减少了65%-70%，认知负荷降低了38%-53%，同时保持了97%-98%的准确性。我们的工作为新的人-代理-计算机交互（HACI）框架和LLM时代应用程序提供商的全新UI设计原则做出了贡献。此外，它还探索了将每个应用程序转变为代理的可能性，为迈向以代理为中心的操作系统（Agent OS）铺平了道路。 | Junting Lu | [PDF](http://arxiv.org/pdf/2409.17140v1) | N/A | Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents | Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low reliability due to the extensive sequential UI interactions. To address this issue, we propose AXIS, a novel LLM-based agents framework prioritize actions through application programming interfaces (APIs) over UI actions. This framework also facilitates the creation and expansion of APIs through automated exploration of applications. Our experiments on Office Word demonstrate that AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans. Our work contributes to a new human-agent-computer interaction (HACI) framework and a fresh UI design principle for application providers in the era of LLMs. It also explores the possibility of turning every applications into agents, paving the way towards an agent-centric operating system (Agent OS). |
| 动态学习：基于动态无人机团队的无人机通信网络自主调控 | 无人机（UAV）通信网络（UCN）是未来移动网络的关键组成部分。为了应对UCN中的动态环境，强化学习（RL）因其无需环境模型即可进行自适应决策的强大能力，已成为一种有前景的解决方案。然而，大多数现有的基于RL的研究集中在控制策略设计，假设UAV集合是固定的。很少有研究探讨当服务的UAV动态变化时，UCN应如何自适应调节。本文讨论了在动态UAV集合下基于RL的策略设计，以实现UCN的自适应调节，涵盖了一般UCN中的反应性策略和太阳能UCN中的前瞻性策略。首先提供了UCN和RL框架的概述。接着详细阐述了潜在的研究方向及其关键挑战和可能的解决方案。最后，我们展示了一些最近的成果作为案例研究，以启发使用不同RL算法处理动态UAV团队的创新方法。 | Ran Zhang | [PDF](http://arxiv.org/pdf/2409.17139v1) | N/A | Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew | Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key component in future mobile networking. To handle the dynamic environments in UCNs, reinforcement learning (RL) has been a promising solution attributed to its strong capability of adaptive decision-making free of the environment models. However, most existing RL-based research focus on control strategy design assuming a fixed set of UAVs. Few works have investigated how UCNs should be adaptively regulated when the serving UAVs change dynamically. This article discusses RL-based strategy design for adaptive UCN regulation given a dynamic UAV set, addressing both reactive strategies in general UCNs and proactive strategies in solar-powered UCNs. An overview of the UCN and the RL framework is first provided. Potential research directions with key challenges and possible solutions are then elaborated. Some of our recent works are presented as case studies to inspire innovative ways to handle dynamic UAV crew with different RL algorithms. |
| 有限时间马尔可夫决策过程（MDPs）中具有一般状态和动作的政策优化景观 | 策略梯度方法在强化学习中得到了广泛应用。然而，策略优化的非凸性给理解策略梯度方法的全局收敛性带来了重大挑战。对于一类具有一般状态和动作空间的有限时域马尔可夫决策过程（MDPs），我们开发了一个框架，该框架提供了一组易于验证的假设，以确保策略优化的Kurdyka-Lojasiewicz（KL）条件。利用KL条件，策略梯度方法尽管在非凸性条件下，仍能以非渐近速率收敛到全局最优策略。我们的结果在各种控制和运营模型中得到了应用，包括熵正则化的表格MDPs、线性二次调节器（LQR）问题、随机库存模型和随机现金余额问题，我们展示了通过随机策略梯度方法，可以在$\tilde{\mathcal{O}}(\epsilon^{-1})$的样本量和关于规划时域的多项式时间内获得一个$\epsilon$-最优策略。我们的结果在文献中首次确立了具有马尔可夫调制需求的多元库存系统和随机现金余额问题的样本复杂性。 | Xin Chen | [PDF](http://arxiv.org/pdf/2409.17138v1) | N/A | Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action | Policy gradient methods are widely used in reinforcement learning. Yet, the nonconvexity of policy optimization imposes significant challenges in understanding the global convergence of policy gradient methods. For a class of finite-horizon Markov Decision Processes (MDPs) with general state and action spaces, we develop a framework that provides a set of easily verifiable assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy optimization. Leveraging the KL condition, policy gradient methods converge to the globally optimal policy with a non-asymptomatic rate despite nonconvexity. Our results find applications in various control and operations models, including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR) problems, stochastic inventory models, and stochastic cash balance problems, for which we show an $\epsilon$-optimal policy can be obtained using a sample size in $\tilde{\mathcal{O}}(\epsilon^{-1})$ and polynomial in terms of the planning horizon by stochastic policy gradient methods. Our result establishes the first sample complexity for multi-period inventory systems with Markov-modulated demands and stochastic cash balance problems in the literature. |
| PACE：将参数高效微调中的泛化与一致性正则化相结合 | 参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）有效地将预训练的视觉变换器适应于下游任务。然而，为了提升任务性能的优化往往以微调模型泛化能力的牺牲为代价。为了解决这一问题，我们从理论上将训练过程中较小的权重梯度范数和较大的数据集与改进的模型泛化能力联系起来。受此联系启发，我们提出通过减少梯度范数来增强泛化能力，并使微调模型与预训练模型对齐，以保留大规模预训练数据中的知识。然而，简单的对齐并不能保证梯度减少，反而可能导致梯度爆炸，增加了管理梯度的复杂性。为了解决这些问题，我们提出了PACE，将参数高效微调的泛化能力与一致性正则化相结合。我们通过乘性噪声扰动从适配器中学习到的特征，并确保微调模型在不同扰动下对同一样本保持一致性。理论分析表明，PACE不仅隐式地对梯度进行正则化以增强泛化能力，还隐式地对齐微调和预训练模型以保留知识。实验证据支持了我们的理论。在四个视觉适应任务（VTAB-1k、FGVC、少样本学习和领域适应）中，PACE优于现有的PEFT方法。代码将在https://github.com/MaxwellYaoNi/PACE 提供。 | Yao Ni | [PDF](http://arxiv.org/pdf/2409.17137v1) | N/A | PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization | Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision transformers to downstream tasks. However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improved model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE outperforms existing PEFT methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and domain adaptation. Code will be available at https://github.com/MaxwellYaoNi/PACE |
| Blox-Net：利用VLM监督、物理模拟和具备重置功能的机器人进行机器人装配的生成式设计 | 生成式AI系统在生成文本、代码和图像方面展示了令人印象深刻的能力。受工业领域“面向装配的设计”研究丰富历史的启发，我们提出了一项新问题：面向机器人装配的生成式设计（Generative Design-for-Robot-Assembly，简称GDfRA）。该任务基于自然语言提示（例如“长颈鹿”）和可用物理组件的图像（如3D打印块）生成一个装配体。输出结果包括这些组件的空间排列以及机器人构建该装配体的指令。输出必须满足两个条件：1）类似于请求的对象；2）能够被带有吸盘夹具的6自由度机器人手臂可靠地装配。随后，我们介绍了Blox-Net，这是一个GDfRA系统，它结合了生成式视觉语言模型与计算机视觉、模拟、扰动分析、运动规划和物理机器人实验中已有的成熟方法，以解决一类GDfRA问题，且仅需最少的人工监督。Blox-Net在其设计的装配体的“可识别性”方面达到了63.5%的Top-1准确率（例如，由视觉语言模型判断其类似于长颈鹿）。经过自动扰动重新设计后，这些设计能够被机器人可靠地装配，在连续10次装配迭代中几乎完美地成功，仅在装配前重置时需要人工干预。令人惊讶的是，从文本词（“长颈鹿”）到可靠的物理装配的整个设计过程完全无需人工干预。 | Andrew Goldberg | [PDF](http://arxiv.org/pdf/2409.17126v1) | N/A | Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset | Generative AI systems have shown impressive capabilities in creating text, code, and images. Inspired by the rich history of research in industrial ''Design for Assembly'', we introduce a novel problem: Generative Design-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on a natural language prompt (e.g., ''giraffe'') and an image of available physical components, such as 3D-printed blocks. The output is an assembly, a spatial arrangement of these components, and instructions for a robot to build this assembly. The output must 1) resemble the requested object and 2) be reliably assembled by a 6 DoF robot arm with a suction gripper. We then present Blox-Net, a GDfRA system that combines generative vision language models with well-established methods in computer vision, simulation, perturbation analysis, motion planning, and physical robot experimentation to solve a class of GDfRA problems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of 63.5% in the ''recognizability'' of its designed assemblies (eg, resembling giraffe as judged by a VLM). These designs, after automated perturbation redesign, were reliably assembled by a robot, achieving near-perfect success across 10 consecutive assembly iterations with human intervention only during reset prior to assembly. Surprisingly, this entire design process from textual word (''giraffe'') to reliable physical assembly is performed with zero human intervention. |
