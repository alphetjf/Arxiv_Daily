
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.39">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-papers">Arxiv Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>PhysGen：基于刚体物理的图像到视频生成</td>
<td>我们提出了PhysGen，这是一种新颖的图像到视频生成方法，它将单张图像和输入条件（例如，施加在图像中物体上的力和扭矩）转换为生成逼真、物理合理且时间上一致的视频。我们的关键洞察是将基于模型的物理模拟与数据驱动的视频生成过程相结合，从而实现可信的图像空间动力学。我们系统的心脏是三个核心组件：（i）一个图像理解模块，能够有效捕捉图像的几何形状、材料和物理参数；（ii）一个图像空间动力学模拟模型，利用刚体物理学和推断的参数来模拟真实行为；（iii）一个基于图像的渲染和细化模块，利用生成视频扩散来生成包含模拟运动的逼真视频片段。生成的视频在物理和外观上都十分逼真，甚至可以精确控制，通过定量比较和全面的用户研究，展示了优于现有数据驱动图像到视频生成工作的卓越结果。PhysGen生成的视频可用于各种下游应用，例如将图像转化为逼真动画，或允许用户与图像互动并创建各种动态效果。项目页面：https://stevenlsw.github.io/physgen/</td>
<td>Shaowei Liu</td>
<td><a href="http://arxiv.org/pdf/2409.18964v1">PDF</a></td>
<td>N/A</td>
<td>PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation</td>
<td>We present PhysGen, a novel image-to-video generation method that converts a single image and an input condition (e.g., force and torque applied to an object in the image) to produce a realistic, physically plausible, and temporally consistent video. Our key insight is to integrate model-based physical simulation with a data-driven video generation process, enabling plausible image-space dynamics. At the heart of our system are three core components: (i) an image understanding module that effectively captures the geometry, materials, and physical parameters of the image; (ii) an image-space dynamics simulation model that utilizes rigid-body physics and inferred parameters to simulate realistic behaviors; and (iii) an image-based rendering and refinement module that leverages generative video diffusion to produce realistic video footage featuring the simulated motion. The resulting videos are realistic in both physics and appearance and are even precisely controllable, showcasing superior results over existing data-driven image-to-video generation works through quantitative comparison and comprehensive user study. PhysGen's resulting videos can be used for various downstream applications, such as turning an image into a realistic animation or allowing users to interact with the image and create various dynamics. Project page: https://stevenlsw.github.io/physgen/</td>
</tr>
<tr>
<td>探索视觉状态空间模型中的令牌剪枝技术</td>
<td>状态空间模型（SSMs）相较于Transformer中的注意力模块，具有保持线性计算复杂度的优势，并已被应用于视觉任务，成为一种新型强大的视觉基础模型。受到视觉Transformer（ViTs）中最终预测仅基于最具信息量的子集的观察启发，我们采取了一项创新步骤，通过基于标记的剪枝来提高基于SSM的视觉模型的效率。然而，直接应用为ViTs设计的现有标记剪枝技术无法带来良好的性能，即使经过广泛的微调。为了解决这一问题，我们重新审视了SSMs独特的计算特性，并发现简单应用会破坏标记的顺序位置。这一洞察促使我们为基于SSM的视觉模型设计一种新颖且通用的标记剪枝方法。我们首先引入了一种剪枝感知隐藏状态对齐方法，以稳定剩余标记的邻域，从而提升性能。此外，基于我们的详细分析，我们提出了一种适应SSM模型的标记重要性评估方法，以指导标记剪枝。通过高效的实现和实际的加速方法，我们的方法带来了实际的加速效果。广泛的实验表明，我们的方法能够在不同任务中显著减少计算量，同时对性能的影响最小。值得注意的是，我们在ImageNet上实现了81.7%的准确率，同时对剪枝后的PlainMamba-L3模型减少了41.6%的FLOPs。此外，我们的工作为理解基于SSM的视觉模型的行为提供了更深入的见解，为未来的研究奠定了基础。</td>
<td>Zheng Zhan</td>
<td><a href="http://arxiv.org/pdf/2409.18962v1">PDF</a></td>
<td>N/A</td>
<td>Exploring Token Pruning in Vision State Space Models</td>
<td>State Space Models (SSMs) have the advantage of keeping linear computational complexity compared to attention modules in transformers, and have been applied to vision tasks as a new type of powerful vision foundation model. Inspired by the observations that the final prediction in vision transformers (ViTs) is only based on a subset of most informative tokens, we take the novel step of enhancing the efficiency of SSM-based vision models through token-based pruning. However, direct applications of existing token pruning techniques designed for ViTs fail to deliver good performance, even with extensive fine-tuning. To address this issue, we revisit the unique computational characteristics of SSMs and discover that naive application disrupts the sequential token positions. This insight motivates us to design a novel and general token pruning method specifically for SSM-based vision models. We first introduce a pruning-aware hidden state alignment method to stabilize the neighborhood of remaining tokens for performance enhancement. Besides, based on our detailed analysis, we propose a token importance evaluation method adapted for SSM models, to guide the token pruning. With efficient implementation and practical acceleration methods, our method brings actual speedup. Extensive experiments demonstrate that our approach can achieve significant computation reduction with minimal impact on performance across different tasks. Notably, we achieve 81.7\% accuracy on ImageNet with a 41.6\% reduction in the FLOPs for pruned PlainMamba-L3. Furthermore, our work provides deeper insights into understanding the behavior of SSM-based vision models for future research.</td>
</tr>
<tr>
<td>ProMerge: 无监督实例分割的提示与合并</td>
<td>无监督实例分割旨在无需依赖人工标注数据的情况下，对图像中的不同对象实例进行分割。这一领域近期取得了显著进展，部分原因在于自监督模型（如DINO）提供的丰富视觉特征表示所带来的强大局部对应关系。最新的最先进方法利用自监督特征将图像表示为图，并通过求解广义特征值系统（即归一化分割）来生成前景掩码。尽管这种方法有效，但其伴随的计算需求导致了较慢的推理速度。在本文中，我们提出了Prompt and Merge（ProMerge），该方法利用自监督视觉特征获取初始的补丁分组，并通过一种策略性的合并方法对这些分段进行处理，同时借助一种复杂的基于背景的掩码修剪技术。ProMerge不仅产生了具有竞争力的结果，而且在推理时间上相比基于归一化分割的最先进方法显著减少。此外，当使用我们的掩码预测作为伪标签训练目标检测器时，所得到的目标检测器在各种具有挑战性的实例分割基准测试中超越了当前领先的非监督模型。</td>
<td>Dylan Li</td>
<td><a href="http://arxiv.org/pdf/2409.18961v1">PDF</a></td>
<td>N/A</td>
<td>ProMerge: Prompt and Merge for Unsupervised Instance Segmentation</td>
<td>Unsupervised instance segmentation aims to segment distinct object instances in an image without relying on human-labeled data. This field has recently seen significant advancements, partly due to the strong local correspondences afforded by rich visual feature representations from self-supervised models (e.g., DINO). Recent state-of-the-art approaches use self-supervised features to represent images as graphs and solve a generalized eigenvalue system (i.e., normalized-cut) to generate foreground masks. While effective, this strategy is limited by its attendant computational demands, leading to slow inference speeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages self-supervised visual features to obtain initial groupings of patches and applies a strategic merging to these segments, aided by a sophisticated background-based mask pruning technique. ProMerge not only yields competitive results but also offers a significant reduction in inference time compared to state-of-the-art normalized-cut-based approaches. Furthermore, when training an object detector using our mask predictions as pseudo-labels, the resulting detector surpasses the current leading unsupervised model on various challenging instance segmentation benchmarks.</td>
</tr>
<tr>
<td>在最小假设下，扩散概率模型的$O(d/T)$收敛理论</td>
<td>基于分数的扩散模型通过学习逆转一个扩散过程来生成新数据，该过程将来自目标分布的数据扰动为噪声，在各种生成任务中取得了显著的成功。尽管它们在经验性能上表现优越，但现有的理论保证往往受到严格假设或次优收敛率的限制。在本文中，我们在最小的假设下为一种流行的基于随机微分方程（SDE）的采样器建立了快速收敛理论。我们的分析表明，在提供$\ell_{2}$准确估计的分数函数的情况下，目标分布与生成分布之间的总变差距离被上界为$O(d/T)$（忽略对数因子），其中$d$是数据维度，$T$是步数。这一结果适用于任何具有有限一阶矩的目标分布。据我们所知，这不仅改进了基于SDE的采样器的现有收敛理论，还改进了另一种基于常微分方程（ODE）的采样器的收敛理论，同时对目标数据分布和分数估计施加了最小的假设。这一成果是通过一组新颖的分析工具实现的，这些工具提供了对反向过程中每一步误差传播的精细描述。</td>
<td>Gen Li</td>
<td><a href="http://arxiv.org/pdf/2409.18959v1">PDF</a></td>
<td>N/A</td>
<td>$O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions</td>
<td>Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical guarantees are often constrained by stringent assumptions or suboptimal convergence rates. In this paper, we establish a fast convergence theory for a popular SDE-based sampler under minimal assumptions. Our analysis shows that, provided $\ell_{2}$-accurate estimates of the score functions, the total variation distance between the target and generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionality and $T$ is the number of steps. This result holds for any target distribution with finite first-order moment. To our knowledge, this improves upon existing convergence theory for both the SDE-based sampler and another ODE-based sampler, while imposing minimal assumptions on the target data distribution and score estimates. This is achieved through a novel set of analytical tools that provides a fine-grained characterization of how the error propagates at each step of the reverse process.</td>
</tr>
<tr>
<td>LML：语言模型学习数据集以进行数据增强预测</td>
<td>本文介绍了一种利用大型语言模型（LLMs）进行分类任务的新方法，这些任务通常由机器学习（ML）模型处理。与依赖大量数据清洗和特征工程的ML模型不同，这种方法通过LLMs简化了流程。本文提出了一个名为“语言模型学习（LML）”的新概念，该概念由一种名为“数据增强预测（DAP）”的新方法驱动。分类过程通过LLMs进行，类似于人类手动探索和理解数据，并使用数据作为参考来决定分类。训练数据被总结和评估，以确定导致每个标签分类的最显著特征。在DAP过程中，系统使用数据摘要自动创建查询，用于从数据集中检索相关行。LLM使用数据摘要和相关行生成分类，即使在处理复杂数据时也能确保令人满意的准确性。DAP中使用数据摘要和相似数据确保了上下文感知的决策制定。所提出的方法在提示中使用“充当可解释的机器学习模型”的词语，以增强预测的可解释性，允许用户审查每个预测背后的逻辑。在一些测试案例中，系统的准确率超过了90%，证明了系统的有效性及其在各种场景中超越传统ML模型的潜力。代码可在https://github.com/Pro-GenAI/LML-DAP获取。</td>
<td>Praneeth Vadlapati</td>
<td><a href="http://arxiv.org/pdf/2409.18957v1">PDF</a></td>
<td>N/A</td>
<td>LML: Language Model Learning a Dataset for Data-Augmented Prediction</td>
<td>This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called "Language Model Learning (LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words "Act as an Explainable Machine Learning Model" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP</td>
</tr>
<tr>
<td>RepairBench：程序修复前沿模型的排行榜</td>
<td>AI驱动的程序修复利用AI模型通过生成补丁来修复有缺陷的软件。AI技术的迅速进步无疑会影响程序修复的最先进性能。然而，要把握这一进展，需要频繁且标准化的评估。我们提出了RepairBench，这是一个用于AI驱动程序修复的新型排行榜。RepairBench的关键特点是：1) 基于执行：所有补丁都会针对测试套件进行编译和执行；2) 它以频繁且标准化的方式评估前沿模型。RepairBench利用两个高质量的基准数据集——Defects4J和GitBug-Java，来评估前沿模型在真实世界程序修复任务中的表现。我们公开发布了RepairBench的评估框架。随着新前沿模型的发布，我们将更新排行榜。</td>
<td>André Silva</td>
<td><a href="http://arxiv.org/pdf/2409.18952v1">PDF</a></td>
<td>N/A</td>
<td>RepairBench: Leaderboard of Frontier Models for Program Repair</td>
<td>AI-driven program repair uses AI models to repair buggy software by producing patches. Rapid advancements in AI surely impact state-of-the-art performance of program repair. Yet, grasping this progress requires frequent and standardized evaluations. We propose RepairBench, a novel leaderboard for AI-driven program repair. The key characteristics of RepairBench are: 1) it is execution-based: all patches are compiled and executed against a test suite, 2) it assesses frontier models in a frequent and standardized way. RepairBench leverages two high-quality benchmarks, Defects4J and GitBug-Java, to evaluate frontier models against real-world program repair tasks. We publicly release the evaluation framework of RepairBench. We will update the leaderboard as new frontier models are released.</td>
</tr>
<tr>
<td>光谱小波丢弃：小波域中的正则化</td>
<td>正则化技术有助于防止过拟合，从而提高卷积神经网络（CNN）的泛化能力。过拟合的一个原因是网络不同部分之间复杂的协同适应，这使得CNN依赖于它们的联合响应，而不是鼓励每个部分独立学习有用的特征表示。频域操作是一种强大的策略，通过利用频率分解来修改具有时间和空间一致性的数据。本研究引入了谱小波丢弃（Spectral Wavelet Dropout, SWD），这是一种新颖的正则化方法，包括两种变体：1D-SWD和2D-SWD。这些变体通过随机丢弃特征图离散小波分解中的细节频率带，来提高CNN的泛化能力。我们的方法与现有的谱“傅里叶”丢弃（Spectral "Fourier" Dropout, 2D-SFD）有所不同，后者是在傅里叶域中消除系数。值得注意的是，SWD仅需要一个超参数，而SFD则需要两个。我们还通过实现一维版本的谱“傅里叶”丢弃（1D-SFD）来扩展文献，为全面比较奠定了基础。我们的评估显示，在CIFAR-10/100基准测试中，1D和2D SWD变体相对于1D-SFD和2D-SFD都表现出竞争性的性能。具体而言，1D-SWD相比1D/2D-SFD具有显著更低的计算复杂度。在Pascal VOC目标检测基准测试中，SWD变体在性能上超越了1D-SFD和2D-SFD，并且在训练过程中表现出更低的计算复杂度。</td>
<td>Rinor Cakaj</td>
<td><a href="http://arxiv.org/pdf/2409.18951v1">PDF</a></td>
<td>N/A</td>
<td>Spectral Wavelet Dropout: Regularization in the Wavelet Domain</td>
<td>Regularization techniques help prevent overfitting and therefore improve the ability of convolutional neural networks (CNNs) to generalize. One reason for overfitting is the complex co-adaptations among different parts of the network, which make the CNN dependent on their joint response rather than encouraging each part to learn a useful feature representation independently. Frequency domain manipulation is a powerful strategy for modifying data that has temporal and spatial coherence by utilizing frequency decomposition. This work introduces Spectral Wavelet Dropout (SWD), a novel regularization method that includes two variants: 1D-SWD and 2D-SWD. These variants improve CNN generalization by randomly dropping detailed frequency bands in the discrete wavelet decomposition of feature maps. Our approach distinguishes itself from the pre-existing Spectral "Fourier" Dropout (2D-SFD), which eliminates coefficients in the Fourier domain. Notably, SWD requires only a single hyperparameter, unlike the two required by SFD. We also extend the literature by implementing a one-dimensional version of Spectral "Fourier" Dropout (1D-SFD), setting the stage for a comprehensive comparison. Our evaluation shows that both 1D and 2D SWD variants have competitive performance on CIFAR-10/100 benchmarks relative to both 1D-SFD and 2D-SFD. Specifically, 1D-SWD has a significantly lower computational complexity compared to 1D/2D-SFD. In the Pascal VOC Object Detection benchmark, SWD variants surpass 1D-SFD and 2D-SFD in performance and demonstrate lower computational complexity during training.</td>
</tr>
<tr>
<td>实现除法归一化的递归神经电路的无条件稳定性</td>
<td>在递归神经模型中，稳定性是一个重大挑战，尤其是在开发能够无缝训练的生物学上合理的神经动力学模型时。传统的皮层电路模型由于动力系统中广泛的非线性而臭名昭著，难以训练，导致优化问题带有难以施加的非线性稳定性约束。相反，递归神经网络（RNNs）在涉及序列数据的任务中表现出色，但缺乏生物学合理性和可解释性。在这项工作中，我们通过将动态分裂归一化（DN）与ORGaNICs的稳定性联系起来，解决了这些挑战。ORGaNICs是一种生物学上合理的递归皮层电路模型，能够动态实现DN，并已被证明能够模拟广泛的神经生理现象。通过使用李雅普诺夫间接方法，我们证明了当递归权重矩阵为单位矩阵时，任意维度的ORGaNICs电路具有无条件的局部稳定性这一显著特性。因此，我们将ORGaNICs与耦合阻尼谐振子系统联系起来，使我们能够推导出电路的能量函数，提供电路及单个神经元所追求的规范性原则。此外，对于一般的递归权重矩阵，我们证明了二维模型的稳定性，并通过实证证明在高维度下稳定性仍然成立。最后，我们展示了由于ORGaNICs的内在稳定性特性和自适应时间常数，可以解决梯度爆炸、消失和振荡的问题，因此可以通过时间反向传播进行训练，而无需梯度裁剪/缩放。通过在RNN基准测试中评估模型的性能，我们发现ORGaNICs在静态图像分类任务中优于其他神经动力学模型，在序列任务中与LSTM表现相当。</td>
<td>Shivang Rawat</td>
<td><a href="http://arxiv.org/pdf/2409.18946v1">PDF</a></td>
<td>N/A</td>
<td>Unconditional stability of a recurrent neural circuit implementing divisive normalization</td>
<td>Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ORGaNICs, a biologically plausible recurrent cortical circuit model that dynamically achieves DN and has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.</td>
</tr>
<tr>
<td>通过声音建立信任：语调如何影响用户对语音助手吸引力的感知</td>
<td>语音助手（VAs）在处理简单任务时广受欢迎，但用户往往对使用它们进行复杂的在线购物等活动持谨慎态度。我们探讨了语音助手的语音特征，如语音助手的语音语调，是否能让用户在处理复杂任务时感觉它们更具吸引力和可信度。我们的研究结果表明，语音助手的语音语调对其被感知的吸引力和可信度有显著影响。实验中的参与者更倾向于被具有积极或中性语调的语音助手所吸引，并最终信任那些他们认为更具吸引力的语音助手。我们得出结论，通过精心设计的语音，结合多种语音语调，可以增强语音助手被感知的可信度。</td>
<td>Sabid Bin Habib Pias</td>
<td><a href="http://arxiv.org/pdf/2409.18941v1">PDF</a></td>
<td>N/A</td>
<td>Building Trust Through Voice: How Vocal Tone Impacts User Perception of Attractiveness of Voice Assistants</td>
<td>Voice Assistants (VAs) are popular for simple tasks, but users are often hesitant to use them for complex activities like online shopping. We explored whether the vocal characteristics like the VA's vocal tone, can make VAs perceived as more attractive and trustworthy to users for complex tasks. Our findings show that the tone of the VA voice significantly impacts its perceived attractiveness and trustworthiness. Participants in our experiment were more likely to be attracted to VAs with positive or neutral tones and ultimately trusted the VAs they found more attractive. We conclude that VA's perceived trustworthiness can be enhanced through thoughtful voice design, incorporating a variety of vocal tones.</td>
</tr>
<tr>
<td>从秒到小时：在综合长视频理解上评估多模态大语言模型</td>
<td>大型语言模型（LLMs）与视觉编码器的结合在视觉理解任务中最近展示了有前景的表现，利用其固有的能力来理解和生成类似人类的文本进行视觉推理。鉴于视觉数据的多样性，多模态大型语言模型（MM-LLMs）在理解和处理图像、短视频和长视频时表现出模型设计和训练的差异。我们的论文专注于长视频理解与静态图像和短视频理解相比所面临的显著差异和独特挑战。与静态图像不同，短视频包含具有空间和事件内时间信息的连续帧，而长视频则由具有事件间和长期时间信息的多事件组成。在这篇综述中，我们的目标是追溯和总结MM-LLMs从图像理解到长视频理解的进展。我们回顾了各种视觉理解任务之间的差异，并强调了长视频理解中的挑战，包括更细粒度的时空细节、动态事件和长期依赖性。然后，我们详细总结了MM-LLMs在理解和处理长视频方面的模型设计和训练方法的进展。最后，我们比较了现有MM-LLMs在不同长度视频理解基准上的表现，并讨论了MM-LLMs在长视频理解中的潜在未来方向。</td>
<td>Heqing Zou</td>
<td><a href="http://arxiv.org/pdf/2409.18938v1">PDF</a></td>
<td>N/A</td>
<td>From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding</td>
<td>The integration of Large Language Models (LLMs) with visual encoders has recently shown promising performance in visual understanding tasks, leveraging their inherent capability to comprehend and generate human-like text for visual reasoning. Given the diverse nature of visual data, MultiModal Large Language Models (MM-LLMs) exhibit variations in model designing and training for understanding images, short videos, and long videos. Our paper focuses on the substantial differences and unique challenges posed by long video understanding compared to static image and short video understanding. Unlike static images, short videos encompass sequential frames with both spatial and within-event temporal information, while long videos consist of multiple events with between-event and long-term temporal information. In this survey, we aim to trace and summarize the advancements of MM-LLMs from image understanding to long video understanding. We review the differences among various visual understanding tasks and highlight the challenges in long video understanding, including more fine-grained spatiotemporal details, dynamic events, and long-term dependencies. We then provide a detailed summary of the advancements in MM-LLMs in terms of model design and training methodologies for understanding long videos. Finally, we compare the performance of existing MM-LLMs on video understanding benchmarks of various lengths and discuss potential future directions for MM-LLMs in long video understanding.</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.525ec568.min.js"></script>
      
    
  </body>
</html>