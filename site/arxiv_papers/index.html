
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-papers">Arxiv Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>RoboTwin：配备生成式数字孪生的双臂机器人基准测试（早期版本）</td>
<td>双臂机器人及其工具使用能力的有效协作在机器人技术的进步中变得越来越重要。这些技能在扩展机器人在多样化的现实世界环境中操作的能力方面发挥着重要作用。然而，进展受到专业训练数据稀缺的阻碍。本文介绍了RoboTwin，这是一个新颖的基准数据集，结合了现实世界远程操作数据和数字孪生中的合成数据，专门设计用于双臂机器人场景。利用COBOT Magic平台，我们收集了关于工具使用和人机交互的多样化数据。我们提出了一种创新的方法，使用AI生成的内容创建数字孪生，将2D图像转化为详细的3D模型。此外，我们利用大型语言模型生成专家级别的训练数据和面向功能的任务特定姿态序列。我们的主要贡献包括：1) RoboTwin基准数据集，2) 高效的从现实到模拟的管道，以及3) 使用语言模型进行自动专家级别数据生成。这些进展旨在解决机器人训练数据的短缺问题，有望加速开发更强大和多功能的机器人系统，以应对广泛的现实世界应用。项目页面可在https://robotwin-benchmark.github.io/early-version/ 找到。</td>
<td>Yao Mu</td>
<td><a href="http://arxiv.org/pdf/2409.02920v1">PDF</a></td>
<td>N/A</td>
<td>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</td>
<td>Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at https://robotwin-benchmark.github.io/early-version/</td>
</tr>
<tr>
<td>UC-NeRF：从内窥镜稀疏视角出发的不确定性感知条件神经辐射场</td>
<td>可视化手术场景对于在微创手术过程中揭示内部解剖结构至关重要。新视角合成是一项关键技术，提供几何和外观重建，增强对手术场景的理解、规划和决策。尽管神经辐射场（NeRF）取得了显著成就，但其直接应用于手术场景由于两个挑战——内窥镜稀疏视图和显著的光度不一致性——而产生了不尽如人意的结果。在本文中，我们提出了用于新视角合成的不确定性感知条件NeRF，以解决从稀疏手术视图中产生的严重形状-辐射模糊问题。UC-NeRF的核心在于结合多视图不确定性估计，以条件化神经辐射场，从而自适应地建模严重的光度不一致性。具体而言，我们的UC-NeRF首先构建了一个多视图立体网络形式的连贯性学习器，以建立稀疏视图之间的几何对应关系，并生成不确定性估计和特征先验。在神经渲染中，我们设计了一个基础自适应NeRF网络，利用不确定性估计来显式处理光度不一致性。此外，采用不确定性引导的几何蒸馏来增强几何学习。在SCARED和Hamlyn数据集上的实验表明，我们在渲染外观和几何方面表现优越，持续超越当前最先进的方法。我们的代码将在\url{https://github.com/wrld/UC-NeRF}上发布。</td>
<td>Jiaxin Guo</td>
<td><a href="http://arxiv.org/pdf/2409.02917v1">PDF</a></td>
<td>N/A</td>
<td>UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views</td>
<td>Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches. Our code will be released at \url{https://github.com/wrld/UC-NeRF}.</td>
</tr>
<tr>
<td>掩码扩散模型实际上是时间无关的掩码模型，并且利用了不准确的分类采样</td>
<td>掩码扩散模型（MDMs）由于其在离散数据生成建模中优于其他离散扩散模型的出色表现，已成为一个热门的研究课题，并且在语言建模任务中与自回归模型（ARMs）展开了竞争。最近在简化掩码扩散框架方面的努力进一步促进了其与连续空间扩散模型的对齐，并带来了更系统化的训练和采样方法。然而，本文揭示了MDMs的训练和采样在理论上与时间变量无关，而时间变量可以说是扩散模型的关键特征，实际上它们等同于掩码模型。在采样方面的联系是通过我们提出的首次命中采样器（FHS）实现的。具体来说，我们证明FHS在理论上等同于MDMs的原始生成过程，同时显著缓解了耗时的类别采样问题，实现了20倍的速度提升。此外，我们的研究挑战了先前关于MDMs在生成困惑度上能超越ARMs的声称。我们首次发现，即使在32位浮点精度下，也存在一个潜在的数值问题，导致类别采样不准确。我们表明，这一数值问题在理论和实证上都降低了有效温度，导致先前文献中对MDMs生成结果的评估不公平。</td>
<td>Kaiwen Zheng</td>
<td><a href="http://arxiv.org/pdf/2409.02908v1">PDF</a></td>
<td>N/A</td>
<td>Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling</td>
<td>Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\times$ speedup. In addition, our investigation challenges previous claims that MDMs can surpass ARMs in generative perplexity. We identify, for the first time, an underlying numerical issue, even with the 32-bit floating-point precision, which results in inaccurate categorical sampling. We show that the numerical issue lowers the effective temperature both theoretically and empirically, leading to unfair assessments of MDMs' generation results in the previous literature.</td>
</tr>
<tr>
<td>机器学习中的拓扑方法：从业者教程</td>
<td>拓扑机器学习（TML）是一个新兴领域，它利用代数拓扑的技术来分析复杂的数据结构，这些结构是传统机器学习方法可能无法捕捉到的。本教程全面介绍了两种关键的TML技术：持久同调和Mapper算法，重点在于实际应用。持久同调捕捉多尺度的拓扑特征，如聚类、环和空洞，而Mapper算法则创建了一个可解释的图，用于总结高维数据。为了提高可访问性，我们采用以数据为中心的方法，使读者能够亲身体验将这些技术应用于相关任务。我们提供了逐步解释、实现、动手示例和案例研究，以展示如何将这些工具应用于现实世界的问题。我们的目标是让研究人员和从业者掌握知识和资源，将TML融入他们的工作中，揭示传统机器学习方法常常忽略的洞察。教程代码可在https://github.com/cakcora/TopologyForML获取。</td>
<td>Baris Coskunuzer</td>
<td><a href="http://arxiv.org/pdf/2409.02901v1">PDF</a></td>
<td>N/A</td>
<td>Topological Methods in Machine Learning: A Tutorial for Practitioners</td>
<td>Topological Machine Learning (TML) is an emerging field that leverages techniques from algebraic topology to analyze complex data structures in ways that traditional machine learning methods may not capture. This tutorial provides a comprehensive introduction to two key TML techniques, persistent homology and the Mapper algorithm, with an emphasis on practical applications. Persistent homology captures multi-scale topological features such as clusters, loops, and voids, while the Mapper algorithm creates an interpretable graph summarizing high-dimensional data. To enhance accessibility, we adopt a data-centric approach, enabling readers to gain hands-on experience applying these techniques to relevant tasks. We provide step-by-step explanations, implementations, hands-on examples, and case studies to demonstrate how these tools can be applied to real-world problems. The goal is to equip researchers and practitioners with the knowledge and resources to incorporate TML into their work, revealing insights often hidden from conventional machine learning methods. The tutorial code is available at https://github.com/cakcora/TopologyForML</td>
</tr>
<tr>
<td>区域数据驱动的天气模型，采用全球拉伸网格</td>
<td>本文介绍了一种适用于区域天气预报应用的数据驱动模型（DDM）。该模型通过引入一种拉伸网格架构扩展了人工智能预报系统，该架构在感兴趣的区域提供高分辨率，而在全球其他地方保持较低分辨率。模型基于图神经网络，自然支持任意多分辨率网格配置。模型应用于北欧地区的短期天气预测，生成空间分辨率为2.5公里、时间分辨率为6小时的预报。模型先在31公里分辨率的43年全球ERA5数据上进行预训练，然后使用来自MetCoOp集合预报系统（MEPS）的3.3年2.5公里分辨率业务分析数据进行进一步细化。模型的性能通过挪威各地测量站的地面观测数据进行评估，并与MEPS的短期天气预报进行比较。DDM在2米温度预报方面优于MEPS的控制运行和集合平均值。模型还生成了具有竞争力的降水和风速预报，但显示出低估极端事件的倾向。</td>
<td>Thomas Nils Nipen</td>
<td><a href="http://arxiv.org/pdf/2409.02891v1">PDF</a></td>
<td>N/A</td>
<td>Regional data-driven weather modeling with a global stretched-grid</td>
<td>A data-driven model (DDM) suitable for regional weather forecasting applications is presented. The model extends the Artificial Intelligence Forecasting System by introducing a stretched-grid architecture that dedicates higher resolution over a regional area of interest and maintains a lower resolution elsewhere on the globe. The model is based on graph neural networks, which naturally affords arbitrary multi-resolution grid configurations.   The model is applied to short-range weather prediction for the Nordics, producing forecasts at 2.5 km spatial and 6 h temporal resolution. The model is pre-trained on 43 years of global ERA5 data at 31 km resolution and is further refined using 3.3 years of 2.5 km resolution operational analyses from the MetCoOp Ensemble Prediction System (MEPS). The performance of the model is evaluated using surface observations from measurement stations across Norway and is compared to short-range weather forecasts from MEPS. The DDM outperforms both the control run and the ensemble mean of MEPS for 2 m temperature. The model also produces competitive precipitation and wind speed forecasts, but is shown to underestimate extreme events.</td>
</tr>
<tr>
<td>LongLLaVA: 通过混合架构高效扩展多模态大语言模型至1000张图像</td>
<td>扩展多模态大语言模型（MLLMs）的长上下文能力对于视频理解、高分辨率图像理解和多模态代理至关重要。这涉及一系列系统性优化，包括模型架构、数据构建和训练策略，特别是解决诸如\textit{随着图像数量增加性能下降}和\textit{高计算成本}等挑战。在本文中，我们将模型架构调整为Mamba和Transformer块的混合结构，采用包含多图像间时序和空间依赖性的数据构建方法，并采用渐进式训练策略。发布的模型\textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant)是首个混合MLLM，它在效率和有效性之间实现了更好的平衡。LongLLaVA不仅在各种基准测试中取得了有竞争力的结果，还保持了高吞吐量和低内存消耗。特别是，它能够在单个A100 80GB GPU上处理近千张图像，显示出在广泛任务中的应用前景。</td>
<td>Xidong Wang</td>
<td><a href="http://arxiv.org/pdf/2409.02889v1">PDF</a></td>
<td>N/A</td>
<td>LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture</td>
<td>Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \textit{degraded performance with more images} and \textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.</td>
</tr>
<tr>
<td>多流深度学习框架用于预测轻度认知障碍与雷伊复杂图形测试</td>
<td>像Rey复杂图形测试（RCFT）这样的绘图测试被广泛用于评估视觉空间技能和记忆等认知功能，因此成为检测轻度认知障碍（MCI）的有价值工具。尽管这些测试具有实用性，但基于这些测试的现有预测模型往往存在样本量小和缺乏外部验证等局限性，这削弱了它们的可靠性。我们开发了一个多流深度学习框架，该框架集成了两个不同的处理流：一个是基于多头自注意力机制的空间流，使用原始RCFT图像；另一个是评分流，采用先前开发的自动化评分系统。我们的模型在来自韩国队列的1,740名受试者的数据上进行了训练，并在来自韩国的222名受试者的外部医院数据集上进行了验证。所提出的多流模型在外部验证中表现出优于基线模型的性能（AUC = 0.872，准确率 = 0.781）。空间流和评分流的结合使得模型既能从原始图像中捕捉复杂的视觉细节，又能结合结构化的评分数据，从而增强其检测细微认知障碍的能力。这种双重方法不仅提高了预测准确性，还增强了模型的鲁棒性，使其在多样化的临床环境中更加可靠。我们的模型对临床实践具有实际意义，可以作为一种成本效益高的工具用于早期MCI筛查。</td>
<td>Junyoung Park</td>
<td><a href="http://arxiv.org/pdf/2409.02883v1">PDF</a></td>
<td>N/A</td>
<td>Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test</td>
<td>Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to assess cognitive functions such as visuospatial skills and memory, making them valuable tools for detecting mild cognitive impairment (MCI). Despite their utility, existing predictive models based on these tests often suffer from limitations like small sample sizes and lack of external validation, which undermine their reliability. We developed a multi-stream deep learning framework that integrates two distinct processing streams: a multi-head self-attention based spatial stream using raw RCFT images and a scoring stream employing a previously developed automated scoring system. Our model was trained on data from 1,740 subjects in the Korean cohort and validated on an external hospital dataset of 222 subjects from Korea. The proposed multi-stream model demonstrated superior performance over baseline models (AUC = 0.872, Accuracy = 0.781) in external validation. The integration of both spatial and scoring streams enables the model to capture intricate visual details from the raw images while also incorporating structured scoring data, which together enhance its ability to detect subtle cognitive impairments. This dual approach not only improves predictive accuracy but also increases the robustness of the model, making it more reliable in diverse clinical settings. Our model has practical implications for clinical settings, where it could serve as a cost-effective tool for early MCI screening.</td>
</tr>
<tr>
<td>基准测试少样本图像分类器中的虚假偏差</td>
<td>少样本图像分类器旨在以最少的监督和有限的数据来识别和分类新数据，但通常表现出对类别间虚假关联和虚假属性的依赖，这种依赖被称为虚假偏差。虚假关联在某些样本中普遍存在，少样本分类器可能会受到由此引发的虚假偏差的影响。目前缺乏一个自动化的基准测试系统来评估少样本分类器对虚假偏差的鲁棒性。在本文中，我们提出了一种系统且严格的基准测试框架，称为FewSTAB，以公平地展示和量化少样本分类器对虚假偏差的多样化鲁棒性程度。FewSTAB创建了带有偏差属性的少样本评估任务，使得使用这些任务进行预测时会表现出较差的表现。为了构建这些任务，我们提出了一种基于预训练视觉语言模型的属性样本选择策略，从而消除了手动数据集整理的需求。这使得FewSTAB能够使用任何现有的测试数据自动基准测试虚假偏差。FewSTAB不仅提供了新的评估维度，还为构建鲁棒分类器提供了新的设计指南。此外，它能够基准测试不同程度的虚假偏差，并支持设计不同程度的鲁棒性。通过在三个数据集上对十种少样本学习方法进行实验，展示了其有效性。我们希望我们的框架能够激发对鲁棒少样本分类器的新设计。我们的代码可在https://github.com/gtzheng/FewSTAB获取。</td>
<td>Guangtao Zheng</td>
<td><a href="http://arxiv.org/pdf/2409.02882v1">PDF</a></td>
<td>N/A</td>
<td>Benchmarking Spurious Bias in Few-Shot Image Classifiers</td>
<td>Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB.</td>
</tr>
<tr>
<td>可配置的基础模型：从模块化角度构建大型语言模型</td>
<td>近年来，大型语言模型（LLMs）在技术上的进步揭示了与其巨大参数需求相关的计算效率和持续可扩展性挑战，使得这些模型在计算资源有限且需要多样化能力的设备和场景中的应用与演进愈发复杂。受到人脑模块化结构的启发，将LLMs分解为众多功能模块的趋势日益明显，这些模块允许部分模块进行推理，并通过模块的动态组合来应对复杂任务，如专家混合（mixture-of-experts）。为了凸显模块化方法的固有效率和可组合性，我们提出了“砖块”（brick）这一术语来代表每个功能模块，并将模块化结构称为可配置的基础模型。本文全面概述并探讨了可配置基础模型的构建、利用及其局限性。我们首先将模块形式化为涌现砖块——预训练阶段涌现的功能神经分区，以及定制砖块——通过额外训练构建的砖块，以增强LLMs的能力和知识。基于多样化的功能砖块，我们进一步提出了四种面向砖块的操作：检索与路由、合并、更新和扩展。这些操作使得LLMs能够根据指令动态配置，以处理复杂任务。为验证我们的观点，我们对广泛使用的LLMs进行了实证分析，发现前馈神经网络（FFN）层遵循模块化模式，具有神经元的功能特化和功能神经分区。最后，我们指出了几个开放问题和未来研究方向。总体而言，本文旨在为现有的LLM研究提供一个新颖的模块化视角，并激发未来创建更高效、可扩展的基础模型。</td>
<td>Chaojun Xiao</td>
<td><a href="http://arxiv.org/pdf/2409.02877v1">PDF</a></td>
<td>N/A</td>
<td>Configurable Foundation Models: Building LLMs from a Modular Perspective</td>
<td>Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.</td>
</tr>
<tr>
<td>城市驾驶混合仿真学习运动规划器</td>
<td>随着nuPlan和Argoverse等开源数据集的发布，基于学习的规划器研究在过去几年中得到了广泛传播。现有系统在模仿人类驾驶员行为方面表现出色，但在保证安全闭环驾驶方面仍面临挑战。相反，基于优化的规划器在短期规划场景中提供了更高的安全性。为了应对这一挑战，本文提出了一种新颖的混合运动规划器，结合了基于学习和基于优化的技术。首先，多层感知器（MLP）生成类似人类的轨迹，然后由基于优化的组件进行细化。该组件不仅最小化了跟踪误差，还计算出在运动学上可行且与障碍物和道路边界无碰撞的轨迹。我们的模型有效地平衡了安全性和人类相似性，缓解了这些目标之间的固有权衡。我们通过仿真实验验证了我们的方法，并进一步通过在真实世界的自动驾驶车辆上部署来展示其有效性。</td>
<td>Cristian Gariboldi</td>
<td><a href="http://arxiv.org/pdf/2409.02871v1">PDF</a></td>
<td>N/A</td>
<td>Hybrid Imitation-Learning Motion Planner for Urban Driving</td>
<td>With the release of open source datasets such as nuPlan and Argoverse, the research around learning-based planners has spread a lot in the last years. Existing systems have shown excellent capabilities in imitating the human driver behaviour, but they struggle to guarantee safe closed-loop driving. Conversely, optimization-based planners offer greater security in short-term planning scenarios. To confront this challenge, in this paper we propose a novel hybrid motion planner that integrates both learning-based and optimization-based techniques. Initially, a multilayer perceptron (MLP) generates a human-like trajectory, which is then refined by an optimization-based component. This component not only minimizes tracking errors but also computes a trajectory that is both kinematically feasible and collision-free with obstacles and road boundaries. Our model effectively balances safety and human-likeness, mitigating the trade-off inherent in these objectives. We validate our approach through simulation experiments and further demonstrate its efficacy by deploying it in real-world self-driving vehicles.</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>