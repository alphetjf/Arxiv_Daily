{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract RoboTwin\uff1a\u914d\u5907\u751f\u6210\u5f0f\u6570\u5b57\u5b6a\u751f\u7684\u53cc\u81c2\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\uff08\u65e9\u671f\u7248\u672c\uff09 \u53cc\u81c2\u673a\u5668\u4eba\u53ca\u5176\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u6709\u6548\u534f\u4f5c\u5728\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd9\u4e9b\u6280\u80fd\u5728\u6269\u5c55\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u7684\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u80fd\u529b\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0c\u8fdb\u5c55\u53d7\u5230\u4e13\u4e1a\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u963b\u788d\u3002\u672c\u6587\u4ecb\u7ecd\u4e86RoboTwin\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u73b0\u5b9e\u4e16\u754c\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u548c\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u5408\u6210\u6570\u636e\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u53cc\u81c2\u673a\u5668\u4eba\u573a\u666f\u3002\u5229\u7528COBOT Magic\u5e73\u53f0\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u5173\u4e8e\u5de5\u5177\u4f7f\u7528\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u591a\u6837\u5316\u6570\u636e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528AI\u751f\u6210\u7684\u5185\u5bb9\u521b\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u5c062D\u56fe\u50cf\u8f6c\u5316\u4e3a\u8be6\u7ec6\u76843D\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e13\u5bb6\u7ea7\u522b\u7684\u8bad\u7ec3\u6570\u636e\u548c\u9762\u5411\u529f\u80fd\u7684\u4efb\u52a1\u7279\u5b9a\u59ff\u6001\u5e8f\u5217\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1) RoboTwin\u57fa\u51c6\u6570\u636e\u96c6\uff0c2) \u9ad8\u6548\u7684\u4ece\u73b0\u5b9e\u5230\u6a21\u62df\u7684\u7ba1\u9053\uff0c\u4ee5\u53ca3) \u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u4e13\u5bb6\u7ea7\u522b\u6570\u636e\u751f\u6210\u3002\u8fd9\u4e9b\u8fdb\u5c55\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u8bad\u7ec3\u6570\u636e\u7684\u77ed\u7f3a\u95ee\u9898\uff0c\u6709\u671b\u52a0\u901f\u5f00\u53d1\u66f4\u5f3a\u5927\u548c\u591a\u529f\u80fd\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u3002\u9879\u76ee\u9875\u9762\u53ef\u5728https://robotwin-benchmark.github.io/early-version/ \u627e\u5230\u3002 Yao Mu PDF N/A RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version) Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at https://robotwin-benchmark.github.io/early-version/ UC-NeRF\uff1a\u4ece\u5185\u7aa5\u955c\u7a00\u758f\u89c6\u89d2\u51fa\u53d1\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6761\u4ef6\u795e\u7ecf\u8f90\u5c04\u573a \u53ef\u89c6\u5316\u624b\u672f\u573a\u666f\u5bf9\u4e8e\u5728\u5fae\u521b\u624b\u672f\u8fc7\u7a0b\u4e2d\u63ed\u793a\u5185\u90e8\u89e3\u5256\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002\u65b0\u89c6\u89d2\u5408\u6210\u662f\u4e00\u9879\u5173\u952e\u6280\u672f\uff0c\u63d0\u4f9b\u51e0\u4f55\u548c\u5916\u89c2\u91cd\u5efa\uff0c\u589e\u5f3a\u5bf9\u624b\u672f\u573a\u666f\u7684\u7406\u89e3\u3001\u89c4\u5212\u548c\u51b3\u7b56\u3002\u5c3d\u7ba1\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u624b\u672f\u573a\u666f\u7531\u4e8e\u4e24\u4e2a\u6311\u6218\u2014\u2014\u5185\u7aa5\u955c\u7a00\u758f\u89c6\u56fe\u548c\u663e\u8457\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u2014\u2014\u800c\u4ea7\u751f\u4e86\u4e0d\u5c3d\u5982\u4eba\u610f\u7684\u7ed3\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u65b0\u89c6\u89d2\u5408\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6761\u4ef6NeRF\uff0c\u4ee5\u89e3\u51b3\u4ece\u7a00\u758f\u624b\u672f\u89c6\u56fe\u4e2d\u4ea7\u751f\u7684\u4e25\u91cd\u5f62\u72b6-\u8f90\u5c04\u6a21\u7cca\u95ee\u9898\u3002UC-NeRF\u7684\u6838\u5fc3\u5728\u4e8e\u7ed3\u5408\u591a\u89c6\u56fe\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ee5\u6761\u4ef6\u5316\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u5efa\u6a21\u4e25\u91cd\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684UC-NeRF\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u89c6\u56fe\u7acb\u4f53\u7f51\u7edc\u5f62\u5f0f\u7684\u8fde\u8d2f\u6027\u5b66\u4e60\u5668\uff0c\u4ee5\u5efa\u7acb\u7a00\u758f\u89c6\u56fe\u4e4b\u95f4\u7684\u51e0\u4f55\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7279\u5f81\u5148\u9a8c\u3002\u5728\u795e\u7ecf\u6e32\u67d3\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u7840\u81ea\u9002\u5e94NeRF\u7f51\u7edc\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u663e\u5f0f\u5904\u7406\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u51e0\u4f55\u84b8\u998f\u6765\u589e\u5f3a\u51e0\u4f55\u5b66\u4e60\u3002\u5728SCARED\u548cHamlyn\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u5728\u6e32\u67d3\u5916\u89c2\u548c\u51e0\u4f55\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u6301\u7eed\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728\\url{https://github.com/wrld/UC-NeRF}\u4e0a\u53d1\u5e03\u3002 Jiaxin Guo PDF N/A UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches. Our code will be released at \\url{https://github.com/wrld/UC-NeRF}. \u63a9\u7801\u6269\u6563\u6a21\u578b\u5b9e\u9645\u4e0a\u662f\u65f6\u95f4\u65e0\u5173\u7684\u63a9\u7801\u6a21\u578b\uff0c\u5e76\u4e14\u5229\u7528\u4e86\u4e0d\u51c6\u786e\u7684\u5206\u7c7b\u91c7\u6837 \u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u7531\u4e8e\u5176\u5728\u79bb\u6563\u6570\u636e\u751f\u6210\u5efa\u6a21\u4e2d\u4f18\u4e8e\u5176\u4ed6\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u51fa\u8272\u8868\u73b0\uff0c\u5df2\u6210\u4e3a\u4e00\u4e2a\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898\uff0c\u5e76\u4e14\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u5c55\u5f00\u4e86\u7ade\u4e89\u3002\u6700\u8fd1\u5728\u7b80\u5316\u63a9\u7801\u6269\u6563\u6846\u67b6\u65b9\u9762\u7684\u52aa\u529b\u8fdb\u4e00\u6b65\u4fc3\u8fdb\u4e86\u5176\u4e0e\u8fde\u7eed\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u5e76\u5e26\u6765\u4e86\u66f4\u7cfb\u7edf\u5316\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u672c\u6587\u63ed\u793a\u4e86MDMs\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u5728\u7406\u8bba\u4e0a\u4e0e\u65f6\u95f4\u53d8\u91cf\u65e0\u5173\uff0c\u800c\u65f6\u95f4\u53d8\u91cf\u53ef\u4ee5\u8bf4\u662f\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff0c\u5b9e\u9645\u4e0a\u5b83\u4eec\u7b49\u540c\u4e8e\u63a9\u7801\u6a21\u578b\u3002\u5728\u91c7\u6837\u65b9\u9762\u7684\u8054\u7cfb\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u9996\u6b21\u547d\u4e2d\u91c7\u6837\u5668\uff08FHS\uff09\u5b9e\u73b0\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660eFHS\u5728\u7406\u8bba\u4e0a\u7b49\u540c\u4e8eMDMs\u7684\u539f\u59cb\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u663e\u8457\u7f13\u89e3\u4e86\u8017\u65f6\u7684\u7c7b\u522b\u91c7\u6837\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8620\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u6311\u6218\u4e86\u5148\u524d\u5173\u4e8eMDMs\u5728\u751f\u6210\u56f0\u60d1\u5ea6\u4e0a\u80fd\u8d85\u8d8aARMs\u7684\u58f0\u79f0\u3002\u6211\u4eec\u9996\u6b21\u53d1\u73b0\uff0c\u5373\u4f7f\u572832\u4f4d\u6d6e\u70b9\u7cbe\u5ea6\u4e0b\uff0c\u4e5f\u5b58\u5728\u4e00\u4e2a\u6f5c\u5728\u7684\u6570\u503c\u95ee\u9898\uff0c\u5bfc\u81f4\u7c7b\u522b\u91c7\u6837\u4e0d\u51c6\u786e\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u4e00\u6570\u503c\u95ee\u9898\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u6e29\u5ea6\uff0c\u5bfc\u81f4\u5148\u524d\u6587\u732e\u4e2d\u5bf9MDMs\u751f\u6210\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\u3002 Kaiwen Zheng PDF N/A Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\\times$ speedup. In addition, our investigation challenges previous claims that MDMs can surpass ARMs in generative perplexity. We identify, for the first time, an underlying numerical issue, even with the 32-bit floating-point precision, which results in inaccurate categorical sampling. We show that the numerical issue lowers the effective temperature both theoretically and empirically, leading to unfair assessments of MDMs' generation results in the previous literature. \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u62d3\u6251\u65b9\u6cd5\uff1a\u4ece\u4e1a\u8005\u6559\u7a0b \u62d3\u6251\u673a\u5668\u5b66\u4e60\uff08TML\uff09\u662f\u4e00\u4e2a\u65b0\u5174\u9886\u57df\uff0c\u5b83\u5229\u7528\u4ee3\u6570\u62d3\u6251\u7684\u6280\u672f\u6765\u5206\u6790\u590d\u6742\u7684\u6570\u636e\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u662f\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u5230\u7684\u3002\u672c\u6559\u7a0b\u5168\u9762\u4ecb\u7ecd\u4e86\u4e24\u79cd\u5173\u952e\u7684TML\u6280\u672f\uff1a\u6301\u4e45\u540c\u8c03\u548cMapper\u7b97\u6cd5\uff0c\u91cd\u70b9\u5728\u4e8e\u5b9e\u9645\u5e94\u7528\u3002\u6301\u4e45\u540c\u8c03\u6355\u6349\u591a\u5c3a\u5ea6\u7684\u62d3\u6251\u7279\u5f81\uff0c\u5982\u805a\u7c7b\u3001\u73af\u548c\u7a7a\u6d1e\uff0c\u800cMapper\u7b97\u6cd5\u5219\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u56fe\uff0c\u7528\u4e8e\u603b\u7ed3\u9ad8\u7ef4\u6570\u636e\u3002\u4e3a\u4e86\u63d0\u9ad8\u53ef\u8bbf\u95ee\u6027\uff0c\u6211\u4eec\u91c7\u7528\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u4f7f\u8bfb\u8005\u80fd\u591f\u4eb2\u8eab\u4f53\u9a8c\u5c06\u8fd9\u4e9b\u6280\u672f\u5e94\u7528\u4e8e\u76f8\u5173\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u9010\u6b65\u89e3\u91ca\u3001\u5b9e\u73b0\u3001\u52a8\u624b\u793a\u4f8b\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u4ee5\u5c55\u793a\u5982\u4f55\u5c06\u8fd9\u4e9b\u5de5\u5177\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u95ee\u9898\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u8ba9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u638c\u63e1\u77e5\u8bc6\u548c\u8d44\u6e90\uff0c\u5c06TML\u878d\u5165\u4ed6\u4eec\u7684\u5de5\u4f5c\u4e2d\uff0c\u63ed\u793a\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e38\u5e38\u5ffd\u7565\u7684\u6d1e\u5bdf\u3002\u6559\u7a0b\u4ee3\u7801\u53ef\u5728https://github.com/cakcora/TopologyForML\u83b7\u53d6\u3002 Baris Coskunuzer PDF N/A Topological Methods in Machine Learning: A Tutorial for Practitioners Topological Machine Learning (TML) is an emerging field that leverages techniques from algebraic topology to analyze complex data structures in ways that traditional machine learning methods may not capture. This tutorial provides a comprehensive introduction to two key TML techniques, persistent homology and the Mapper algorithm, with an emphasis on practical applications. Persistent homology captures multi-scale topological features such as clusters, loops, and voids, while the Mapper algorithm creates an interpretable graph summarizing high-dimensional data. To enhance accessibility, we adopt a data-centric approach, enabling readers to gain hands-on experience applying these techniques to relevant tasks. We provide step-by-step explanations, implementations, hands-on examples, and case studies to demonstrate how these tools can be applied to real-world problems. The goal is to equip researchers and practitioners with the knowledge and resources to incorporate TML into their work, revealing insights often hidden from conventional machine learning methods. The tutorial code is available at https://github.com/cakcora/TopologyForML \u533a\u57df\u6570\u636e\u9a71\u52a8\u7684\u5929\u6c14\u6a21\u578b\uff0c\u91c7\u7528\u5168\u7403\u62c9\u4f38\u7f51\u683c \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u533a\u57df\u5929\u6c14\u9884\u62a5\u5e94\u7528\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08DDM\uff09\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u62c9\u4f38\u7f51\u683c\u67b6\u6784\u6269\u5c55\u4e86\u4eba\u5de5\u667a\u80fd\u9884\u62a5\u7cfb\u7edf\uff0c\u8be5\u67b6\u6784\u5728\u611f\u5174\u8da3\u7684\u533a\u57df\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\uff0c\u800c\u5728\u5168\u7403\u5176\u4ed6\u5730\u65b9\u4fdd\u6301\u8f83\u4f4e\u5206\u8fa8\u7387\u3002\u6a21\u578b\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u81ea\u7136\u652f\u6301\u4efb\u610f\u591a\u5206\u8fa8\u7387\u7f51\u683c\u914d\u7f6e\u3002\u6a21\u578b\u5e94\u7528\u4e8e\u5317\u6b27\u5730\u533a\u7684\u77ed\u671f\u5929\u6c14\u9884\u6d4b\uff0c\u751f\u6210\u7a7a\u95f4\u5206\u8fa8\u7387\u4e3a2.5\u516c\u91cc\u3001\u65f6\u95f4\u5206\u8fa8\u7387\u4e3a6\u5c0f\u65f6\u7684\u9884\u62a5\u3002\u6a21\u578b\u5148\u572831\u516c\u91cc\u5206\u8fa8\u7387\u768443\u5e74\u5168\u7403ERA5\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u6765\u81eaMetCoOp\u96c6\u5408\u9884\u62a5\u7cfb\u7edf\uff08MEPS\uff09\u76843.3\u5e742.5\u516c\u91cc\u5206\u8fa8\u7387\u4e1a\u52a1\u5206\u6790\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u7ec6\u5316\u3002\u6a21\u578b\u7684\u6027\u80fd\u901a\u8fc7\u632a\u5a01\u5404\u5730\u6d4b\u91cf\u7ad9\u7684\u5730\u9762\u89c2\u6d4b\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0eMEPS\u7684\u77ed\u671f\u5929\u6c14\u9884\u62a5\u8fdb\u884c\u6bd4\u8f83\u3002DDM\u57282\u7c73\u6e29\u5ea6\u9884\u62a5\u65b9\u9762\u4f18\u4e8eMEPS\u7684\u63a7\u5236\u8fd0\u884c\u548c\u96c6\u5408\u5e73\u5747\u503c\u3002\u6a21\u578b\u8fd8\u751f\u6210\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u964d\u6c34\u548c\u98ce\u901f\u9884\u62a5\uff0c\u4f46\u663e\u793a\u51fa\u4f4e\u4f30\u6781\u7aef\u4e8b\u4ef6\u7684\u503e\u5411\u3002 Thomas Nils Nipen PDF N/A Regional data-driven weather modeling with a global stretched-grid A data-driven model (DDM) suitable for regional weather forecasting applications is presented. The model extends the Artificial Intelligence Forecasting System by introducing a stretched-grid architecture that dedicates higher resolution over a regional area of interest and maintains a lower resolution elsewhere on the globe. The model is based on graph neural networks, which naturally affords arbitrary multi-resolution grid configurations.   The model is applied to short-range weather prediction for the Nordics, producing forecasts at 2.5 km spatial and 6 h temporal resolution. The model is pre-trained on 43 years of global ERA5 data at 31 km resolution and is further refined using 3.3 years of 2.5 km resolution operational analyses from the MetCoOp Ensemble Prediction System (MEPS). The performance of the model is evaluated using surface observations from measurement stations across Norway and is compared to short-range weather forecasts from MEPS. The DDM outperforms both the control run and the ensemble mean of MEPS for 2 m temperature. The model also produces competitive precipitation and wind speed forecasts, but is shown to underestimate extreme events. LongLLaVA: \u901a\u8fc7\u6df7\u5408\u67b6\u6784\u9ad8\u6548\u6269\u5c55\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u81f31000\u5f20\u56fe\u50cf \u6269\u5c55\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u5bf9\u4e8e\u89c6\u9891\u7406\u89e3\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u548c\u591a\u6a21\u6001\u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u6d89\u53ca\u4e00\u7cfb\u5217\u7cfb\u7edf\u6027\u4f18\u5316\uff0c\u5305\u62ec\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u6784\u5efa\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u7279\u522b\u662f\u89e3\u51b3\u8bf8\u5982\\textit{\u968f\u7740\u56fe\u50cf\u6570\u91cf\u589e\u52a0\u6027\u80fd\u4e0b\u964d}\u548c\\textit{\u9ad8\u8ba1\u7b97\u6210\u672c}\u7b49\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u6a21\u578b\u67b6\u6784\u8c03\u6574\u4e3aMamba\u548cTransformer\u5757\u7684\u6df7\u5408\u7ed3\u6784\uff0c\u91c7\u7528\u5305\u542b\u591a\u56fe\u50cf\u95f4\u65f6\u5e8f\u548c\u7a7a\u95f4\u4f9d\u8d56\u6027\u7684\u6570\u636e\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u3002\u53d1\u5e03\u7684\u6a21\u578b\\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant)\u662f\u9996\u4e2a\u6df7\u5408MLLM\uff0c\u5b83\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002LongLLaVA\u4e0d\u4ec5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8fd8\u4fdd\u6301\u4e86\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5185\u5b58\u6d88\u8017\u3002\u7279\u522b\u662f\uff0c\u5b83\u80fd\u591f\u5728\u5355\u4e2aA100 80GB GPU\u4e0a\u5904\u7406\u8fd1\u5343\u5f20\u56fe\u50cf\uff0c\u663e\u793a\u51fa\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002 Xidong Wang PDF N/A LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \\textit{degraded performance with more images} and \\textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks. \u591a\u6d41\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u9884\u6d4b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u4e0e\u96f7\u4f0a\u590d\u6742\u56fe\u5f62\u6d4b\u8bd5 \u50cfRey\u590d\u6742\u56fe\u5f62\u6d4b\u8bd5\uff08RCFT\uff09\u8fd9\u6837\u7684\u7ed8\u56fe\u6d4b\u8bd5\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u7a7a\u95f4\u6280\u80fd\u548c\u8bb0\u5fc6\u7b49\u8ba4\u77e5\u529f\u80fd\uff0c\u56e0\u6b64\u6210\u4e3a\u68c0\u6d4b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u7684\u6709\u4ef7\u503c\u5de5\u5177\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6d4b\u8bd5\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u57fa\u4e8e\u8fd9\u4e9b\u6d4b\u8bd5\u7684\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u5f80\u5f80\u5b58\u5728\u6837\u672c\u91cf\u5c0f\u548c\u7f3a\u4e4f\u5916\u90e8\u9a8c\u8bc1\u7b49\u5c40\u9650\u6027\uff0c\u8fd9\u524a\u5f31\u4e86\u5b83\u4eec\u7684\u53ef\u9760\u6027\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6d41\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4e24\u4e2a\u4e0d\u540c\u7684\u5904\u7406\u6d41\uff1a\u4e00\u4e2a\u662f\u57fa\u4e8e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u7a7a\u95f4\u6d41\uff0c\u4f7f\u7528\u539f\u59cbRCFT\u56fe\u50cf\uff1b\u53e6\u4e00\u4e2a\u662f\u8bc4\u5206\u6d41\uff0c\u91c7\u7528\u5148\u524d\u5f00\u53d1\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u6765\u81ea\u97e9\u56fd\u961f\u5217\u76841,740\u540d\u53d7\u8bd5\u8005\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u5728\u6765\u81ea\u97e9\u56fd\u7684222\u540d\u53d7\u8bd5\u8005\u7684\u5916\u90e8\u533b\u9662\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u6240\u63d0\u51fa\u7684\u591a\u6d41\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff08AUC = 0.872\uff0c\u51c6\u786e\u7387 = 0.781\uff09\u3002\u7a7a\u95f4\u6d41\u548c\u8bc4\u5206\u6d41\u7684\u7ed3\u5408\u4f7f\u5f97\u6a21\u578b\u65e2\u80fd\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u6355\u6349\u590d\u6742\u7684\u89c6\u89c9\u7ec6\u8282\uff0c\u53c8\u80fd\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u8bc4\u5206\u6570\u636e\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u68c0\u6d4b\u7ec6\u5fae\u8ba4\u77e5\u969c\u788d\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u53cc\u91cd\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u5728\u591a\u6837\u5316\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u66f4\u52a0\u53ef\u9760\u3002\u6211\u4eec\u7684\u6a21\u578b\u5bf9\u4e34\u5e8a\u5b9e\u8df5\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5de5\u5177\u7528\u4e8e\u65e9\u671fMCI\u7b5b\u67e5\u3002 Junyoung Park PDF N/A Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to assess cognitive functions such as visuospatial skills and memory, making them valuable tools for detecting mild cognitive impairment (MCI). Despite their utility, existing predictive models based on these tests often suffer from limitations like small sample sizes and lack of external validation, which undermine their reliability. We developed a multi-stream deep learning framework that integrates two distinct processing streams: a multi-head self-attention based spatial stream using raw RCFT images and a scoring stream employing a previously developed automated scoring system. Our model was trained on data from 1,740 subjects in the Korean cohort and validated on an external hospital dataset of 222 subjects from Korea. The proposed multi-stream model demonstrated superior performance over baseline models (AUC = 0.872, Accuracy = 0.781) in external validation. The integration of both spatial and scoring streams enables the model to capture intricate visual details from the raw images while also incorporating structured scoring data, which together enhance its ability to detect subtle cognitive impairments. This dual approach not only improves predictive accuracy but also increases the robustness of the model, making it more reliable in diverse clinical settings. Our model has practical implications for clinical settings, where it could serve as a cost-effective tool for early MCI screening. \u57fa\u51c6\u6d4b\u8bd5\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u5668\u4e2d\u7684\u865a\u5047\u504f\u5dee \u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u5668\u65e8\u5728\u4ee5\u6700\u5c11\u7684\u76d1\u7763\u548c\u6709\u9650\u7684\u6570\u636e\u6765\u8bc6\u522b\u548c\u5206\u7c7b\u65b0\u6570\u636e\uff0c\u4f46\u901a\u5e38\u8868\u73b0\u51fa\u5bf9\u7c7b\u522b\u95f4\u865a\u5047\u5173\u8054\u548c\u865a\u5047\u5c5e\u6027\u7684\u4f9d\u8d56\uff0c\u8fd9\u79cd\u4f9d\u8d56\u88ab\u79f0\u4e3a\u865a\u5047\u504f\u5dee\u3002\u865a\u5047\u5173\u8054\u5728\u67d0\u4e9b\u6837\u672c\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5c11\u6837\u672c\u5206\u7c7b\u5668\u53ef\u80fd\u4f1a\u53d7\u5230\u7531\u6b64\u5f15\u53d1\u7684\u865a\u5047\u504f\u5dee\u7684\u5f71\u54cd\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u6765\u8bc4\u4f30\u5c11\u6837\u672c\u5206\u7c7b\u5668\u5bf9\u865a\u5047\u504f\u5dee\u7684\u9c81\u68d2\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u4e14\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u79f0\u4e3aFewSTAB\uff0c\u4ee5\u516c\u5e73\u5730\u5c55\u793a\u548c\u91cf\u5316\u5c11\u6837\u672c\u5206\u7c7b\u5668\u5bf9\u865a\u5047\u504f\u5dee\u7684\u591a\u6837\u5316\u9c81\u68d2\u6027\u7a0b\u5ea6\u3002FewSTAB\u521b\u5efa\u4e86\u5e26\u6709\u504f\u5dee\u5c5e\u6027\u7684\u5c11\u6837\u672c\u8bc4\u4f30\u4efb\u52a1\uff0c\u4f7f\u5f97\u4f7f\u7528\u8fd9\u4e9b\u4efb\u52a1\u8fdb\u884c\u9884\u6d4b\u65f6\u4f1a\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u8868\u73b0\u3002\u4e3a\u4e86\u6784\u5efa\u8fd9\u4e9b\u4efb\u52a1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5c5e\u6027\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u624b\u52a8\u6570\u636e\u96c6\u6574\u7406\u7684\u9700\u6c42\u3002\u8fd9\u4f7f\u5f97FewSTAB\u80fd\u591f\u4f7f\u7528\u4efb\u4f55\u73b0\u6709\u7684\u6d4b\u8bd5\u6570\u636e\u81ea\u52a8\u57fa\u51c6\u6d4b\u8bd5\u865a\u5047\u504f\u5dee\u3002FewSTAB\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u8fd8\u4e3a\u6784\u5efa\u9c81\u68d2\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u6307\u5357\u3002\u6b64\u5916\uff0c\u5b83\u80fd\u591f\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u540c\u7a0b\u5ea6\u7684\u865a\u5047\u504f\u5dee\uff0c\u5e76\u652f\u6301\u8bbe\u8ba1\u4e0d\u540c\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf9\u5341\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u6846\u67b6\u80fd\u591f\u6fc0\u53d1\u5bf9\u9c81\u68d2\u5c11\u6837\u672c\u5206\u7c7b\u5668\u7684\u65b0\u8bbe\u8ba1\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/gtzheng/FewSTAB\u83b7\u53d6\u3002 Guangtao Zheng PDF N/A Benchmarking Spurious Bias in Few-Shot Image Classifiers Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB. \u53ef\u914d\u7f6e\u7684\u57fa\u7840\u6a21\u578b\uff1a\u4ece\u6a21\u5757\u5316\u89d2\u5ea6\u6784\u5efa\u5927\u578b\u8bed\u8a00\u6a21\u578b \u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6280\u672f\u4e0a\u7684\u8fdb\u6b65\u63ed\u793a\u4e86\u4e0e\u5176\u5de8\u5927\u53c2\u6570\u9700\u6c42\u76f8\u5173\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6301\u7eed\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u6a21\u578b\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4e14\u9700\u8981\u591a\u6837\u5316\u80fd\u529b\u7684\u8bbe\u5907\u548c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4e0e\u6f14\u8fdb\u6108\u53d1\u590d\u6742\u3002\u53d7\u5230\u4eba\u8111\u6a21\u5757\u5316\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u5c06LLMs\u5206\u89e3\u4e3a\u4f17\u591a\u529f\u80fd\u6a21\u5757\u7684\u8d8b\u52bf\u65e5\u76ca\u660e\u663e\uff0c\u8fd9\u4e9b\u6a21\u5757\u5141\u8bb8\u90e8\u5206\u6a21\u5757\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u7684\u52a8\u6001\u7ec4\u5408\u6765\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\uff0c\u5982\u4e13\u5bb6\u6df7\u5408\uff08mixture-of-experts\uff09\u3002\u4e3a\u4e86\u51f8\u663e\u6a21\u5757\u5316\u65b9\u6cd5\u7684\u56fa\u6709\u6548\u7387\u548c\u53ef\u7ec4\u5408\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u7816\u5757\u201d\uff08brick\uff09\u8fd9\u4e00\u672f\u8bed\u6765\u4ee3\u8868\u6bcf\u4e2a\u529f\u80fd\u6a21\u5757\uff0c\u5e76\u5c06\u6a21\u5757\u5316\u7ed3\u6784\u79f0\u4e3a\u53ef\u914d\u7f6e\u7684\u57fa\u7840\u6a21\u578b\u3002\u672c\u6587\u5168\u9762\u6982\u8ff0\u5e76\u63a2\u8ba8\u4e86\u53ef\u914d\u7f6e\u57fa\u7840\u6a21\u578b\u7684\u6784\u5efa\u3001\u5229\u7528\u53ca\u5176\u5c40\u9650\u6027\u3002\u6211\u4eec\u9996\u5148\u5c06\u6a21\u5757\u5f62\u5f0f\u5316\u4e3a\u6d8c\u73b0\u7816\u5757\u2014\u2014\u9884\u8bad\u7ec3\u9636\u6bb5\u6d8c\u73b0\u7684\u529f\u80fd\u795e\u7ecf\u5206\u533a\uff0c\u4ee5\u53ca\u5b9a\u5236\u7816\u5757\u2014\u2014\u901a\u8fc7\u989d\u5916\u8bad\u7ec3\u6784\u5efa\u7684\u7816\u5757\uff0c\u4ee5\u589e\u5f3aLLMs\u7684\u80fd\u529b\u548c\u77e5\u8bc6\u3002\u57fa\u4e8e\u591a\u6837\u5316\u7684\u529f\u80fd\u7816\u5757\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u56db\u79cd\u9762\u5411\u7816\u5757\u7684\u64cd\u4f5c\uff1a\u68c0\u7d22\u4e0e\u8def\u7531\u3001\u5408\u5e76\u3001\u66f4\u65b0\u548c\u6269\u5c55\u3002\u8fd9\u4e9b\u64cd\u4f5c\u4f7f\u5f97LLMs\u80fd\u591f\u6839\u636e\u6307\u4ee4\u52a8\u6001\u914d\u7f6e\uff0c\u4ee5\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002\u4e3a\u9a8c\u8bc1\u6211\u4eec\u7684\u89c2\u70b9\uff0c\u6211\u4eec\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08FFN\uff09\u5c42\u9075\u5faa\u6a21\u5757\u5316\u6a21\u5f0f\uff0c\u5177\u6709\u795e\u7ecf\u5143\u7684\u529f\u80fd\u7279\u5316\u548c\u529f\u80fd\u795e\u7ecf\u5206\u533a\u3002\u6700\u540e\uff0c\u6211\u4eec\u6307\u51fa\u4e86\u51e0\u4e2a\u5f00\u653e\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u603b\u4f53\u800c\u8a00\uff0c\u672c\u6587\u65e8\u5728\u4e3a\u73b0\u6709\u7684LLM\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u65b0\u9896\u7684\u6a21\u5757\u5316\u89c6\u89d2\uff0c\u5e76\u6fc0\u53d1\u672a\u6765\u521b\u5efa\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6a21\u578b\u3002 Chaojun Xiao PDF N/A Configurable Foundation Models: Building LLMs from a Modular Perspective Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models. \u57ce\u5e02\u9a7e\u9a76\u6df7\u5408\u4eff\u771f\u5b66\u4e60\u8fd0\u52a8\u89c4\u5212\u5668 \u968f\u7740nuPlan\u548cArgoverse\u7b49\u5f00\u6e90\u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u5668\u7814\u7a76\u5728\u8fc7\u53bb\u51e0\u5e74\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u4f20\u64ad\u3002\u73b0\u6709\u7cfb\u7edf\u5728\u6a21\u4eff\u4eba\u7c7b\u9a7e\u9a76\u5458\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4fdd\u8bc1\u5b89\u5168\u95ed\u73af\u9a7e\u9a76\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u76f8\u53cd\uff0c\u57fa\u4e8e\u4f18\u5316\u7684\u89c4\u5212\u5668\u5728\u77ed\u671f\u89c4\u5212\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u5b66\u4e60\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u6280\u672f\u3002\u9996\u5148\uff0c\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8f68\u8ff9\uff0c\u7136\u540e\u7531\u57fa\u4e8e\u4f18\u5316\u7684\u7ec4\u4ef6\u8fdb\u884c\u7ec6\u5316\u3002\u8be5\u7ec4\u4ef6\u4e0d\u4ec5\u6700\u5c0f\u5316\u4e86\u8ddf\u8e2a\u8bef\u5dee\uff0c\u8fd8\u8ba1\u7b97\u51fa\u5728\u8fd0\u52a8\u5b66\u4e0a\u53ef\u884c\u4e14\u4e0e\u969c\u788d\u7269\u548c\u9053\u8def\u8fb9\u754c\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\u3002\u6211\u4eec\u7684\u6a21\u578b\u6709\u6548\u5730\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u4eba\u7c7b\u76f8\u4f3c\u6027\uff0c\u7f13\u89e3\u4e86\u8fd9\u4e9b\u76ee\u6807\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\u3002\u6211\u4eec\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u8fdb\u4e00\u6b65\u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0a\u90e8\u7f72\u6765\u5c55\u793a\u5176\u6709\u6548\u6027\u3002 Cristian Gariboldi PDF N/A Hybrid Imitation-Learning Motion Planner for Urban Driving With the release of open source datasets such as nuPlan and Argoverse, the research around learning-based planners has spread a lot in the last years. Existing systems have shown excellent capabilities in imitating the human driver behaviour, but they struggle to guarantee safe closed-loop driving. Conversely, optimization-based planners offer greater security in short-term planning scenarios. To confront this challenge, in this paper we propose a novel hybrid motion planner that integrates both learning-based and optimization-based techniques. Initially, a multilayer perceptron (MLP) generates a human-like trajectory, which is then refined by an optimization-based component. This component not only minimizes tracking errors but also computes a trajectory that is both kinematically feasible and collision-free with obstacles and road boundaries. Our model effectively balances safety and human-likeness, mitigating the trade-off inherent in these objectives. We validate our approach through simulation experiments and further demonstrate its efficacy by deploying it in real-world self-driving vehicles."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}