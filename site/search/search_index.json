{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract Molmo \u548c PixMo\uff1a\u4e3a\u6700\u5148\u8fdb\u7684\u8de8\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u5f00\u653e\u6743\u91cd\u548c\u5f00\u653e\u6570\u636e \u5f53\u4eca\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u4ecd\u7136\u662f\u4e13\u6709\u7684\u3002\u6700\u5f3a\u5927\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4ece\u4e13\u6709\u7684VLMs\u4e2d\u63d0\u53d6\u7684\u5408\u6210\u6570\u636e\u4ee5\u5b9e\u73b0\u826f\u597d\u6027\u80fd\uff0c\u5b9e\u9645\u4e0a\u662f\u5c06\u8fd9\u4e9b\u5c01\u95ed\u6a21\u578b\u63d0\u70bc\u4e3a\u5f00\u6e90\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u793e\u533a\u4ecd\u7136\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u4ece\u5934\u6784\u5efa\u9ad8\u6027\u80fdVLMs\u7684\u57fa\u7840\u77e5\u8bc6\u3002\u6211\u4eec\u63d0\u51fa\u4e86Molmo\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684VLMs\u5bb6\u65cf\uff0c\u5728\u5176\u5f00\u653e\u6027\u7c7b\u522b\u4e2d\u5904\u4e8e\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u662f\u4e00\u4e2a\u5168\u65b0\u3001\u9ad8\u5ea6\u8be6\u7ec6\u7684\u5168\u4eba\u7c7b\u6807\u6ce8\u8005\u57fa\u4e8e\u8bed\u97f3\u63cf\u8ff0\u6536\u96c6\u7684\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u652f\u6301\u5e7f\u6cdb\u7684\u7528\u6237\u4ea4\u4e92\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u5fae\u8c03\u6570\u636e\u96c6\u6df7\u5408\uff0c\u5305\u62ec\u81ea\u7136\u60c5\u5883\u4e0b\u7684\u95ee\u7b54\u548c\u521b\u65b0\u7684\u4e8c\u7ef4\u6307\u5411\u6570\u636e\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5bf9\u6a21\u578b\u67b6\u6784\u7ec6\u8282\u7684\u7cbe\u5fc3\u9009\u62e9\u3001\u7cbe\u5fc3\u8c03\u4f18\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ee5\u53ca\u6700\u5173\u952e\u7684\uff0c\u6211\u4eec\u65b0\u6536\u96c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u5c06\u88ab\u53d1\u5e03\u3002Molmo\u5bb6\u65cf\u4e2d\u6700\u597d\u768472B\u6a21\u578b\u4e0d\u4ec5\u5728\u5f00\u6e90\u6743\u91cd\u548c\u6570\u636e\u6a21\u578b\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u4e14\u5728\u5b66\u672f\u57fa\u51c6\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4e0eGPT-4o\u3001Claude 3.5\u548cGemini 1.5\u7b49\u4e13\u6709\u7cfb\u7edf\u76f8\u6bd4\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u5c06\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u53d1\u5e03\u6240\u6709\u6a21\u578b\u6743\u91cd\u3001\u6807\u6ce8\u548c\u5fae\u8c03\u6570\u636e\u4ee5\u53ca\u6e90\u4ee3\u7801\u3002\u90e8\u5206\u6a21\u578b\u6743\u91cd\u3001\u63a8\u7406\u4ee3\u7801\u548c\u6f14\u793a\u53ef\u5728https://molmo.allenai.org\u83b7\u53d6\u3002 Matt Deitke PDF N/A Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&amp;A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.   We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org. DreamWaltz-G\uff1a\u4ece\u9aa8\u67b6\u5f15\u5bfc\u76842D\u6269\u6563\u4e2d\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u76843D\u9ad8\u65af\u5934\u50cf \u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e8c\u7ef4\u6269\u6563\u6a21\u578b\u548c\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u6587\u672c\u5230\u4e09\u7ef4\u5934\u50cf\u751f\u6210\u65b9\u9762\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u751f\u6210\u80fd\u591f\u8fdb\u884c\u8868\u8fbe\u6027\u52a8\u753b\u7684\u9ad8\u8d28\u91cf\u4e09\u7ef4\u5934\u50cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DreamWaltz-G\uff0c\u4e00\u79cd\u4ece\u6587\u672c\u751f\u6210\u53ef\u52a8\u753b\u4e09\u7ef4\u5934\u50cf\u7684\u65b0\u578b\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u5728\u4e8e\u9aa8\u67b6\u5f15\u5bfc\u7684\u5206\u6570\u84b8\u998f\u548c\u6df7\u5408\u4e09\u7ef4\u9ad8\u65af\u5934\u50cf\u8868\u793a\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6240\u63d0\u51fa\u7684\u9aa8\u67b6\u5f15\u5bfc\u5206\u6570\u84b8\u998f\u5c06\u4e09\u7ef4\u4eba\u4f53\u6a21\u677f\u4e2d\u7684\u9aa8\u67b6\u63a7\u5236\u878d\u5165\u4e8c\u7ef4\u6269\u6563\u6a21\u578b\uff0c\u589e\u5f3a\u4e86SDS\u76d1\u7763\u5728\u89c6\u89d2\u548c\u4eba\u4f53\u59ff\u6001\u65b9\u9762\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u6709\u52a9\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5934\u50cf\uff0c\u7f13\u89e3\u4e86\u591a\u9762\u3001\u591a\u4f59\u80a2\u4f53\u548c\u6a21\u7cca\u7b49\u95ee\u9898\u3002\u6240\u63d0\u51fa\u7684\u6df7\u5408\u4e09\u7ef4\u9ad8\u65af\u5934\u50cf\u8868\u793a\u57fa\u4e8e\u9ad8\u6548\u7684\u4e09\u7ef4\u9ad8\u65af\u5206\u5e03\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u9690\u5f0f\u573a\u548c\u53c2\u6570\u5316\u4e09\u7ef4\u7f51\u683c\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\u3001\u7a33\u5b9a\u7684SDS\u4f18\u5316\u548c\u8868\u8fbe\u6027\u52a8\u753b\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDreamWaltz-G\u5728\u751f\u6210\u548c\u52a8\u753b\u4e09\u7ef4\u5934\u50cf\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u52a8\u753b\u8868\u8fbe\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6846\u67b6\u8fd8\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5305\u62ec\u4eba\u4f53\u89c6\u9891\u91cd\u6f14\u548c\u591a\u4e3b\u4f53\u573a\u666f\u5408\u6210\u3002 Yukun Huang PDF N/A DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition. \u5dee\u5206\u9690\u79c1\u6b63\u5219\u5316\uff1a\u901a\u8fc7\u635f\u5931\u51fd\u6570\u6b63\u5219\u5316\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u5305\u542b\u654f\u611f\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u5e94\u66b4\u9732\u8fd9\u4e9b\u6570\u636e\u96c6\u4e2d\u7684\u79c1\u4eba\u4fe1\u606f\u3002\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08DP-SGD\uff09\u8981\u6c42\u5bf9\u6807\u51c6\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7b97\u6cd5\u8fdb\u884c\u4fee\u6539\uff0c\u4ee5\u8bad\u7ec3\u65b0\u6a21\u578b\u3002\u5728\u8fd9\u7bc7\u77ed\u6587\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4ee5\u66f4\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u76f8\u540c\u7684\u76ee\u6807\u3002 Francisco Aguilera-Mart\u00ednez PDF N/A Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization Training machine learning models based on neural networks requires large datasets, which may contain sensitive information. The models, however, should not expose private information from these datasets. Differentially private SGD [DP-SGD] requires the modification of the standard stochastic gradient descent [SGD] algorithm for training new models. In this short paper, a novel regularization strategy is proposed to achieve the same goal in a more efficient manner. \u56fe\u50cf\u4e0a\u6ce8\u610f\u529b\u63d0\u793a\u7528\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b \u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76f8\u6bd4\uff0c\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u540c\u6837\u80fd\u591f\u63a5\u53d7\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u56e0\u6b64\u5c55\u73b0\u51fa\u66f4\u591a\u6709\u8da3\u7684\u6d8c\u73b0\u80fd\u529b\uff0c\u5e76\u5728\u5404\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u53d7\u5230LLMs\u4e2d\u6587\u672c\u63d0\u793a\u7684\u542f\u53d1\uff0c\u89c6\u89c9\u63d0\u793a\u5df2\u88ab\u63a2\u7d22\u7528\u4e8e\u589e\u5f3aLVLMs\u611f\u77e5\u89c6\u89c9\u4fe1\u606f\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u89c6\u89c9\u63d0\u793a\u6280\u672f\u4ec5\u5904\u7406\u89c6\u89c9\u8f93\u5165\uff0c\u800c\u672a\u8003\u8651\u6587\u672c\u67e5\u8be2\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u6839\u636e\u6587\u672c\u6307\u4ee4\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u6280\u672f\uff0c\u540d\u4e3a\u56fe\u50cf\u4e0a\u7684\u6ce8\u610f\u529b\u63d0\u793a\uff08Attention Prompting on Image\uff09\uff0c\u5b83\u4ec5\u901a\u8fc7\u5728\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u4e0a\u53e0\u52a0\u4e00\u4e2a\u6587\u672c\u67e5\u8be2\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u70ed\u56fe\uff0c\u5c31\u80fd\u6709\u6548\u63d0\u5347LVLM\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4f7f\u7528\u7c7b\u4f3cCLIP\u7684\u8f85\u52a9\u6a21\u578b\u4e3a\u8f93\u5165\u56fe\u50cf\u751f\u6210\u4e00\u4e2a\u4f9d\u8d56\u4e8e\u6587\u672c\u67e5\u8be2\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u3002\u7136\u540e\uff0c\u8be5\u70ed\u56fe\u7b80\u5355\u5730\u4e0e\u539f\u59cb\u56fe\u50cf\u7684\u50cf\u7d20\u503c\u76f8\u4e58\uff0c\u4ee5\u83b7\u5f97LVLM\u7684\u5b9e\u9645\u8f93\u5165\u56fe\u50cf\u3002\u5728\u5404\u79cd\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u4f8b\u5982\uff0c\u56fe\u50cf\u4e0a\u7684\u6ce8\u610f\u529b\u63d0\u793a\u5728MM-Vet\u548cLLaVA-Wild\u57fa\u51c6\u4e0a\u5206\u522b\u5c06LLaVA-1.5\u7684\u6027\u80fd\u63d0\u5347\u4e863.8%\u548c2.9%\u3002 Runpeng Yu PDF N/A Attention Prompting on Image for Large Vision-Language Models Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs' capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models' ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose a new prompting technique named Attention Prompting on Image, which just simply overlays a text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, Attention Prompting on Image improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively. FineZip\uff1a\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u65e0\u635f\u6587\u672c\u538b\u7f29\u4e2d\u7684\u6781\u9650 \u5c3d\u7ba1\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u5df2\u88ab\u8bc1\u660e\u4e0e\u538b\u7f29\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e76\u672a\u88ab\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edf\u4e2d\u3002\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u548c\u53d8\u538b\u5668\uff08transformer\uff09\u7684\u538b\u7f29\u6280\u672f\uff0c\u4ee5\u89e3\u7b54\u8fd9\u4e00\u95ee\u9898\u3002\u6211\u4eec\u5bf9\u6bd4\u4e86\u4f20\u7edf\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edf\u4e0e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u548cLLM\u7684\u6587\u672c\u538b\u7f29\u65b9\u6cd5\u3002\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u6781\u4f4e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6700\u8fd1\u7684\u4e00\u4e2a\u57fa\u4e8eLlama3-8B\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edfLLMZip\uff0c\u5c3d\u7ba1\u5728\u538b\u7f29\u6bd4\u4e0a\u6709\u4e86\u5de8\u5927\u63d0\u5347\uff0c\u4f46\u538b\u7f2910 MB\u7684\u6587\u672c\u9700\u89819.5\u5929\u7684\u65f6\u95f4\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FineZip\u2014\u2014\u4e00\u79cd\u65b0\u578b\u57fa\u4e8eLLM\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u5728\u7ebf\u8bb0\u5fc6\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u7684\u601d\u60f3\uff0c\u6781\u5927\u5730\u51cf\u5c11\u4e86\u538b\u7f29\u65f6\u95f4\u3002\u4e0eLLMZip\u76849.5\u5929\u76f8\u6bd4\uff0cFineZip\u53ef\u4ee5\u5728\u5927\u7ea64\u5c0f\u65f6\u5185\u5b8c\u6210\u4e0a\u8ff0\u8bed\u6599\u7684\u538b\u7f29\uff0c\u6027\u80fd\u63d0\u534754\u500d\u4e14\u8868\u73b0\u76f8\u5f53\u3002FineZip\u5728\u538b\u7f29\u6bd4\u4e0a\u5927\u5e45\u8d85\u8d8a\u4f20\u7edf\u7684\u7b97\u6cd5\u538b\u7f29\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u7ea650%\u3002\u901a\u8fc7\u8fd9\u9879\u5de5\u4f5c\uff0c\u6211\u4eec\u8fc8\u51fa\u4e86\u4f7f\u57fa\u4e8eLLM\u7684\u65e0\u635f\u6587\u672c\u538b\u7f29\u6210\u4e3a\u73b0\u5b9e\u7684\u7b2c\u4e00\u6b65\u3002\u5c3d\u7ba1FineZip\u5728\u8fd9\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4f46LLMs\u4ecd\u672a\u6210\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u538b\u7f29\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u521b\u65b0\u94fa\u5e73\u9053\u8def\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002 Fazal Mittu PDF N/A FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression While the language modeling objective has been shown to be deeply connected with compression, it is surprising that modern LLMs are not employed in practical text compression systems. In this paper, we provide an in-depth analysis of neural network and transformer-based compression techniques to answer this question. We compare traditional text compression systems with neural network and LLM-based text compression methods. Although LLM-based systems significantly outperform conventional compression methods, they are highly impractical. Specifically, LLMZip, a recent text compression system using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with huge improvements in compression ratios. To overcome this, we present FineZip - a novel LLM-based text compression system that combines ideas of online memorization and dynamic context to reduce the compression time immensely. FineZip can compress the above corpus in approximately 4 hours compared to 9.5 days, a 54 times improvement over LLMZip and comparable performance. FineZip outperforms traditional algorithmic compression methods with a large margin, improving compression ratios by approximately 50\\%. With this work, we take the first step towards making lossless text compression with LLMs a reality. While FineZip presents a significant step in that direction, LLMs are still not a viable solution for large-scale text compression. We hope our work paves the way for future research and innovation to solve this problem. \u5c06\u6bcf\u4e2a\u5e94\u7528\u7a0b\u5e8f\u8f6c\u53d8\u4e3a\u667a\u80fd\u4ee3\u7406\uff1a\u57fa\u4e8eAPI\u4f18\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u52a9\u529b\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92 \u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f7f\u5f97\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u80fd\u591f\u76f4\u63a5\u4e0e\u5e94\u7528\u7a0b\u5e8f\u7528\u6237\u754c\u9762\uff08UI\uff09\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ece\u800c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u63d0\u5347\u4ee3\u7406\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4ee3\u7406\u7531\u4e8e\u5e7f\u6cdb\u7684\u987a\u5e8fUI\u4ea4\u4e92\uff0c\u5e38\u5e38\u9762\u4e34\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u53ef\u9760\u6027\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AXIS\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u4f18\u5148\u901a\u8fc7\u5e94\u7528\u7a0b\u5e8f\u7f16\u7a0b\u63a5\u53e3\uff08APIs\uff09\u800c\u975eUI\u52a8\u4f5c\u6765\u6267\u884c\u64cd\u4f5c\u3002\u8be5\u6846\u67b6\u8fd8\u901a\u8fc7\u81ea\u52a8\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\u6765\u4fc3\u8fdbAPI\u7684\u521b\u5efa\u548c\u6269\u5c55\u3002\u6211\u4eec\u5728Office Word\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0cAXIS\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u4e8665%-70%\uff0c\u8ba4\u77e5\u8d1f\u8377\u964d\u4f4e\u4e8638%-53%\uff0c\u540c\u65f6\u4fdd\u6301\u4e8697%-98%\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u65b0\u7684\u4eba-\u4ee3\u7406-\u8ba1\u7b97\u673a\u4ea4\u4e92\uff08HACI\uff09\u6846\u67b6\u548cLLM\u65f6\u4ee3\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u5546\u7684\u5168\u65b0UI\u8bbe\u8ba1\u539f\u5219\u505a\u51fa\u4e86\u8d21\u732e\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63a2\u7d22\u4e86\u5c06\u6bcf\u4e2a\u5e94\u7528\u7a0b\u5e8f\u8f6c\u53d8\u4e3a\u4ee3\u7406\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u8fc8\u5411\u4ee5\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u64cd\u4f5c\u7cfb\u7edf\uff08Agent OS\uff09\u94fa\u5e73\u4e86\u9053\u8def\u3002 Junting Lu PDF N/A Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low reliability due to the extensive sequential UI interactions. To address this issue, we propose AXIS, a novel LLM-based agents framework prioritize actions through application programming interfaces (APIs) over UI actions. This framework also facilitates the creation and expansion of APIs through automated exploration of applications. Our experiments on Office Word demonstrate that AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans. Our work contributes to a new human-agent-computer interaction (HACI) framework and a fresh UI design principle for application providers in the era of LLMs. It also explores the possibility of turning every applications into agents, paving the way towards an agent-centric operating system (Agent OS). \u52a8\u6001\u5b66\u4e60\uff1a\u57fa\u4e8e\u52a8\u6001\u65e0\u4eba\u673a\u56e2\u961f\u7684\u65e0\u4eba\u673a\u901a\u4fe1\u7f51\u7edc\u81ea\u4e3b\u8c03\u63a7 \u65e0\u4eba\u673a\uff08UAV\uff09\u901a\u4fe1\u7f51\u7edc\uff08UCN\uff09\u662f\u672a\u6765\u79fb\u52a8\u7f51\u7edc\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u4e3a\u4e86\u5e94\u5bf9UCN\u4e2d\u7684\u52a8\u6001\u73af\u5883\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u56e0\u5176\u65e0\u9700\u73af\u5883\u6a21\u578b\u5373\u53ef\u8fdb\u884c\u81ea\u9002\u5e94\u51b3\u7b56\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8eRL\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u63a7\u5236\u7b56\u7565\u8bbe\u8ba1\uff0c\u5047\u8bbeUAV\u96c6\u5408\u662f\u56fa\u5b9a\u7684\u3002\u5f88\u5c11\u6709\u7814\u7a76\u63a2\u8ba8\u5f53\u670d\u52a1\u7684UAV\u52a8\u6001\u53d8\u5316\u65f6\uff0cUCN\u5e94\u5982\u4f55\u81ea\u9002\u5e94\u8c03\u8282\u3002\u672c\u6587\u8ba8\u8bba\u4e86\u5728\u52a8\u6001UAV\u96c6\u5408\u4e0b\u57fa\u4e8eRL\u7684\u7b56\u7565\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0UCN\u7684\u81ea\u9002\u5e94\u8c03\u8282\uff0c\u6db5\u76d6\u4e86\u4e00\u822cUCN\u4e2d\u7684\u53cd\u5e94\u6027\u7b56\u7565\u548c\u592a\u9633\u80fdUCN\u4e2d\u7684\u524d\u77bb\u6027\u7b56\u7565\u3002\u9996\u5148\u63d0\u4f9b\u4e86UCN\u548cRL\u6846\u67b6\u7684\u6982\u8ff0\u3002\u63a5\u7740\u8be6\u7ec6\u9610\u8ff0\u4e86\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\u53ca\u5176\u5173\u952e\u6311\u6218\u548c\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e9b\u6700\u8fd1\u7684\u6210\u679c\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u4ee5\u542f\u53d1\u4f7f\u7528\u4e0d\u540cRL\u7b97\u6cd5\u5904\u7406\u52a8\u6001UAV\u56e2\u961f\u7684\u521b\u65b0\u65b9\u6cd5\u3002 Ran Zhang PDF N/A Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key component in future mobile networking. To handle the dynamic environments in UCNs, reinforcement learning (RL) has been a promising solution attributed to its strong capability of adaptive decision-making free of the environment models. However, most existing RL-based research focus on control strategy design assuming a fixed set of UAVs. Few works have investigated how UCNs should be adaptively regulated when the serving UAVs change dynamically. This article discusses RL-based strategy design for adaptive UCN regulation given a dynamic UAV set, addressing both reactive strategies in general UCNs and proactive strategies in solar-powered UCNs. An overview of the UCN and the RL framework is first provided. Potential research directions with key challenges and possible solutions are then elaborated. Some of our recent works are presented as case studies to inspire innovative ways to handle dynamic UAV crew with different RL algorithms. \u6709\u9650\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\u4e2d\u5177\u6709\u4e00\u822c\u72b6\u6001\u548c\u52a8\u4f5c\u7684\u653f\u7b56\u4f18\u5316\u666f\u89c2 \u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u7b56\u7565\u4f18\u5316\u7684\u975e\u51f8\u6027\u7ed9\u7406\u89e3\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u5168\u5c40\u6536\u655b\u6027\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u5bf9\u4e8e\u4e00\u7c7b\u5177\u6709\u4e00\u822c\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u7ec4\u6613\u4e8e\u9a8c\u8bc1\u7684\u5047\u8bbe\uff0c\u4ee5\u786e\u4fdd\u7b56\u7565\u4f18\u5316\u7684Kurdyka-Lojasiewicz\uff08KL\uff09\u6761\u4ef6\u3002\u5229\u7528KL\u6761\u4ef6\uff0c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5c3d\u7ba1\u5728\u975e\u51f8\u6027\u6761\u4ef6\u4e0b\uff0c\u4ecd\u80fd\u4ee5\u975e\u6e10\u8fd1\u901f\u7387\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u7b56\u7565\u3002\u6211\u4eec\u7684\u7ed3\u679c\u5728\u5404\u79cd\u63a7\u5236\u548c\u8fd0\u8425\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u5e94\u7528\uff0c\u5305\u62ec\u71b5\u6b63\u5219\u5316\u7684\u8868\u683cMDPs\u3001\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u95ee\u9898\u3001\u968f\u673a\u5e93\u5b58\u6a21\u578b\u548c\u968f\u673a\u73b0\u91d1\u4f59\u989d\u95ee\u9898\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u901a\u8fc7\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728$\\tilde{\\mathcal{O}}(\\epsilon^{-1})$\u7684\u6837\u672c\u91cf\u548c\u5173\u4e8e\u89c4\u5212\u65f6\u57df\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u83b7\u5f97\u4e00\u4e2a$\\epsilon$-\u6700\u4f18\u7b56\u7565\u3002\u6211\u4eec\u7684\u7ed3\u679c\u5728\u6587\u732e\u4e2d\u9996\u6b21\u786e\u7acb\u4e86\u5177\u6709\u9a6c\u5c14\u53ef\u592b\u8c03\u5236\u9700\u6c42\u7684\u591a\u5143\u5e93\u5b58\u7cfb\u7edf\u548c\u968f\u673a\u73b0\u91d1\u4f59\u989d\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u6027\u3002 Xin Chen PDF N/A Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action Policy gradient methods are widely used in reinforcement learning. Yet, the nonconvexity of policy optimization imposes significant challenges in understanding the global convergence of policy gradient methods. For a class of finite-horizon Markov Decision Processes (MDPs) with general state and action spaces, we develop a framework that provides a set of easily verifiable assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy optimization. Leveraging the KL condition, policy gradient methods converge to the globally optimal policy with a non-asymptomatic rate despite nonconvexity. Our results find applications in various control and operations models, including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR) problems, stochastic inventory models, and stochastic cash balance problems, for which we show an $\\epsilon$-optimal policy can be obtained using a sample size in $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ and polynomial in terms of the planning horizon by stochastic policy gradient methods. Our result establishes the first sample complexity for multi-period inventory systems with Markov-modulated demands and stochastic cash balance problems in the literature. PACE\uff1a\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u7684\u6cdb\u5316\u4e0e\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f8\u7ed3\u5408 \u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08Parameter-Efficient Fine-Tuning, PEFT\uff09\u6709\u6548\u5730\u5c06\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u53d8\u6362\u5668\u9002\u5e94\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u7684\u4f18\u5316\u5f80\u5f80\u4ee5\u5fae\u8c03\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u727a\u7272\u4e3a\u4ee3\u4ef7\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8f83\u5c0f\u7684\u6743\u91cd\u68af\u5ea6\u8303\u6570\u548c\u8f83\u5927\u7684\u6570\u636e\u96c6\u4e0e\u6539\u8fdb\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u8054\u7cfb\u8d77\u6765\u3002\u53d7\u6b64\u8054\u7cfb\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u51cf\u5c11\u68af\u5ea6\u8303\u6570\u6765\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4f7f\u5fae\u8c03\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u9f50\uff0c\u4ee5\u4fdd\u7559\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u7b80\u5355\u7684\u5bf9\u9f50\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u68af\u5ea6\u51cf\u5c11\uff0c\u53cd\u800c\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u7206\u70b8\uff0c\u589e\u52a0\u4e86\u7ba1\u7406\u68af\u5ea6\u7684\u590d\u6742\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PACE\uff0c\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u901a\u8fc7\u4e58\u6027\u566a\u58f0\u6270\u52a8\u4ece\u9002\u914d\u5668\u4e2d\u5b66\u4e60\u5230\u7684\u7279\u5f81\uff0c\u5e76\u786e\u4fdd\u5fae\u8c03\u6a21\u578b\u5728\u4e0d\u540c\u6270\u52a8\u4e0b\u5bf9\u540c\u4e00\u6837\u672c\u4fdd\u6301\u4e00\u81f4\u6027\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cPACE\u4e0d\u4ec5\u9690\u5f0f\u5730\u5bf9\u68af\u5ea6\u8fdb\u884c\u6b63\u5219\u5316\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u9690\u5f0f\u5730\u5bf9\u9f50\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u4fdd\u7559\u77e5\u8bc6\u3002\u5b9e\u9a8c\u8bc1\u636e\u652f\u6301\u4e86\u6211\u4eec\u7684\u7406\u8bba\u3002\u5728\u56db\u4e2a\u89c6\u89c9\u9002\u5e94\u4efb\u52a1\uff08VTAB-1k\u3001FGVC\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u9886\u57df\u9002\u5e94\uff09\u4e2d\uff0cPACE\u4f18\u4e8e\u73b0\u6709\u7684PEFT\u65b9\u6cd5\u3002\u4ee3\u7801\u5c06\u5728https://github.com/MaxwellYaoNi/PACE \u63d0\u4f9b\u3002 Yao Ni PDF N/A PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision transformers to downstream tasks. However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improved model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE outperforms existing PEFT methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and domain adaptation. Code will be available at https://github.com/MaxwellYaoNi/PACE Blox-Net\uff1a\u5229\u7528VLM\u76d1\u7763\u3001\u7269\u7406\u6a21\u62df\u548c\u5177\u5907\u91cd\u7f6e\u529f\u80fd\u7684\u673a\u5668\u4eba\u8fdb\u884c\u673a\u5668\u4eba\u88c5\u914d\u7684\u751f\u6210\u5f0f\u8bbe\u8ba1 \u751f\u6210\u5f0fAI\u7cfb\u7edf\u5728\u751f\u6210\u6587\u672c\u3001\u4ee3\u7801\u548c\u56fe\u50cf\u65b9\u9762\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u53d7\u5de5\u4e1a\u9886\u57df\u201c\u9762\u5411\u88c5\u914d\u7684\u8bbe\u8ba1\u201d\u7814\u7a76\u4e30\u5bcc\u5386\u53f2\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u9879\u65b0\u95ee\u9898\uff1a\u9762\u5411\u673a\u5668\u4eba\u88c5\u914d\u7684\u751f\u6210\u5f0f\u8bbe\u8ba1\uff08Generative Design-for-Robot-Assembly\uff0c\u7b80\u79f0GDfRA\uff09\u3002\u8be5\u4efb\u52a1\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff08\u4f8b\u5982\u201c\u957f\u9888\u9e7f\u201d\uff09\u548c\u53ef\u7528\u7269\u7406\u7ec4\u4ef6\u7684\u56fe\u50cf\uff08\u59823D\u6253\u5370\u5757\uff09\u751f\u6210\u4e00\u4e2a\u88c5\u914d\u4f53\u3002\u8f93\u51fa\u7ed3\u679c\u5305\u62ec\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u7a7a\u95f4\u6392\u5217\u4ee5\u53ca\u673a\u5668\u4eba\u6784\u5efa\u8be5\u88c5\u914d\u4f53\u7684\u6307\u4ee4\u3002\u8f93\u51fa\u5fc5\u987b\u6ee1\u8db3\u4e24\u4e2a\u6761\u4ef6\uff1a1\uff09\u7c7b\u4f3c\u4e8e\u8bf7\u6c42\u7684\u5bf9\u8c61\uff1b2\uff09\u80fd\u591f\u88ab\u5e26\u6709\u5438\u76d8\u5939\u5177\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u624b\u81c2\u53ef\u9760\u5730\u88c5\u914d\u3002\u968f\u540e\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Blox-Net\uff0c\u8fd9\u662f\u4e00\u4e2aGDfRA\u7cfb\u7edf\uff0c\u5b83\u7ed3\u5408\u4e86\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6a21\u62df\u3001\u6270\u52a8\u5206\u6790\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u7269\u7406\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5df2\u6709\u7684\u6210\u719f\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4e00\u7c7bGDfRA\u95ee\u9898\uff0c\u4e14\u4ec5\u9700\u6700\u5c11\u7684\u4eba\u5de5\u76d1\u7763\u3002Blox-Net\u5728\u5176\u8bbe\u8ba1\u7684\u88c5\u914d\u4f53\u7684\u201c\u53ef\u8bc6\u522b\u6027\u201d\u65b9\u9762\u8fbe\u5230\u4e8663.5%\u7684Top-1\u51c6\u786e\u7387\uff08\u4f8b\u5982\uff0c\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u5176\u7c7b\u4f3c\u4e8e\u957f\u9888\u9e7f\uff09\u3002\u7ecf\u8fc7\u81ea\u52a8\u6270\u52a8\u91cd\u65b0\u8bbe\u8ba1\u540e\uff0c\u8fd9\u4e9b\u8bbe\u8ba1\u80fd\u591f\u88ab\u673a\u5668\u4eba\u53ef\u9760\u5730\u88c5\u914d\uff0c\u5728\u8fde\u7eed10\u6b21\u88c5\u914d\u8fed\u4ee3\u4e2d\u51e0\u4e4e\u5b8c\u7f8e\u5730\u6210\u529f\uff0c\u4ec5\u5728\u88c5\u914d\u524d\u91cd\u7f6e\u65f6\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u4ece\u6587\u672c\u8bcd\uff08\u201c\u957f\u9888\u9e7f\u201d\uff09\u5230\u53ef\u9760\u7684\u7269\u7406\u88c5\u914d\u7684\u6574\u4e2a\u8bbe\u8ba1\u8fc7\u7a0b\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002 Andrew Goldberg PDF N/A Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset Generative AI systems have shown impressive capabilities in creating text, code, and images. Inspired by the rich history of research in industrial ''Design for Assembly'', we introduce a novel problem: Generative Design-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on a natural language prompt (e.g., ''giraffe'') and an image of available physical components, such as 3D-printed blocks. The output is an assembly, a spatial arrangement of these components, and instructions for a robot to build this assembly. The output must 1) resemble the requested object and 2) be reliably assembled by a 6 DoF robot arm with a suction gripper. We then present Blox-Net, a GDfRA system that combines generative vision language models with well-established methods in computer vision, simulation, perturbation analysis, motion planning, and physical robot experimentation to solve a class of GDfRA problems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of 63.5% in the ''recognizability'' of its designed assemblies (eg, resembling giraffe as judged by a VLM). These designs, after automated perturbation redesign, were reliably assembled by a robot, achieving near-perfect success across 10 consecutive assembly iterations with human intervention only during reset prior to assembly. Surprisingly, this entire design process from textual word (''giraffe'') to reliable physical assembly is performed with zero human intervention."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}