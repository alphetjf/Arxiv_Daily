
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-papers">Arxiv Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>搭建情节与语义：一种用于长篇视频理解的新型框架</td>
<td>尽管现有研究往往将长视频视为扩展的短视频，我们提出了一种新颖的方法，更准确地反映了人类的认知。本文介绍了BREASE：用于长视频理解的桥接剧集与语义（BRidging Episodes And SEmantics）模型，该模型模拟情节记忆积累以捕捉动作序列，并通过分散在视频中的语义知识对其进行强化。我们的工作有两个关键贡献：首先，我们开发了一个情节压缩器（Episodic COmpressor，ECO），能有效地从微观到半宏观层面聚合关键表征。其次，我们提出了一种语义检索器（Semantics reTRiever，SeTR），通过关注更广泛的上下文，用语义信息增强这些聚合表征，显著降低特征维度同时保留相关宏观级别信息。大量实验表明，BREASE在零样本和全监督设置下的多个长视频理解基准测试中均达到了最先进的性能。项目页面和代码位于：https://joslefaure.github.io/assets/html/hermes.html。</td>
<td>Gueter Josmy Faure</td>
<td><a href="http://arxiv.org/pdf/2408.17443v1">PDF</a></td>
<td>N/A</td>
<td>Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding</td>
<td>While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html.</td>
</tr>
<tr>
<td>SelectTTS：基于离散单元帧选择的语音合成技术，可模拟任意人物的声音。</td>
<td>在多说话人文本到语音（TTS）中合成未见说话者的声音是一个持续的挑战。大多数多说话人TTS模型依赖于在训练过程中通过说话人条件建模来模拟说话人特征。通过这种方法对未见说话人属性进行建模，需要增加模型复杂性，这使得再现结果和改进结果变得困难。我们设计了一个简单的替代方案。我们提出了SelectTTS，一种新颖的方法，从目标说话人中选择适当的帧，并使用帧级自监督学习（SSL）特征进行解码。我们展示了这种方法可以有效地捕捉未见说话人的说话人特征，并在客观和主观指标上与其他多说话人TTS框架取得可比的结果。通过SelectTTS，我们展示了从目标说话人的语音中选择帧是实现低模型复杂性下未见说话人泛化的直接方式。我们在说话人相似性性能上超过了SOTA基线XTTS-v2和VALL-E，模型参数减少了8倍以上，训练数据减少了270倍。</td>
<td>Ismail Rasim Ulgen</td>
<td><a href="http://arxiv.org/pdf/2408.17432v1">PDF</a></td>
<td>N/A</td>
<td>SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection</td>
<td>Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data</td>
</tr>
<tr>
<td>利用大型语言模型提升多说话者自动语音识别性能</td>
<td>在对话场景中识别多个说话者的重叠语音是自动语音识别（ASR）中最具挑战性的问题之一。序列化输出训练（SOT）是一种经典的处理多说话者ASR的方法，其核心思想是根据各说话者的语音发射时间将他们的转录文本串联起来进行训练。然而，由对话中多个相关话语串联而成的SOT风格转录文本，在很大程度上依赖于对长上下文的建模。因此，相较于主要强调基于注意力编码器-解码器（AED）架构中编码器性能的传统方法，一种利用大型语言模型（LLM）并发挥预训练解码器能力的新方法可能更适合这种复杂且具有挑战性的场景。本文提出了一种基于LLM的SOT方法用于多说话者ASR，该方法利用预训练的语音编码器和LLM，并通过适当的策略在多说话者数据集上进行微调。实验结果表明，我们的方法在模拟数据集LibriMix上超越了传统的基于AED的方法，并在真实世界数据集AMI的评估集上达到了最先进的性能，超过了以往工作中使用1000倍以上监督数据训练的AED模型。</td>
<td>Mohan Shi</td>
<td><a href="http://arxiv.org/pdf/2408.17431v1">PDF</a></td>
<td>N/A</td>
<td>Advancing Multi-talker ASR Performance with Large Language Models</td>
<td>Recognizing overlapping speech from multiple speakers in conversational scenarios is one of the most challenging problem for automatic speech recognition (ASR). Serialized output training (SOT) is a classic method to address multi-talker ASR, with the idea of concatenating transcriptions from multiple speakers according to the emission times of their speech for training. However, SOT-style transcriptions, derived from concatenating multiple related utterances in a conversation, depend significantly on modeling long contexts. Therefore, compared to traditional methods that primarily emphasize encoder performance in attention-based encoder-decoder (AED) architectures, a novel approach utilizing large language models (LLMs) that leverages the capabilities of pre-trained decoders may be better suited for such complex and challenging scenarios. In this paper, we propose an LLM-based SOT approach for multi-talker ASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on multi-talker dataset using appropriate strategies. Experimental results demonstrate that our approach surpasses traditional AED-based methods on the simulated dataset LibriMix and achieves state-of-the-art performance on the evaluation set of the real-world dataset AMI, outperforming the AED model trained with 1000 times more supervised data in previous works.</td>
</tr>
<tr>
<td>使用视觉语言模型进行开放词汇时间动作定位</td>
<td>视频动作定位旨在从长视频中找出特定动作的时间点。尽管现有的基于学习的方法取得了成功，但这些方法需要对视频进行标注，带来了不小的劳动力成本。本文提出了一种基于新兴的现成视觉语言模型（VLM）的无学习、开放词汇方法。挑战在于，VLM既未设计用于处理长视频，也未专门针对寻找动作进行优化。我们通过扩展一种迭代视觉提示技术来克服这些问题。具体而言，我们将视频帧采样为带有帧索引标签的拼接图像，使VLM猜测被认为最接近动作起始/结束的帧。通过缩小采样时间窗口迭代此过程，从而找到动作起始和结束的特定帧。我们证明，这种采样技术能够产生合理的结果，展示了VLM在理解视频方面的实际扩展。示例代码可在https://microsoft.github.io/VLM-Video-Action-Localization/获取。</td>
<td>Naoki Wake</td>
<td><a href="http://arxiv.org/pdf/2408.17422v2">PDF</a></td>
<td>N/A</td>
<td>Open-vocabulary Temporal Action Localization using VLMs</td>
<td>Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. A sample code is available at https://microsoft.github.io/VLM-Video-Action-Localization/.</td>
</tr>
<tr>
<td>获取功能需求启发的途径：应用商店与基于大型语言模型的方法</td>
<td>过去十年间，受应用商店（AppStore）启发的需求获取方法已被证明极为有益。开发者通常通过研究竞争对手的应用程序来汲取新功能的灵感。随着生成式人工智能的发展，近期的研究展示了基于大型语言模型（LLM）的需求获取潜力。LLM能够在此过程中提供新功能创意的启发。尽管这两种方法在实践中日益流行，但关于它们差异的深入见解却相对匮乏。我们报告了一项比较研究，针对将功能细化为子功能的问题，探讨了基于AppStore和LLM的方法。通过手动分析从这两种方法推荐的1200个子功能，我们揭示了它们的益处、挑战及关键差异。虽然两种方法推荐的子功能都具有高度相关性和清晰的描述，但LLM在处理新颖、未见的应用领域方面似乎更具优势。此外，一些推荐的特性存在虚构且可行性不明确的情况，这强调了在需求获取过程中人类分析师的重要性。</td>
<td>Jialiang Wei</td>
<td><a href="http://arxiv.org/pdf/2408.17404v1">PDF</a></td>
<td>N/A</td>
<td>Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach</td>
<td>Over the past decade, app store (AppStore)-inspired requirements elicitation has proven to be highly beneficial. Developers often explore competitors' apps to gather inspiration for new features. With the advance of Generative AI, recent studies have demonstrated the potential of large language model (LLM)-inspired requirements elicitation. LLMs can assist in this process by providing inspiration for new feature ideas. While both approaches are gaining popularity in practice, there is a lack of insight into their differences. We report on a comparative study between AppStore- and LLM-based approaches for refining features into sub-features. By manually analyzing 1,200 sub-features recommended from both approaches, we identified their benefits, challenges, and key differences. While both approaches recommend highly relevant sub-features with clear descriptions, LLMs seem more powerful particularly concerning novel unseen app scopes. Moreover, some recommended features are imaginary with unclear feasibility, which suggests the importance of a human-analyst in the elicitation loop.</td>
</tr>
<tr>
<td>探索解释内容与格式对用户理解和信任的影响</td>
<td>近年来，为了解释“黑箱”AI模型的输出，人们引入了各种方法。然而，用户是否真正理解并信任这些解释尚不清楚。在本文中，我们聚焦于评估癌症风险的回归工具的解释，并考察解释的内容和格式对以用户为中心的理解和信任指标的影响。就内容而言，我们实验了两种解释方法：流行的SHAP，基于博弈论概念，可能对普通用户来说较为复杂；以及基于特征遮挡的occlusion-1，可能更易于理解。就格式而言，我们将SHAP解释以传统图表形式（SC）呈现，而occlusion-1解释则以图表（OC）和文本（OT）形式呈现，其简单性也适合这种呈现方式。实验包括对参与者进行用户研究，询问他们对回归工具输出解释的主观和客观理解和信任，参与者分为两个不同专业水平（一般人群和具有一定医学训练的人）。在两项研究中，我们发现，当基于内容进行比较时，总体上用户对occlusion-1解释的主观理解和信任明显优于SHAP解释。然而，在控制格式进行直接比较时，大多数情况下只发现了OT解释优于SC解释的证据，这表明occlusion-1解释优于SHAP解释可能是因为用户偏好文本而非图表形式的解释。最后，我们没有发现解释类型在客观理解方面存在差异的证据。因此，总体而言，解释的内容和格式的选择需要仔细考虑，因为在某些情况下，格式而非内容可能在提升用户体验方面起关键作用。</td>
<td>Antonio Rago</td>
<td><a href="http://arxiv.org/pdf/2408.17401v1">PDF</a></td>
<td>N/A</td>
<td>Exploring the Effect of Explanation Content and Format on User Comprehension and Trust</td>
<td>In recent years, various methods have been introduced for explaining the outputs of "black-box" AI models. However, it is not well understood whether users actually comprehend and trust these explanations. In this paper, we focus on explanations for a regression tool for assessing cancer risk and examine the effect of the explanations' content and format on the user-centric metrics of comprehension and trust. Regarding content, we experiment with two explanation methods: the popular SHAP, based on game-theoretic notions and thus potentially complex for everyday users to comprehend, and occlusion-1, based on feature occlusion which may be more comprehensible. Regarding format, we present SHAP explanations as charts (SC), as is conventional, and occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature also lends itself. The experiments amount to user studies questioning participants, with two different levels of expertise (the general population and those with some medical training), on their subjective and objective comprehension of and trust in explanations for the outputs of the regression tool. In both studies we found a clear preference in terms of subjective comprehension and trust for occlusion-1 over SHAP explanations in general, when comparing based on content. However, direct comparisons of explanations when controlling for format only revealed evidence for OT over SC explanations in most cases, suggesting that the dominance of occlusion-1 over SHAP explanations may be driven by a preference for text over charts as explanations. Finally, we found no evidence of a difference between the explanation types in terms of objective comprehension. Thus overall, the choice of the content and format of explanations needs careful attention, since in some contexts format, rather than content, may play the critical role in improving user experience.</td>
</tr>
<tr>
<td>公平感知图模型估计</td>
<td>本文探讨了图模型（GMs）估计中的公平性问题，特别是高斯模型、协方差模型和伊辛模型。这些模型在理解高维数据中的复杂关系方面发挥着至关重要的作用。然而，标准的图模型可能导致有偏见的结果，尤其是在基础数据涉及敏感特征或受保护群体时。为了解决这一问题，我们引入了一个全面的框架，旨在减少与受保护属性相关的图模型估计中的偏见。我们的方法涉及将成对图差异误差和一个定制的损失函数整合到一个非光滑多目标优化问题中，力求在保持图模型有效性的同时，实现不同敏感群体间的公平性。对合成数据集和现实世界数据集的实验评估表明，我们的框架能有效减轻偏见，同时不损害图模型的性能。</td>
<td>Zhuoping Zhou</td>
<td><a href="http://arxiv.org/pdf/2408.17396v1">PDF</a></td>
<td>N/A</td>
<td>Fairness-Aware Estimation of Graphical Models</td>
<td>This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance.</td>
</tr>
<tr>
<td>使用神经切线集合进行持续学习</td>
<td>持续学习的一种自然策略是权衡一组固定函数的贝叶斯集成。这表明，如果一个（单一的）神经网络可以被解释为一个集成，那么可以设计出有效的算法，这些算法可以在不遗忘的情况下进行学习。为了实现这一可能性，我们观察到，具有N个参数的神经网络分类器可以被解释为N个分类器的加权集成，并且在懒惰机制的极限中，这些分类器在整个学习过程中是固定的。我们称这些分类器为神经切线专家，并证明它们输出的标签概率分布是有效的。然后，我们推导出每个专家在给定过去数据的情况下的似然性和后验概率。令人惊讶的是，我们发现这些专家的后验更新等同于网络权重上随机梯度下降（SGD）的缩放和投影形式。在远离懒惰机制的情况下，网络可以被视为随时间改进的适应性专家的集合。这些结果为神经网络提供了一种新的解释，即作为专家的贝叶斯集成，为理解和减轻持续学习环境中的灾难性遗忘提供了一个原则性的框架。</td>
<td>Ari S. Benjamin</td>
<td><a href="http://arxiv.org/pdf/2408.17394v1">PDF</a></td>
<td>N/A</td>
<td>Continual learning with the neural tangent ensemble</td>
<td>A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We term these classifiers the neural tangent experts and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly, we learn that the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings.</td>
</tr>
<tr>
<td>非凸两阶段随机优化问题的贝叶斯优化</td>
<td>贝叶斯优化是一种解决昂贵、黑箱优化问题的样本高效方法。随机规划关注于在不确定性下的优化，通常情况下，平均性能是人们关心的量。在两阶段问题的第一阶段，必须在这种不确定性面前做出即刻决策，而在第二阶段，则在不确定性解决后做出等待观察决策。许多随机规划方法假设目标评估成本低廉且为线性或凸性。在这项工作中，我们将贝叶斯优化应用于解决评估成本高昂的非凸两阶段随机规划问题。我们构建了一个基于知识梯度的获取函数，以联合优化第一和第二阶段的变量，确立了渐近一致性的保证，并提供了计算效率高的近似。我们展示了与另一种我们构建的、在两种变量类型之间交替关注的方法相比拟的实证结果，以及优于标准、朴素的两步基准的实证结果。我们表明，变量类型之间的维度差异和长度尺度可能导致两步算法效率低下，而联合和交替获取函数在所有测试问题中表现良好。实验在合成和现实世界的例子上进行。</td>
<td>Jack M. Buckingham</td>
<td><a href="http://arxiv.org/pdf/2408.17387v1">PDF</a></td>
<td>N/A</td>
<td>Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization Problems</td>
<td>Bayesian optimization is a sample-efficient method for solving expensive, black-box optimization problems. Stochastic programming concerns optimization under uncertainty where, typically, average performance is the quantity of interest. In the first stage of a two-stage problem, here-and-now decisions must be made in the face of this uncertainty, while in the second stage, wait-and-see decisions are made after the uncertainty has been resolved. Many methods in stochastic programming assume that the objective is cheap to evaluate and linear or convex. In this work, we apply Bayesian optimization to solve non-convex, two-stage stochastic programs which are expensive to evaluate. We formulate a knowledge-gradient-based acquisition function to jointly optimize the first- and second-stage variables, establish a guarantee of asymptotic consistency and provide a computationally efficient approximation. We demonstrate comparable empirical results to an alternative we formulate which alternates its focus between the two variable types, and superior empirical results over the standard, naive, two-step benchmark. We show that differences in the dimension and length scales between the variable types can lead to inefficiencies of the two-step algorithm, while the joint and alternating acquisition functions perform well in all problems tested. Experiments are conducted on both synthetic and real-world examples.</td>
</tr>
<tr>
<td>LASSO-MOGAT：一种用于癌症分类的多组学图注意力框架</td>
<td>近年来，将机器学习方法应用于分析基因表达模式的变化已成为癌症研究中的一种强大手段，加深了我们对癌症发展和进展背后分子机制的理解。结合基因表达数据与其他类型的组学数据，已被众多研究证实能提升癌症分类效果。尽管取得了这些进展，有效整合高维多组学数据并捕捉不同生物层之间的复杂关系仍然是一个挑战。本文介绍了LASSO-MOGAT（LASSO-多组学门控注意力），这是一种新颖的基于图的深度学习框架，它整合了信使RNA、微RNA和DNA甲基化数据，用于分类31种癌症类型。利用LIMMA和LASSO回归进行差异表达分析和特征选择，并借助图注意力网络（GATs）纳入蛋白质-蛋白质相互作用（PPI）网络，LASSO-MOGAT有效地捕捉了多组学数据中的复杂关系。通过五折交叉验证的实验验证表明，该方法具有高精度和可靠性，并能提供对癌症分子机制的全面洞察。所提出的基于蛋白质-蛋白质相互作用的图注意力架构对图中边的注意力系数的计算，被证明对于识别癌症分类中多组学数据的协同效应是有益的。</td>
<td>Fadi Alharbi</td>
<td><a href="http://arxiv.org/pdf/2408.17384v1">PDF</a></td>
<td>N/A</td>
<td>LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer Classification</td>
<td>The application of machine learning methods to analyze changes in gene expression patterns has recently emerged as a powerful approach in cancer research, enhancing our understanding of the molecular mechanisms underpinning cancer development and progression. Combining gene expression data with other types of omics data has been reported by numerous works to improve cancer classification outcomes. Despite these advances, effectively integrating high-dimensional multi-omics data and capturing the complex relationships across different biological layers remains challenging. This paper introduces LASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep learning framework that integrates messenger RNA, microRNA, and DNA methylation data to classify 31 cancer types. Utilizing differential expression analysis with LIMMA and LASSO regression for feature selection, and leveraging Graph Attention Networks (GATs) to incorporate protein-protein interaction (PPI) networks, LASSO-MOGAT effectively captures intricate relationships within multi-omics data. Experimental validation using five-fold cross-validation demonstrates the method's precision, reliability, and capacity for providing comprehensive insights into cancer molecular mechanisms. The computation of attention coefficients for the edges in the graph by the proposed graph-attention architecture based on protein-protein interactions proved beneficial for identifying synergies in multi-omics data for cancer classification.</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>