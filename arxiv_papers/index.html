
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-papers">Arxiv Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>连接片段与语义：一种理解长篇视频的新框架</td>
<td>尽管现有研究常将长视频视为扩展的短视频，我们提出了一种更准确反映人类认知的新方法。本文介绍了BREASE：一种用于长视频理解的模型，通过模拟情景记忆的积累来捕捉动作序列，并结合视频中分散的语义知识对其进行强化。我们的工作做出了两个关键贡献：首先，我们开发了情景压缩器（ECO），它能高效地从微观到半宏观层面聚合关键表征。其次，我们提出了语义检索器（SeTR），通过聚焦于更广泛上下文来增强这些聚合表征的语义信息，显著降低特征维度同时保留相关的宏观级别信息。广泛的实验表明，BREASE在零样本和完全监督设置下的多个长视频理解基准测试中达到了最先进的性能。项目页面和代码位于：https://joslefaure.github.io/assets/html/hermes.html。</td>
<td>Gueter Josmy Faure</td>
<td><a href="http://arxiv.org/pdf/2408.17443v1">PDF</a></td>
<td>N/A</td>
<td>Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding</td>
<td>While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html.</td>
</tr>
<tr>
<td>SelectTTS：基于离散单元帧选择的语音合成技术，可模拟任何人的声音</td>
<td>合成未见说话者的声音是多说话者文本到语音（TTS）中一个持续的挑战。大多数多说话者TTS模型依赖于在训练期间通过说话者调节来建模说话者特征。通过这种方法对未见说话者属性进行建模，需要增加模型复杂度，这使得再现结果和改进结果变得具有挑战性。我们设计了一个简单的替代方案。我们提出了SelectTTS，一种新颖的方法，从目标说话者中选择合适的帧，并使用帧级自监督学习（SSL）特征进行解码。我们展示了这种方法可以有效地捕捉未见说话者的说话者特征，并在客观和主观指标上与其他多说话者TTS框架取得可比的结果。通过SelectTTS，我们展示了从目标说话者的语音中选择帧是实现未见说话者泛化的直接方式，且模型复杂度较低。我们在说话者相似性性能上优于SOTA基线XTTS-v2和VALL-E，模型参数减少了8倍以上，训练数据减少了270倍。</td>
<td>Ismail Rasim Ulgen</td>
<td><a href="http://arxiv.org/pdf/2408.17432v1">PDF</a></td>
<td>N/A</td>
<td>SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection</td>
<td>Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data</td>
</tr>
<tr>
<td>利用大型语言模型提升多说话人自动语音识别性能</td>
<td>在对话场景中识别来自多个说话者的重叠语音是自动语音识别（ASR）中最具挑战性的问题之一。序列化输出训练（SOT）是一种经典的方法来解决多说话者ASR问题，其思路是根据多个说话者的语音发射时间将他们的转录文本串联起来进行训练。然而，源自对话中多个相关话语串联的SOT风格转录文本，在很大程度上依赖于对长上下文的建模。因此，与主要强调基于注意力的编码器-解码器（AED）架构中编码器性能的传统方法相比，利用大型语言模型（LLMs）并借助预训练解码器能力的新方法可能更适合这种复杂且具有挑战性的场景。在本文中，我们提出了一种基于LLM的SOT方法用于多说话者ASR，利用预训练的语音编码器和LLM，通过适当的策略对多说话者数据集进行微调。实验结果表明，我们的方法在模拟数据集LibriMix上超越了传统的基于AED的方法，并在真实世界数据集AMI的评估集上达到了最先进的性能，超过了先前工作中使用1000倍以上监督数据训练的AED模型。</td>
<td>Mohan Shi</td>
<td><a href="http://arxiv.org/pdf/2408.17431v1">PDF</a></td>
<td>N/A</td>
<td>Advancing Multi-talker ASR Performance with Large Language Models</td>
<td>Recognizing overlapping speech from multiple speakers in conversational scenarios is one of the most challenging problem for automatic speech recognition (ASR). Serialized output training (SOT) is a classic method to address multi-talker ASR, with the idea of concatenating transcriptions from multiple speakers according to the emission times of their speech for training. However, SOT-style transcriptions, derived from concatenating multiple related utterances in a conversation, depend significantly on modeling long contexts. Therefore, compared to traditional methods that primarily emphasize encoder performance in attention-based encoder-decoder (AED) architectures, a novel approach utilizing large language models (LLMs) that leverages the capabilities of pre-trained decoders may be better suited for such complex and challenging scenarios. In this paper, we propose an LLM-based SOT approach for multi-talker ASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on multi-talker dataset using appropriate strategies. Experimental results demonstrate that our approach surpasses traditional AED-based methods on the simulated dataset LibriMix and achieves state-of-the-art performance on the evaluation set of the real-world dataset AMI, outperforming the AED model trained with 1000 times more supervised data in previous works.</td>
</tr>
<tr>
<td>使用视觉语言模型进行开放词汇时间动作定位</td>
<td>视频动作定位旨在从长视频中找出特定动作的时间点。尽管现有的基于学习的方法取得了成功，但这些方法需要对视频进行标注，带来了可观的劳动力成本。本文提出了一种无需学习、开放词汇的方法，基于新兴的现成视觉语言模型（VLM）。挑战在于，VLM既不是设计来处理长视频的，也不是专为寻找动作而定制的。我们通过扩展一种迭代的视觉提示技术来克服这些问题。具体而言，我们将视频帧采样成带有帧索引标签的拼接图像，让VLM猜测被认为是动作开始/结束最接近的帧。通过缩小采样时间窗口来迭代这一过程，最终找到动作开始和结束的特定帧。我们证明，这种采样技术能产生合理的结果，展示了VLM在理解视频方面的实际扩展。示例代码可在 https://microsoft.github.io/VLM-Video-Action-Localization/ 获取。</td>
<td>Naoki Wake</td>
<td><a href="http://arxiv.org/pdf/2408.17422v2">PDF</a></td>
<td>N/A</td>
<td>Open-vocabulary Temporal Action Localization using VLMs</td>
<td>Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. A sample code is available at https://microsoft.github.io/VLM-Video-Action-Localization/.</td>
</tr>
<tr>
<td>获取功能启发：应用商店与基于大型语言模型的方法</td>
<td>在过去十年中，受应用商店（AppStore）启发的需求获取已被证明非常有益。开发者通常会研究竞争对手的应用程序，以收集新功能的灵感。随着生成式人工智能的发展，最近的研究表明，受大型语言模型（LLM）启发的需求获取具有巨大潜力。LLM可以在这个过程中提供新功能创意的灵感。尽管这两种方法在实践中越来越受欢迎，但人们对其差异的了解甚少。我们报告了一项比较研究，对比了基于AppStore和LLM的方法在将功能细化为子功能方面的效果。通过手动分析从这两种方法推荐的1200个子功能，我们识别了它们的优点、挑战和关键差异。虽然这两种方法推荐的子功能都具有高度相关性和清晰的描述，但LLM在处理新颖、未见过的应用范围方面似乎更具优势。此外，一些推荐的特性可能是虚构的，可行性不明确，这表明在需求获取过程中人类分析师的重要性。</td>
<td>Jialiang Wei</td>
<td><a href="http://arxiv.org/pdf/2408.17404v1">PDF</a></td>
<td>N/A</td>
<td>Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach</td>
<td>Over the past decade, app store (AppStore)-inspired requirements elicitation has proven to be highly beneficial. Developers often explore competitors' apps to gather inspiration for new features. With the advance of Generative AI, recent studies have demonstrated the potential of large language model (LLM)-inspired requirements elicitation. LLMs can assist in this process by providing inspiration for new feature ideas. While both approaches are gaining popularity in practice, there is a lack of insight into their differences. We report on a comparative study between AppStore- and LLM-based approaches for refining features into sub-features. By manually analyzing 1,200 sub-features recommended from both approaches, we identified their benefits, challenges, and key differences. While both approaches recommend highly relevant sub-features with clear descriptions, LLMs seem more powerful particularly concerning novel unseen app scopes. Moreover, some recommended features are imaginary with unclear feasibility, which suggests the importance of a human-analyst in the elicitation loop.</td>
</tr>
<tr>
<td>探索解释内容与格式对用户理解和信任的影响</td>
<td>近年来，多种方法被提出用于解释“黑箱”AI模型的输出结果。然而，用户是否真正理解并信任这些解释尚不明确。本文聚焦于一种评估癌症风险的回归工具的解释，并探讨了解释内容和格式对用户理解度和信任度的影响。在内容方面，我们实验了两种解释方法：流行的SHAP，基于博弈论概念，可能对普通用户来说较为复杂；以及基于特征遮挡的occlusion-1，可能更易于理解。在格式方面，我们将SHAP解释以图表形式（SC）呈现，这是常规做法，而occlusion-1解释则同时以图表（OC）和文本（OT）形式展示，其简单性也适合这种呈现方式。实验涉及用户研究，向两类不同专业水平的参与者（普通人群和具有一定医学背景的人）询问他们对回归工具输出解释的主观和客观理解及信任程度。在两项研究中，我们发现，基于内容比较时，用户在主观理解度和信任度上普遍偏好occlusion-1解释而非SHAP解释。然而，在控制格式进行直接比较时，大多数情况下仅显示出OT解释优于SC解释，这表明occlusion-1对SHAP解释的优势可能源于用户对文本解释的偏好而非图表。最后，我们在客观理解度方面未发现解释类型间的差异。因此，总体而言，选择解释的内容和格式需谨慎考虑，因为在某些情境下，格式而非内容可能对提升用户体验起关键作用。</td>
<td>Antonio Rago</td>
<td><a href="http://arxiv.org/pdf/2408.17401v1">PDF</a></td>
<td>N/A</td>
<td>Exploring the Effect of Explanation Content and Format on User Comprehension and Trust</td>
<td>In recent years, various methods have been introduced for explaining the outputs of "black-box" AI models. However, it is not well understood whether users actually comprehend and trust these explanations. In this paper, we focus on explanations for a regression tool for assessing cancer risk and examine the effect of the explanations' content and format on the user-centric metrics of comprehension and trust. Regarding content, we experiment with two explanation methods: the popular SHAP, based on game-theoretic notions and thus potentially complex for everyday users to comprehend, and occlusion-1, based on feature occlusion which may be more comprehensible. Regarding format, we present SHAP explanations as charts (SC), as is conventional, and occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature also lends itself. The experiments amount to user studies questioning participants, with two different levels of expertise (the general population and those with some medical training), on their subjective and objective comprehension of and trust in explanations for the outputs of the regression tool. In both studies we found a clear preference in terms of subjective comprehension and trust for occlusion-1 over SHAP explanations in general, when comparing based on content. However, direct comparisons of explanations when controlling for format only revealed evidence for OT over SC explanations in most cases, suggesting that the dominance of occlusion-1 over SHAP explanations may be driven by a preference for text over charts as explanations. Finally, we found no evidence of a difference between the explanation types in terms of objective comprehension. Thus overall, the choice of the content and format of explanations needs careful attention, since in some contexts format, rather than content, may play the critical role in improving user experience.</td>
</tr>
<tr>
<td>公平感知图模型估计</td>
<td>本文探讨了图模型（尤其是高斯模型、协方差模型和伊辛模型）估计中的公平性问题。这些模型在理解高维数据中的复杂关系方面发挥着至关重要的作用。然而，标准的图模型可能导致有偏差的结果，特别是在基础数据涉及敏感特征或受保护群体时。为解决这一问题，我们引入了一个全面的框架，旨在减少与受保护属性相关的图模型估计中的偏差。我们的方法将成对图差异误差和定制损失函数整合到一个非光滑多目标优化问题中，力求在不同敏感群体间实现公平，同时保持图模型的有效性。对合成数据集和真实世界数据集的实验评估表明，我们的框架能有效减少偏差，且不会削弱图模型的性能。</td>
<td>Zhuoping Zhou</td>
<td><a href="http://arxiv.org/pdf/2408.17396v1">PDF</a></td>
<td>N/A</td>
<td>Fairness-Aware Estimation of Graphical Models</td>
<td>This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance.</td>
</tr>
<tr>
<td>神经切线系综的持续学习</td>
<td>持续学习的一种自然策略是权衡一组固定函数的贝叶斯集成。这表明，如果一个（单一的）神经网络可以被解释为一个集成，那么可以设计出有效的算法，这些算法在学习过程中不会遗忘。为了实现这一可能性，我们观察到，具有N个参数的神经网络分类器可以被解释为N个分类器的加权集成，并且在懒惰机制的极限下，这些分类器在整个学习过程中是固定的。我们称这些分类器为神经切线专家，并证明它们输出的标签概率分布是有效的。然后，我们推导出每个专家在给定过去数据情况下的似然和后验概率。令人惊讶的是，我们发现这些专家的后验更新等同于网络权重的缩放和投影形式的随机梯度下降（SGD）。在懒惰机制之外，网络可以被视为随时间改进的适应性专家的集成。这些结果为神经网络提供了一种新的解释，即作为专家的贝叶斯集成，为理解和缓解持续学习环境中的灾难性遗忘提供了一个有原则的框架。</td>
<td>Ari S. Benjamin</td>
<td><a href="http://arxiv.org/pdf/2408.17394v1">PDF</a></td>
<td>N/A</td>
<td>Continual learning with the neural tangent ensemble</td>
<td>A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We term these classifiers the neural tangent experts and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly, we learn that the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings.</td>
</tr>
<tr>
<td>非凸两阶段随机优化问题的贝叶斯优化</td>
<td>贝叶斯优化是一种高效采样的方法，用于解决成本高昂、黑箱优化问题。随机规划关注在不确定性下的优化，通常情况下，关注的是平均性能。在两阶段问题的第一阶段，必须在面对这种不确定性时做出“此时此地”的决策；而在第二阶段，则在不确定性解决后做出“观望等待”的决策。许多随机规划方法假设目标函数易于评估且为线性或凸函数。在这项工作中，我们应用贝叶斯优化来解决评估成本高昂的非凸两阶段随机规划问题。我们提出了一种基于知识梯度的采集函数，用于联合优化第一阶段和第二阶段的变量，建立了渐近一致性的保证，并提供了计算效率高的近似方法。我们展示了与另一种我们提出的方法相比具有相当的实证结果，后者在两种变量类型之间交替其关注点，并且优于标准的、朴素的两步基准方法。我们表明，变量类型之间在维度和大尺度上的差异可能导致两步算法效率低下，而联合和交替采集函数在所有测试问题中表现良好。实验在合成和真实世界的例子上进行。</td>
<td>Jack M. Buckingham</td>
<td><a href="http://arxiv.org/pdf/2408.17387v1">PDF</a></td>
<td>N/A</td>
<td>Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization Problems</td>
<td>Bayesian optimization is a sample-efficient method for solving expensive, black-box optimization problems. Stochastic programming concerns optimization under uncertainty where, typically, average performance is the quantity of interest. In the first stage of a two-stage problem, here-and-now decisions must be made in the face of this uncertainty, while in the second stage, wait-and-see decisions are made after the uncertainty has been resolved. Many methods in stochastic programming assume that the objective is cheap to evaluate and linear or convex. In this work, we apply Bayesian optimization to solve non-convex, two-stage stochastic programs which are expensive to evaluate. We formulate a knowledge-gradient-based acquisition function to jointly optimize the first- and second-stage variables, establish a guarantee of asymptotic consistency and provide a computationally efficient approximation. We demonstrate comparable empirical results to an alternative we formulate which alternates its focus between the two variable types, and superior empirical results over the standard, naive, two-step benchmark. We show that differences in the dimension and length scales between the variable types can lead to inefficiencies of the two-step algorithm, while the joint and alternating acquisition functions perform well in all problems tested. Experiments are conducted on both synthetic and real-world examples.</td>
</tr>
<tr>
<td>LASSO-MOGAT：一种用于癌症分类的多组学图注意力框架</td>
<td>将机器学习方法应用于分析基因表达模式的变化，近年来在癌症研究中崭露头角，成为一种强大的手段，加深了我们对癌症发展和进展背后分子机制的理解。结合基因表达数据与其他类型的组学数据，已被众多研究证实能提升癌症分类的效果。尽管取得了这些进展，有效整合高维多组学数据并捕捉不同生物层级间的复杂关系仍颇具挑战。本文介绍了LASSO-MOGAT（LASSO-多组学门控注意力），一种新颖的基于图的深度学习框架，该框架整合了信使RNA、微RNA及DNA甲基化数据，用以分类31种癌症类型。通过利用LIMMA进行差异表达分析及LASSO回归进行特征选择，并借助图注意力网络（GATs）纳入蛋白质-蛋白质相互作用（PPI）网络，LASSO-MOGAT有效地捕捉了多组学数据中的复杂关系。采用五折交叉验证的实验验证表明，该方法具有高精度、可靠性和提供深入癌症分子机制全面见解的能力。基于蛋白质-蛋白质相互作用的图注意力架构所计算的图中边的注意力系数，被证实有助于识别多组学数据中癌症分类的协同效应。</td>
<td>Fadi Alharbi</td>
<td><a href="http://arxiv.org/pdf/2408.17384v1">PDF</a></td>
<td>N/A</td>
<td>LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer Classification</td>
<td>The application of machine learning methods to analyze changes in gene expression patterns has recently emerged as a powerful approach in cancer research, enhancing our understanding of the molecular mechanisms underpinning cancer development and progression. Combining gene expression data with other types of omics data has been reported by numerous works to improve cancer classification outcomes. Despite these advances, effectively integrating high-dimensional multi-omics data and capturing the complex relationships across different biological layers remains challenging. This paper introduces LASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep learning framework that integrates messenger RNA, microRNA, and DNA methylation data to classify 31 cancer types. Utilizing differential expression analysis with LIMMA and LASSO regression for feature selection, and leveraging Graph Attention Networks (GATs) to incorporate protein-protein interaction (PPI) networks, LASSO-MOGAT effectively captures intricate relationships within multi-omics data. Experimental validation using five-fold cross-validation demonstrates the method's precision, reliability, and capacity for providing comprehensive insights into cancer molecular mechanisms. The computation of attention coefficients for the edges in the graph by the proposed graph-attention architecture based on protein-protein interactions proved beneficial for identifying synergies in multi-omics data for cancer classification.</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>