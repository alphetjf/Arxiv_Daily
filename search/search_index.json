{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u4f7f\u7528\u57fa\u4e8eSAM2\u7684\u8ddf\u8e2a\u8fdb\u884c\u5728\u7ebf\u8f74\u4f30\u8ba1\u7684\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c \u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u9700\u8981\u7cbe\u786e\u7684\u7269\u4f53\u4ea4\u4e92\uff0c\u5176\u4e2d\u7269\u4f53\u7684\u8f74\u5fc5\u987b\u4ed4\u7ec6\u8003\u8651\u3002\u5148\u524d\u7684\u7814\u7a76\u91c7\u7528\u4e86\u4ea4\u4e92\u611f\u77e5\u6765\u64cd\u4f5c\u94f0\u63a5\u7269\u4f53\uff0c\u4f46\u901a\u5e38\u5f00\u73af\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u4ea4\u4e92\u52a8\u529b\u5b66\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u7ba1\u9053\uff0c\u5c06\u4ea4\u4e92\u611f\u77e5\u4e0e\u4ece\u5206\u5272\u76843D\u70b9\u4e91\u4e2d\u5728\u7ebf\u8f74\u4f30\u8ba1\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4efb\u4f55\u4ea4\u4e92\u611f\u77e5\u6280\u672f\u4f5c\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u5f15\u53d1\u8f7b\u5fae\u7684\u7269\u4f53\u79fb\u52a8\u6765\u751f\u6210\u52a8\u6001\u573a\u666f\u7684\u70b9\u4e91\u5e27\u3002\u7136\u540e\u4f7f\u7528Segment Anything Model 2\uff08SAM2\uff09\u5bf9\u8fd9\u4e9b\u70b9\u4e91\u8fdb\u884c\u5206\u5272\uff0c\u4e4b\u540e\u5bf9\u7269\u4f53\u7684\u79fb\u52a8\u90e8\u5206\u8fdb\u884c\u63a9\u7801\u5904\u7406\uff0c\u4ee5\u8fdb\u884c\u7cbe\u786e\u7684\u5728\u7ebf\u8f74\u4f30\u8ba1\uff0c\u6307\u5bfc\u540e\u7eed\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6d89\u53ca\u94f0\u63a5\u7269\u4f53\u7684\u64cd\u4f5c\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u8f74\u63a7\u5236\u7684\u4efb\u52a1\u4e2d\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://hytidel.github.io/video-tracking-for-axis-estimation/\u3002 Xi Wang PDF N/A Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking Articulated object manipulation requires precise object interaction, where the object's axis must be carefully considered. Previous research employed interactive perception for manipulating articulated objects, but typically, open-loop approaches often suffer from overlooking the interaction dynamics. To address this limitation, we present a closed-loop pipeline integrating interactive perception with online axis estimation from segmented 3D point clouds. Our method leverages any interactive perception technique as a foundation for interactive perception, inducing slight object movement to generate point cloud frames of the evolving dynamic scene. These point clouds are then segmented using Segment Anything Model 2 (SAM2), after which the moving part of the object is masked for accurate motion online axis estimation, guiding subsequent robotic actions. Our approach significantly enhances the precision and efficiency of manipulation tasks involving articulated objects. Experiments in simulated environments demonstrate that our method outperforms baseline approaches, especially in tasks that demand precise axis-based control. Project Page: https://hytidel.github.io/video-tracking-for-axis-estimation/. Gen2Act\uff1a\u65b0\u9896\u573a\u666f\u4e2d\u7684\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u52a9\u529b\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c \u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5982\u4f55\u63a8\u5e7f\u5230\u6d89\u53ca\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u578b\u548c\u65b0\u52a8\u4f5c\u7684\u65b0\u4efb\u52a1\uff1f\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u901a\u8fc7\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u4ece\u7f51\u7edc\u6570\u636e\u4e2d\u9884\u6d4b\u8fd0\u52a8\u4fe1\u606f\uff0c\u5e76\u5c06\u673a\u5668\u4eba\u7b56\u7565\u6761\u4ef6\u5316\u4e8e\u751f\u6210\u7684\u89c6\u9891\u3002\u6211\u4eec\u6ca1\u6709\u5c1d\u8bd5\u6269\u5c55\u6602\u8d35\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\uff0c\u800c\u662f\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u5728\u6613\u83b7\u53d6\u7684\u7f51\u7edc\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u6765\u5b9e\u73b0\u6cdb\u5316\u3002\u6211\u4eec\u7684\u65b9\u6cd5Gen2Act\u5c06\u8bed\u8a00\u6761\u4ef6\u5316\u7684\u64cd\u4f5c\u89c6\u4e3a\u96f6\u6837\u672c\u7684\u4eba\u7c7b\u89c6\u9891\u751f\u6210\uff0c\u968f\u540e\u901a\u8fc7\u4e00\u4e2a\u4ee5\u751f\u6210\u89c6\u9891\u4e3a\u6761\u4ef6\u7684\u5355\u4e00\u7b56\u7565\u8fdb\u884c\u6267\u884c\u3002\u4e3a\u4e86\u8bad\u7ec3\u8be5\u7b56\u7565\uff0c\u6211\u4eec\u4f7f\u7528\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u91cf\u6bd4\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u6570\u636e\u91cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002Gen2Act\u5b8c\u5168\u4e0d\u9700\u8981\u5fae\u8c03\u89c6\u9891\u6a21\u578b\uff0c\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u751f\u6210\u4eba\u7c7b\u89c6\u9891\u3002\u6211\u4eec\u5728\u591a\u6837\u5316\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7ed3\u679c\u5c55\u793a\u4e86Gen2Act\u5982\u4f55\u5b9e\u73b0\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u578b\u7684\u64cd\u4f5c\uff0c\u5e76\u6267\u884c\u673a\u5668\u4eba\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u7684\u65b0\u4efb\u52a1\u7684\u65b0\u52a8\u4f5c\u3002\u89c6\u9891\u6f14\u793a\u8bf7\u8bbf\u95ee\uff1ahttps://homangab.github.io/gen2act/ Homanga Bharadhwaj PDF N/A Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video. To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data. Videos are at https://homangab.github.io/gen2act/ \u5b66\u4e60\u5e2e\u52a9\uff1a\u8bad\u7ec3\u6a21\u578b\u4ee5\u8f85\u52a9\u4f20\u7edf\u8bbe\u5907 \u5728\u7269\u7406\u8bbe\u5907\u4e0a\u901a\u8fc7\u786c\u4ef6\u5b9e\u73b0\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u80fd\u4f1a\u88ab\u957f\u671f\u90e8\u7f72\u3002\u8bbe\u5907\u7684\u8ba1\u7b97\u80fd\u529b\u53ef\u80fd\u6709\u9650\uff0c\u5e76\u4e14\u968f\u7740\u65b0\u6280\u672f\u7684\u6539\u8fdb\u800c\u53d8\u5f97\u8fc7\u65f6\u3002\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5927\u5c0f\uff0c\u5c06\u90e8\u5206\u8ba1\u7b97\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u8f6c\u79fb\u5230\u8fb9\u7f18\u4e91\uff09\u53ef\u4ee5\u5e2e\u52a9\u8fd9\u4e9b\u8001\u65e7\u8bbe\u5907\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u95ee\u9898\u7f6e\u4e8e\u5e26\u5f03\u6743\u7684\u5b66\u4e60\u7684\u6846\u67b6\u4e2d\uff08LWA\uff09\uff0c\u5176\u4e2d\u4e13\u5bb6\uff08\u8fb9\u7f18\uff09\u5fc5\u987b\u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u534f\u52a9\u5ba2\u6237\u7aef\uff08\u8bbe\u5907\uff09\u3002\u5148\u524d\u5173\u4e8eLWA\u7684\u5de5\u4f5c\u5047\u8bbe\u8fb9\u7f18\u662f\u4e00\u4e2a\u9884\u8a00\u673a\u6216\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5e76\u8bad\u7ec3\u5ba2\u6237\u7aef\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f62\u5f0f\u5316\u4e86\u76f8\u53cd\u7684\u95ee\u9898\uff0c\u5373\u8bad\u7ec3\u4e13\u5bb6\u4ee5\u9002\u5e94\u56fa\u5b9a\u7684\uff08\u8001\u65e7\u7684\uff09\u5ba2\u6237\u7aef\u3002\u4e0eLWA\u4e00\u6837\uff0c\u5ba2\u6237\u7aef\u4f7f\u7528\u62d2\u7edd\u89c4\u5219\u6765\u51b3\u5b9a\u4f55\u65f6\u5c06\u63a8\u7406\u4efb\u52a1\u5378\u8f7d\u7ed9\u4e13\u5bb6\uff08\u9700\u4ed8\u51fa\u6210\u672c\uff09\u3002\u6211\u4eec\u627e\u5230\u4e86\u8d1d\u53f6\u65af\u6700\u4f18\u89c4\u5219\uff0c\u8bc1\u660e\u4e86\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u627e\u5230\u4e86\u4e00\u81f4\u7684\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4f18\u4e8e\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u62d2\u7edd\u89c4\u5219\u3002 Yu Wu PDF N/A Learning To Help: Training Models to Assist Legacy Devices Machine learning models implemented in hardware on physical devices may be deployed for a long time. The computational abilities of the device may be limited and become outdated with respect to newer improvements. Because of the size of ML models, offloading some computation (e.g. to an edge cloud) can help such legacy devices. We cast this problem in the framework of learning with abstention (LWA) in which the expert (edge) must be trained to assist the client (device). Prior work on LWA trains the client assuming the edge is either an oracle or a human expert. In this work, we formalize the reverse problem of training the expert for a fixed (legacy) client. As in LWA, the client uses a rejection rule to decide when to offload inference to the expert (at a cost). We find the Bayes-optimal rule, prove a generalization bound, and find a consistent surrogate loss function. Empirical results show that our framework outperforms confidence-based rejection rules. \u4e16\u754c\u7530\u5730\uff1a\u5168\u7403\u519c\u4e1a\u7530\u5730\u8fb9\u754c\u5206\u5272\u7684\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u6570\u636e\u96c6 \u519c\u7530\u8fb9\u754c\u662f\u519c\u4e1a\u76d1\u6d4b\u548c\u8bc4\u4f30\u7684\u57fa\u7840\u6570\u636e\u96c6\uff0c\u4f46\u624b\u52a8\u91c7\u96c6\u6210\u672c\u9ad8\u6602\u3002\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u4ece\u9065\u611f\u56fe\u50cf\u4e2d\u63d0\u53d6\u519c\u7530\u8fb9\u754c\uff0c\u6709\u52a9\u4e8e\u5728\u5168\u7403\u8303\u56f4\u5185\u6ee1\u8db3\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u5f53\u524d\u7528\u4e8e\u519c\u7530\u5b9e\u4f8b\u5206\u5272\u7684ML\u65b9\u6cd5\u5728\u5730\u7406\u8986\u76d6\u8303\u56f4\u3001\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4ee3\u8868\u5168\u7403\u519c\u7530\u591a\u6837\u6027\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6539\u8fdbML\u65b9\u6cd5\u7684\u7814\u7a76\u53d7\u5230\u9650\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u4e16\u754c\u519c\u7530\u201d\uff08Fields of The World, FTW\uff09\u2014\u2014\u4e00\u4e2a\u6db5\u76d6\u56db\u5927\u6d32\uff08\u6b27\u6d32\u3001\u975e\u6d32\u3001\u4e9a\u6d32\u548c\u5357\u7f8e\u6d32\uff0924\u4e2a\u56fd\u5bb6\u7684\u519c\u4e1a\u519c\u7530\u5b9e\u4f8b\u5206\u5272\u7684\u65b0\u578bML\u57fa\u51c6\u6570\u636e\u96c6\u3002FTW\u7684\u6570\u636e\u89c4\u6a21\u662f\u4e4b\u524d\u6570\u636e\u96c6\u7684\u5341\u500d\uff0c\u5305\u542b70,462\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u5305\u542b\u5b9e\u4f8b\u548c\u8bed\u4e49\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4e0e\u591a\u65e5\u671f\u3001\u591a\u5149\u8c31\u7684Sentinel-2\u536b\u661f\u56fe\u50cf\u914d\u5bf9\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u65b0FTW\u57fa\u51c6\u7684\u57fa\u7ebf\u6a21\u578b\u7ed3\u679c\uff0c\u8868\u660e\u5728\u672a\u89c1\u8fc7\u7684\u56fd\u5bb6\u4e2d\uff0c\u4f7f\u7528FTW\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\u4e0a\u4f18\u4e8e\u672a\u7ecf\u8fc7\u591a\u6837\u5316\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u793a\u4e86FTW\u6a21\u578b\u5728\u57c3\u585e\u4fc4\u6bd4\u4e9aSentinel-2\u56fe\u50cf\u4e0a\u7684\u79ef\u6781\u96f6\u6837\u672c\u5b9a\u6027\u7ed3\u679c\u3002 Hannah Kerner PDF N/A Fields of The World: A Machine Learning Benchmark Dataset For Global Agricultural Field Boundary Segmentation Crop field boundaries are foundational datasets for agricultural monitoring and assessments but are expensive to collect manually. Machine learning (ML) methods for automatically extracting field boundaries from remotely sensed images could help realize the demand for these datasets at a global scale. However, current ML methods for field instance segmentation lack sufficient geographic coverage, accuracy, and generalization capabilities. Further, research on improving ML methods is restricted by the lack of labeled datasets representing the diversity of global agricultural fields. We present Fields of The World (FTW) -- a novel ML benchmark dataset for agricultural field instance segmentation spanning 24 countries on four continents (Europe, Africa, Asia, and South America). FTW is an order of magnitude larger than previous datasets with 70,462 samples, each containing instance and semantic segmentation masks paired with multi-date, multi-spectral Sentinel-2 satellite images. We provide results from baseline models for the new FTW benchmark, show that models trained on FTW have better zero-shot and fine-tuning performance in held-out countries than models that aren't pre-trained with diverse datasets, and show positive qualitative zero-shot results of FTW models in a real-world scenario -- running on Sentinel-2 scenes over Ethiopia. LLM\u56de\u97f3\u5ba4\uff1a\u4e2a\u6027\u5316\u4e0e\u81ea\u52a8\u5316\u7684\u865a\u5047\u4fe1\u606f\u4f20\u64ad \u6700\u8fd1\u7684\u8fdb\u5c55\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982GPT4\u548cLlama2\u5728\u6458\u8981\u3001\u7ffb\u8bd1\u548c\u5185\u5bb9\u5ba1\u67e5\u7b49\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u62c5\u5fe7\uff0c\u7279\u522b\u662f\u5173\u4e8eLLMs\u53ef\u80fd\u5927\u89c4\u6a21\u4f20\u64ad\u5177\u6709\u8bf4\u670d\u529b\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9519\u8bef\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u4f1a\u663e\u8457\u5f71\u54cd\u516c\u4f17\u8206\u8bba\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u98ce\u9669\uff0c\u91cd\u70b9\u5173\u6ce8LLMs\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u4f5c\u4e3a\u4e8b\u5b9e\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u7814\u7a76\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u6784\u5efa\u4e86LLM\u56de\u97f3\u5ba4\uff0c\u4e00\u4e2a\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u804a\u5929\u5ba4\u7684\u53d7\u63a7\u6570\u5b57\u73af\u5883\uff0c\u9519\u8bef\u4fe1\u606f\u5e38\u5e38\u5728\u5176\u4e2d\u4f20\u64ad\u3002\u56de\u97f3\u5ba4\uff0c\u5373\u4e2a\u4f53\u53ea\u4e0e\u5fd7\u540c\u9053\u5408\u7684\u4eba\u4e92\u52a8\uff0c\u8fdb\u4e00\u6b65\u52a0\u6df1\u4e86\u4fe1\u5ff5\u3002\u901a\u8fc7\u7814\u7a76\u6076\u610f\u673a\u5668\u4eba\u5728\u8fd9\u4e2a\u73af\u5883\u4e2d\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u3002\u6211\u4eec\u56de\u987e\u4e86\u5f53\u524d\u7684LLMs\uff0c\u63a2\u8ba8\u4e86\u9519\u8bef\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u5e76\u5e94\u7528\u4e86\u6700\u5148\u8fdb\u7684\u5fae\u8c03\u6280\u672f\u3002\u4f7f\u7528\u5fae\u8f6f\u7684phi2\u6a21\u578b\uff0c\u901a\u8fc7\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u6211\u4eec\u751f\u6210\u4e86\u6709\u5bb3\u5185\u5bb9\u4ee5\u521b\u5efa\u56de\u97f3\u5ba4\u3002\u8fd9\u4e00\u8bbe\u7f6e\u901a\u8fc7GPT4\u8bc4\u4f30\u5176\u8bf4\u670d\u529b\u548c\u6709\u5bb3\u6027\uff0c\u63ed\u793a\u4e86\u56f4\u7ed5LLMs\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5f3a\u7684\u9632\u62a4\u63aa\u65bd\u6765\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\u3002 Tony Ma PDF N/A LLM Echo Chamber: personalized and automated disinformation Recent advancements have showcased the capabilities of Large Language Models like GPT4 and Llama2 in tasks such as summarization, translation, and content review. However, their widespread use raises concerns, particularly around the potential for LLMs to spread persuasive, humanlike misinformation at scale, which could significantly influence public opinion. This study examines these risks, focusing on LLMs ability to propagate misinformation as factual. To investigate this, we built the LLM Echo Chamber, a controlled digital environment simulating social media chatrooms, where misinformation often spreads. Echo chambers, where individuals only interact with like minded people, further entrench beliefs. By studying malicious bots spreading misinformation in this environment, we can better understand this phenomenon. We reviewed current LLMs, explored misinformation risks, and applied sota finetuning techniques. Using Microsoft phi2 model, finetuned with our custom dataset, we generated harmful content to create the Echo Chamber. This setup, evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the ethical concerns surrounding LLMs and emphasizes the need for stronger safeguards against misinformation. \u6807\u7b7e\u589e\u5f3a\u7684\u6570\u636e\u96c6\u84b8\u998f \u4f20\u7edf\u6570\u636e\u96c6\u84b8\u998f\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u8868\u793a\uff0c\u800c\u5f80\u5f80\u5ffd\u89c6\u6807\u7b7e\u7684\u91cd\u8981\u4f5c\u7528\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6807\u7b7e\u589e\u5f3a\u6570\u636e\u96c6\u84b8\u998f\uff08Label-Augmented Dataset Distillation, LADD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u6807\u7b7e\u589e\u5f3a\u6765\u63d0\u5347\u6570\u636e\u96c6\u84b8\u998f\u6548\u679c\u7684\u65b0\u6846\u67b6\u3002LADD\u5bf9\u6bcf\u4e2a\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u5b50\u91c7\u6837\uff0c\u751f\u6210\u989d\u5916\u7684\u5bc6\u96c6\u6807\u7b7e\u4ee5\u6355\u6349\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u8fd9\u4e9b\u5bc6\u96c6\u6807\u7b7e\u4ec5\u9700\u589e\u52a02.5%\u7684\u5b58\u50a8\u7a7a\u95f4\uff08\u9488\u5bf9ImageNet\u5b50\u96c6\uff09\uff0c\u5374\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63d0\u4f9b\u5f3a\u5927\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u6211\u4eec\u7684\u6807\u7b7e\u751f\u6210\u7b56\u7565\u80fd\u591f\u8865\u5145\u73b0\u6709\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5176\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLADD\u5728\u8ba1\u7b97\u5f00\u9500\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u4e09\u79cd\u9ad8\u6027\u80fd\u7684\u6570\u636e\u96c6\u84b8\u998f\u7b97\u6cd5\uff0cLADD\u5728\u51c6\u786e\u6027\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574714.9%\u7684\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u84b8\u998f\u8d85\u53c2\u6570\u548c\u7b97\u6cd5\u4e2d\u5747\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u84b8\u998f\u6570\u636e\u96c6\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002 Seoungyoon Kang PDF N/A Label-Augmented Dataset Distillation Traditional dataset distillation primarily focuses on image representation while often overlooking the important role of labels. In this study, we introduce Label-Augmented Dataset Distillation (LADD), a new dataset distillation framework enhancing dataset distillation with label augmentations. LADD sub-samples each synthetic image, generating additional dense labels to capture rich semantics. These dense labels require only a 2.5% increase in storage (ImageNet subsets) with significant performance benefits, providing strong learning signals. Our label generation strategy can complement existing dataset distillation methods for significantly enhancing their training efficiency and performance. Experimental results demonstrate that LADD outperforms existing methods in terms of computational overhead and accuracy. With three high-performance dataset distillation algorithms, LADD achieves remarkable gains by an average of 14.9% in accuracy. Furthermore, the effectiveness of our method is proven across various datasets, distillation hyperparameters, and algorithms. Finally, our method improves the cross-architecture robustness of the distilled dataset, which is important in the application scenario. \u901a\u8fc7\u5ec9\u4ef7\u5730\u6392\u5e8f\u6316\u6398\u7684\u89c4\u5219\uff0c\u9ad8\u6548\u5730\u5b66\u4e60\u6982\u7387\u903b\u8f91\u6a21\u578b \u6982\u7387\u903b\u8f91\u6a21\u578b\u662f\u795e\u7ecf\u7b26\u53f7\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u5bf9\u4e8e\u9700\u8981\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027\u7684\u4efb\u52a1\u800c\u8a00\uff0c\u5b83\u4eec\u672c\u8eab\u5c31\u662f\u91cd\u8981\u7684\u6a21\u578b\u3002\u4e0e\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\uff0c\u903b\u8f91\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u624b\u5de5\u6784\u5efa\uff0c\u8fd9\u4f7f\u5f97\u5176\u5f00\u53d1\u6210\u672c\u9ad8\u6602\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u5c3d\u7ba1\u5b58\u5728\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u903b\u8f91\u6a21\u578b\u7684\u7b97\u6cd5\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e3a\u903b\u8f91\u89c4\u5219\u5f15\u5165\u4e86\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u7ec4\u5408\u5b9a\u4e49\u4e3a\u89c4\u5219\u6548\u7528\u2014\u2014\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u903b\u8f91\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86SPECTRUM\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5173\u7cfb\u6570\u636e\u4e2d\u5b66\u4e60\u903b\u8f91\u6a21\u578b\u3002\u5176\u53ef\u6269\u5c55\u6027\u6e90\u4e8e\u4e00\u79cd\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6316\u6398\u6570\u636e\u4e2d\u7684\u91cd\u590d\u7ed3\u6784\uff0c\u4ee5\u53ca\u53e6\u4e00\u79cd\u4f7f\u7528\u5ec9\u4ef7\u6548\u7528\u5ea6\u91cf\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u9ad8\u6548\u5730\u5bf9\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u6784\u6784\u5efa\u7684\u89c4\u5219\u8fdb\u884c\u6392\u5e8f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u6240\u5b66\u903b\u8f91\u6a21\u578b\u7684\u6548\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u56e0\u6b64\uff0cSPECTRUM\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u6bd4\u4ee5\u5f80\u7684\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u5730\u5b66\u4e60\u5230\u66f4\u51c6\u786e\u7684\u903b\u8f91\u6a21\u578b\u3002 Jonathan Feldstein PDF N/A Efficiently Learning Probabilistic Logical Models by Cheaply Ranking Mined Rules Probabilistic logical models are a core component of neurosymbolic AI and are important models in their own right for tasks that require high explainability. Unlike neural networks, logical models are often handcrafted using domain expertise, making their development costly and prone to errors. While there are algorithms that learn logical models from data, they are generally prohibitively expensive, limiting their applicability in real-world settings. In this work, we introduce precision and recall for logical rules and define their composition as rule utility -- a cost-effective measure to evaluate the predictive power of logical models. Further, we introduce SPECTRUM, a scalable framework for learning logical models from relational data. Its scalability derives from a linear-time algorithm that mines recurrent structures in the data along with a second algorithm that, using the cheap utility measure, efficiently ranks rules built from these structures. Moreover, we derive theoretical guarantees on the utility of the learnt logical model. As a result, SPECTRUM learns more accurate logical models orders of magnitude faster than previous methods on real-world datasets. \u4f7f\u7528\u751f\u5b58\u53d8\u6362\u5668\u3001\u6781\u7aef\u68af\u5ea6\u63d0\u5347\u548cCox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u9884\u6d4b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u7684\u6076\u5316 \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528ADNI\u961f\u5217\u4e2d\u7684\u4ee3\u8c22\u7ec4\u5b66\u6570\u636e\uff0c\u9884\u6d4b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u4e2a\u4f53\u8ba4\u77e5\u8870\u9000\u7684\u65b0\u65b9\u6cd5\uff0c\u5373\u751f\u5b58\u53d8\u6362\u5668\u548c\u6781\u7aef\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u3002\u901a\u8fc7\u5229\u7528\u5e94\u7528\u4e8e\u751f\u5b58\u5206\u6790\u7684\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u548c\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u6280\u672f\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7a81\u663e\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u66f4\u51c6\u786e\u5730\u65e9\u671f\u68c0\u6d4b\u548c\u5e72\u9884\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6027\u75f4\u5446\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8fd9\u9879\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u975e\u4fb5\u5165\u6027\u751f\u7269\u6807\u5fd7\u7269\u548c\u521b\u65b0\u5efa\u6a21\u5de5\u5177\u5728\u63d0\u9ad8\u75f4\u5446\u98ce\u9669\u8bc4\u4f30\u51c6\u786e\u6027\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u548c\u60a3\u8005\u62a4\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u4e00\u4e2a\u5305\u542b100\u6b21\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u91cd\u590d\u7684\u5168\u9762\u8499\u7279\u5361\u7f57\u6a21\u62df\u8fc7\u7a0b\u8868\u660e\uff0c\u57fa\u4e8e\u53d8\u6362\u5668\u548cXGBoost\u7684\u751f\u5b58\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u522b\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u5e73\u5747C-index\u8868\u73b0\uff0c\u5206\u522b\u4e3a0.85\u548c0.8\uff0c\u5e76\u4e14\u4f18\u4e8e\u4f20\u7edf\u7684\u751f\u5b58\u5206\u6790Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\uff0c\u540e\u8005\u8fbe\u5230\u4e860.77\u7684\u5e73\u5747C-index\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u8499\u7279\u5361\u7f57\u6a21\u62df\u4e2d\u83b7\u5f97\u7684C-index\u8868\u73b0\u7684\u6807 Henry Musto PDF N/A Predicting Deterioration in Mild Cognitive Impairment with Survival Transformers, Extreme Gradient Boosting and Cox Proportional Hazard Modelling The paper proposes a novel approach of survival transformers and extreme gradient boosting models in predicting cognitive deterioration in individuals with mild cognitive impairment (MCI) using metabolomics data in the ADNI cohort. By leveraging advanced machine learning and transformer-based techniques applied in survival analysis, the proposed approach highlights the potential of these techniques for more accurate early detection and intervention in Alzheimer's dementia disease. This research also underscores the importance of non-invasive biomarkers and innovative modelling tools in enhancing the accuracy of dementia risk assessments, offering new avenues for clinical practice and patient care. A comprehensive Monte Carlo simulation procedure consisting of 100 repetitions of a nested cross-validation in which models were trained and evaluated, indicates that the survival machine learning models based on Transformer and XGBoost achieved the highest mean C-index performances, namely 0.85 and 0.8, respectively, and that they are superior to the conventional survival analysis Cox Proportional Hazards model which achieved a mean C-Index of 0.77. Moreover, based on the standard deviations of the C-Index performances obtained in the Monte Carlo simulation, we established that both survival machine learning models above are more stable than the conventional statistical model. \u5fae\u8c03\u662f\u53ef\u884c\u7684\uff0c\u53ea\u8981\u7ecf\u8fc7\u6821\u51c6 \u5fae\u8c03\u65e0\u7591\u662f\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u4f8b\u5982\u57fa\u7840\u6a21\u578b\uff09\u5b9a\u5236\u5230\u4e0b\u6e38\u5e94\u7528\u7684\u6700\u76f4\u63a5\u65b9\u6cd5\uff0c\u4f46\u5b83\u4e5f\u4f34\u968f\u7740\u4e22\u5931\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u83b7\u5f97\u7684\u6709\u4ef7\u503c\u77e5\u8bc6\u7684\u6f5c\u5728\u98ce\u9669\u3002\u4f8b\u5982\uff0c\u5c06\u4e00\u4e2a\u80fd\u591f\u8bc6\u522b\u5927\u91cf\u7c7b\u522b\u7684\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\u5fae\u8c03\u4e3a\u638c\u63e1\u5f53\u524d\u5b50\u96c6\u4e2d\u7684\u7c7b\u522b\uff0c\u5df2\u88ab\u8bc1\u660e\u4f1a\u5927\u5e45\u964d\u4f4e\u6a21\u578b\u5728\u5176\u4ed6\u5148\u524d\u5b66\u4e60\u7c7b\u522b\u4e2d\u7684\u51c6\u786e\u6027\u3002\u56e0\u6b64\uff0c\u5f53\u5fae\u8c03\u540e\u7684\u6a21\u578b\u9047\u5230\u8d85\u51fa\u5fae\u8c03\u6570\u636e\u8303\u56f4\u7684\u7c7b\u522b\u65f6\uff0c\u5f88\u96be\u8fdb\u4e00\u6b65\u4f7f\u7528\u5b83\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u5256\u6790\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u65e8\u5728\u56de\u7b54\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u201c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e2d\u7a76\u7adf\u635f\u5931\u4e86\u4ec0\u4e48\uff1f\u201d\u4ee4\u6211\u4eec\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u5fae\u8c03\u540e\u7684\u6a21\u578b\u65e2\u6ca1\u6709\u5fd8\u8bb0\u5176\u4ed6\u7c7b\u522b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e5f\u6ca1\u6709\u964d\u4f4e\u8bc6\u522b\u8fd9\u4e9b\u7c7b\u522b\u7684\u7279\u5f81\u3002\u76f8\u53cd\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u901a\u5e38\u4f1a\u4e3a\u8fd9\u4e9b\u5176\u4ed6\u7c7b\u522b\u751f\u6210\u66f4\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\uff0c\u5373\u4f7f\u8fd9\u4e9b\u7c7b\u522b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7f3a\u5931\uff01\u771f\u6b63\u635f\u5bb3\u51c6\u786e\u6027\u7684\u662f\u5fae\u8c03\u7c7b\u522b\u4e0e\u5176\u4ed6\u7c7b\u522b\u4e4b\u95f4\u4e0d\u4e00\u81f4\u7684logit\u5c3a\u5ea6\uff0c\u8fd9\u610f\u5473\u7740\u7b80\u5355\u7684\u540e\u5904\u7406\u6821\u51c6\u4e0d\u4ec5\u53ef\u4ee5\u6062\u590d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u80fd\u529b\uff0c\u8fd8\u80fd\u540c\u65f6\u63ed\u793a\u6240\u6709\u7c7b\u522b\u7279\u5f81\u7684\u6539\u8fdb\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4ee5\u5c55\u793a\u6211\u4eec\u53d1\u73b0\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u521d\u6b65\u7684\u89e3\u91ca\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7406\u8bba\u5206\u6790\u7684\u65b0\u65b9\u5411\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated\u83b7\u53d6\u3002 Zheda Mai PDF N/A Fine-Tuning is Fine, if Calibrated Fine-tuning is arguably the most straightforward way to tailor a pre-trained model (e.g., a foundation model) to downstream applications, but it also comes with the risk of losing valuable knowledge the model had learned in pre-training. For example, fine-tuning a pre-trained classifier capable of recognizing a large number of classes to master a subset of classes at hand is shown to drastically degrade the model's accuracy in the other classes it had previously learned. As such, it is hard to further use the fine-tuned model when it encounters classes beyond the fine-tuning data. In this paper, we systematically dissect the issue, aiming to answer the fundamental question, ''What has been damaged in the fine-tuned model?'' To our surprise, we find that the fine-tuned model neither forgets the relationship among the other classes nor degrades the features to recognize these classes. Instead, the fine-tuned model often produces more discriminative features for these other classes, even if they were missing during fine-tuning! {What really hurts the accuracy is the discrepant logit scales between the fine-tuning classes and the other classes}, implying that a simple post-processing calibration would bring back the pre-trained model's capability and at the same time unveil the feature improvement over all classes. We conduct an extensive empirical study to demonstrate the robustness of our findings and provide preliminary explanations underlying them, suggesting new directions for future theoretical analysis. Our code is available at https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated. \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u5bf9\u8bdd\u5f0f\u7528\u6237\u754c\u9762\u4e2d\u7684\u5173\u8054\u6570\u636e\u68c0\u7d22 \u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u4e2a\u9886\u57df\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u4e30\u5bcc\u4fe1\u606f\u7cfb\u7edf\u3001\u63d0\u53d6\u548c\u63a2\u7d22\u5173\u8054\u6570\u636e\uff08LD\uff09\u4ee5\u53ca\u8d44\u6e90\u63cf\u8ff0\u6846\u67b6\uff08RDF\uff09\u4e09\u5143\u7ec4\u5b58\u50a8\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5c06LLMs\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\uff0c\u91cd\u70b9\u5728\u4e8e\u589e\u5f3a\u5bf9\u8bdd\u7528\u6237\u754c\u9762\uff08UIs\uff09\u53ca\u5176\u901a\u8fc7\u751f\u6210\u66f4\u51c6\u786e\u7684SPARQL\u67e5\u8be2\u8fdb\u884c\u6570\u636e\u63d0\u53d6\u7684\u80fd\u529b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002\u901a\u5e38\uff0c\u5bf9\u8bddUI\u6a21\u578b\u5728\u5f15\u5165\u65b0\u6570\u636e\u96c6\u6216\u66f4\u65b0\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u63d0\u53d6\u5de5\u5177\u7684\u529f\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06LLMs\u878d\u5165\u5bf9\u8bddUI\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u7406\u89e3\u548c\u6709\u6548\u5904\u7406\u7528\u6237\u67e5\u8be2\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5229\u7528LLMs\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6539\u8fdb\u4e86\u5728\u91c7\u7528\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u7684\u7f51\u7edc\u7cfb\u7edf\u4e2d\u7684RDF\u5b9e\u4f53\u63d0\u53d6\u3002\u8fd9\u79cd\u96c6\u6210\u4fc3\u8fdb\u4e86\u66f4\u52a0\u7ec6\u81f4\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ea4\u4e92\u6a21\u578b\uff0c\u8fd9\u5bf9\u4e8e\u5904\u7406RDF\u6570\u636e\u96c6\u548c\u5173\u8054\u5f00\u653e\u6570\u636e\uff08LOD\uff09\u7aef\u70b9\u4e2d\u5e38\u89c1\u7684\u590d\u6742\u67e5\u8be2\u6a21\u5f0f\u81f3\u5173\u91cd\u8981\u3002\u5bf9\u8fd9\u79cd\u65b9\u6cd5\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u7cfb\u7edf\u8868\u8fbe\u80fd\u529b\u548c\u7528\u6237\u67e5\u8be2\u54cd\u5e94\u7684\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u8868\u660e\u8fd9\u4e00\u9886\u57df\u672a\u6765\u7814\u7a76\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002\u8fd9\u9879\u7814\u7a76\u4e0d\u4ec5\u7a81\u663e\u4e86LLMs\u5728\u589e\u5f3a\u73b0\u6709\u4fe1\u606f\u7cfb\u7edf\u65b9\u9762\u7684\u591a\u529f\u80fd\u6027\uff0c\u8fd8\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u7f51\u7edc\u4fe1\u606f\u7cfb\u7edf\u66f4\u4e13\u4e1a\u5316\u9886\u57df\u7684\u6f5c\u5728\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002 Omar Mussa PDF N/A Towards Enhancing Linked Data Retrieval in Conversational UIs using Large Language Models Despite the recent broad adoption of Large Language Models (LLMs) across various domains, their potential for enriching information systems in extracting and exploring Linked Data (LD) and Resource Description Framework (RDF) triplestores has not been extensively explored. This paper examines the integration of LLMs within existing systems, emphasising the enhancement of conversational user interfaces (UIs) and their capabilities for data extraction by producing more accurate SPARQL queries without the requirement for model retraining. Typically, conversational UI models necessitate retraining with the introduction of new datasets or updates, limiting their functionality as general-purpose extraction tools. Our approach addresses this limitation by incorporating LLMs into the conversational UI workflow, significantly enhancing their ability to comprehend and process user queries effectively. By leveraging the advanced natural language understanding capabilities of LLMs, our method improves RDF entity extraction within web systems employing conventional chatbots. This integration facilitates a more nuanced and context-aware interaction model, critical for handling the complex query patterns often encountered in RDF datasets and Linked Open Data (LOD) endpoints. The evaluation of this methodology shows a marked enhancement in system expressivity and the accuracy of responses to user queries, indicating a promising direction for future research in this area. This investigation not only underscores the versatility of LLMs in enhancing existing information systems but also sets the stage for further explorations into their potential applications within more specialised domains of web information systems."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}