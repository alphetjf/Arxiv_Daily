{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u7528\u4e8e\u4e73\u817a\u6444\u5f71\u7684\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u4e0e\u591a\u5c3a\u5ea6\u5bf9\u9f50 \u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u7531\u4e8e\u8fd9\u4e9b\u9650\u5236\uff0c\u73b0\u6709\u7684CLIP\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u80f8\u90e8X\u5149\u7b49\u62e5\u6709\u4e30\u5bcc\u56fe\u50cf\u62a5\u544a\u6570\u636e\u7684\u6a21\u6001\u4e0a\uff0c\u8bb8\u591a\u5176\u4ed6\u91cd\u8981\u7684\u6a21\u6001\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u6b64\uff0c\u6211\u4eec\u9996\u6b21\u5c06\u5b8c\u6574\u7684CLIP\u6a21\u578b\u5e94\u7528\u4e8e\u4e73\u817a\u6444\u5f71\uff0c\u8fd9\u4e00\u9886\u57df\u7531\u4e8e\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u611f\u5174\u8da3\u533a\u57df\u5c0f\u4ee5\u53ca\u6570\u636e\u4e0d\u5e73\u8861\u800c\u9762\u4e34\u663e\u8457\u6311\u6218\u3002\u6211\u4eec\u9996\u5148\u4e3a\u4e73\u817a\u6444\u5f71\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u5176\u591a\u89c6\u89d2\u7279\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5bf9\u79f0\u5c40\u90e8\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u66f4\u597d\u5730\u5173\u6ce8\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u7ec6\u8282\u7279\u5f81\u3002\u6700\u540e\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u5148\u8bad\u7ec3\u4e86\u533b\u5b66\u77e5\u8bc6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u9650\u5236\u3002\u6211\u4eec\u7684\u591a\u89c6\u89d2\u548c\u591a\u5c3a\u5ea6\u5bf9\u9f50\uff08MaMA\uff09\u65b9\u6cd5\u5728\u4e24\u4e2a\u5927\u578b\u771f\u5b9e\u4e16\u754c\u4e73\u817a\u6444\u5f71\u6570\u636e\u96c6EMBED\u548cRSNA-Mammo\u4e0a\uff0c\u9488\u5bf9\u4e09\u79cd\u4e0d\u540c\u4efb\u52a1\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a\u6700\u5927\u57fa\u7ebf\u768452%\u3002 Yuexi Du PDF N/A Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography Contrastive Language-Image Pre-training (CLIP) shows promise in medical image analysis but requires substantial data and computational resources. Due to these restrictions, existing CLIP applications in medical imaging focus mainly on modalities like chest X-rays that have abundant image-report data available, leaving many other important modalities under-explored. Here, we propose the first adaptation of the full CLIP model to mammography, which presents significant challenges due to labeled data scarcity, high-resolution images with small regions of interest, and data imbalance. We first develop a specialized supervision framework for mammography that leverages its multi-view nature. Furthermore, we design a symmetric local alignment module to better focus on detailed features in high-resolution images. Lastly, we incorporate a parameter-efficient fine-tuning approach for large language models pre-trained with medical knowledge to address data limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms state-of-the-art baselines for three different tasks on two large real-world mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared with the largest baseline. \u5bfb\u627e\u7280\u725b\u800c\u4e0d\u5bfb\u627e\u7280\u725b\uff1a\u5229\u7528\u5357\u975e\u7280\u725b\u6816\u606f\u5730\u7684\u591a\u6a21\u6001\u5f71\u50cf\u8fdb\u884c\u4e3b\u52a8\u5b66\u4e60 \u5730\u7403\u4e0a\u8bb8\u591a\u5177\u6709\u9b45\u529b\u7684\u5de8\u578b\u52a8\u7269\u6b63\u56e0\u4eba\u7c7b\u6d3b\u52a8\u800c\u6fd2\u4e34\u706d\u7edd\uff0c\u5c24\u5176\u662f\u7280\u725b\uff0c\u7531\u4e8e\u975e\u6d32\u7684\u76d7\u730e\u5371\u673a\uff0c\u5b83\u4eec\u6b63\u9762\u4e34\u706d\u7edd\u7684\u98ce\u9669\u3002\u76d1\u6d4b\u7280\u725b\u7684\u79fb\u52a8\u5bf9\u5176\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9057\u61be\u7684\u662f\uff0c\u7531\u4e8e\u7280\u725b\u884c\u8e2a\u9690\u79d8\uff0c\u8fd9\u4e00\u4efb\u52a1\u4e00\u76f4\u96be\u4ee5\u5b9e\u73b0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5373\u7ed8\u5236\u7fa4\u4f53\u6392\u6cc4\u5730\u70b9\uff08\u79f0\u4e3a\u7caa\u5806\uff09\u7684\u5730\u56fe\uff0c\u8fd9\u4e9b\u5730\u70b9\u63d0\u4f9b\u4e86\u5173\u4e8e\u7280\u725b\u7a7a\u95f4\u884c\u4e3a\u7684\u4fe1\u606f\uff0c\u5bf9\u53cd\u76d7\u730e\u3001\u7ba1\u7406\u548c\u91cd\u65b0\u5f15\u5165\u5de5\u4f5c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u672c\u6587\u9996\u6b21\u901a\u8fc7\u6784\u5efa\u5206\u7c7b\u5668\uff0c\u5229\u7528\u9065\u611f\u7684\u70ed\u6210\u50cf\u3001RGB\u548cLiDAR\u56fe\u50cf\uff0c\u5728\u88ab\u52a8\u548c\u4e3b\u52a8\u5b66\u4e60\u73af\u5883\u4e2d\u68c0\u6d4b\u5e76\u7ed8\u5236\u4e86\u7280\u725b\u7caa\u5806\u7684\u4f4d\u7f6e\u3002\u7531\u4e8e\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u56e0\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86MultimodAL\uff0c\u8fd9\u662f\u4e00\u4e2a\u91c7\u7528\u6392\u5e8f\u6280\u672f\u548c\u591a\u6a21\u6001\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u4e0e\u88ab\u52a8\u5b66\u4e60\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4e14\u4ec5\u970094%\u7684\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5904\u7406\u7c7b\u4f3c\u89c4\u6a21\u7684\u6570\u636e\u96c6\u65f6\uff0c\u53ef\u4ee5\u8282\u7701\u8d85\u8fc776\u5c0f\u65f6\u7684\u6807\u6ce8\u65f6\u95f4\u3002\u51fa\u4e4e\u610f\u6599\u7684\u662f\uff0c\u6211\u4eec\u7684\u7caa\u5806\u5730\u56fe\u63ed\u793a\u4e86\u7280\u725b\u7caa\u5806\u5e76\u975e\u968f\u673a\u5206\u5e03\u5728\u6574\u4e2a\u666f\u89c2\u4e2d\uff0c\u800c\u662f\u5448\u73b0\u51fa\u805a\u96c6\u5206\u5e03\u3002\u56e0\u6b64\uff0c\u62a4\u6797\u5458\u5e94\u96c6\u4e2d\u5728\u7caa\u5806\u5bc6\u5ea6\u9ad8\u7684\u533a\u57df\uff0c\u4ee5\u52a0\u5f3a\u53cd\u76d7\u730e\u5de5\u4f5c\uff0c\u8fd9\u4e0e\u8054\u5408\u56fd\u76ee\u680715.7\u76f8\u4e00\u81f4\u3002 Lucia Gordon PDF N/A Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats Much of Earth's charismatic megafauna is endangered by human activities, particularly the rhino, which is at risk of extinction due to the poaching crisis in Africa. Monitoring rhinos' movement is crucial to their protection but has unfortunately proven difficult because rhinos are elusive. Therefore, instead of tracking rhinos, we propose the novel approach of mapping communal defecation sites, called middens, which give information about rhinos' spatial behavior valuable to anti-poaching, management, and reintroduction efforts. This paper provides the first-ever mapping of rhino midden locations by building classifiers to detect them using remotely sensed thermal, RGB, and LiDAR imagery in passive and active learning settings. As existing active learning methods perform poorly due to the extreme class imbalance in our dataset, we design MultimodAL, an active learning system employing a ranking technique and multimodality to achieve competitive performance with passive learning models with 94% fewer labels. Our methods could therefore save over 76 hours in labeling time when used on a similarly-sized dataset. Unexpectedly, our midden map reveals that rhino middens are not randomly distributed throughout the landscape; rather, they are clustered. Consequently, rangers should be targeted at areas with high midden densities to strengthen anti-poaching efforts, in line with UN Target 15.7. MALPOLON\uff1a\u6df1\u5ea6\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u6846\u67b6 \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMALPOLON\u7684\u6df1\u5ea6\u7269\u79cd\u5206\u5e03\u6a21\u578b\uff08deep-SDM\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528Python\u7f16\u5199\uff0c\u57fa\u4e8ePyTorch\u5e93\u6784\u5efa\uff0c\u65e8\u5728\u4e3a\u4ec5\u5177\u5907\u4e00\u822cPython\u8bed\u8a00\u6280\u80fd\u7684\u7528\u6237\uff08\u5982\u751f\u6001\u5efa\u6a21\u5b66\u8005\uff09\u63d0\u4f9b\u4fbf\u5229\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u8f7b\u677e\u8bad\u7ec3\u548c\u63a8\u65ad\u6df1\u5ea6\u7269\u79cd\u5206\u5e03\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5206\u4eab\u3002\u8fd9\u4e9b\u7528\u6237\u5bf9\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6784\u5efa\u65b0\u7684\u7269\u79cd\u5206\u5e03\u6a21\u578b\u611f\u5174\u8da3\u3002\u6b64\u5916\uff0c\u66f4\u9ad8\u7ea7\u7684\u7528\u6237\u4e5f\u53ef\u4ee5\u5229\u7528\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u7279\u6027\uff0c\u901a\u8fc7\u91cd\u5199\u73b0\u6709\u7c7b\u6765\u8fd0\u884c\u66f4\u5177\u4f53\u7684\u5b9e\u9a8c\uff0c\u540c\u65f6\u501f\u52a9\u4e00\u952e\u5f0f\u793a\u4f8b\uff0c\u5229\u7528\u81ea\u5b9a\u4e49\u6216\u63d0\u4f9b\u7684\u539f\u59cb\u53ca\u9884\u5904\u7406\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002\u8be5\u6846\u67b6\u5df2\u5728GitHub\u548cPyPi\u4e0a\u5f00\u6e90\uff0c\u5e76\u9644\u6709\u8be6\u5c3d\u7684\u6587\u6863\u548c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u4f7f\u7528\u793a\u4f8b\u3002MALPOLON\u63d0\u4f9b\u4e86\u7b80\u4fbf\u7684\u5b89\u88c5\u65b9\u5f0f\u3001\u57fa\u4e8eYAML\u7684\u914d\u7f6e\u3001\u5e76\u884c\u8ba1\u7b97\u3001\u591aGPU\u5229\u7528\u3001\u57fa\u51c6\u548c\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\uff0c\u4ee5\u53ca\u4e30\u5bcc\u7684\u6559\u7a0b\u548c\u6587\u6863\uff0c\u65e8\u5728\u63d0\u5347\u751f\u6001\u5b66\u8005\u548c\u7814\u7a76\u4eba\u5458\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6027\u80fd\u6269\u5c55\u6027\u3002 Theo Larcher PDF N/A MALPOLON: A Framework for Deep Species Distribution Modeling This paper describes a deep-SDM framework, MALPOLON. Written in Python and built upon the PyTorch library, this framework aims to facilitate training and inferences of deep species distribution models (deep-SDM) and sharing for users with only general Python language skills (e.g., modeling ecologists) who are interested in testing deep learning approaches to build new SDMs. More advanced users can also benefit from the framework's modularity to run more specific experiments by overriding existing classes while taking advantage of press-button examples to train neural networks on multiple classification tasks using custom or provided raw and pre-processed datasets. The framework is open-sourced on GitHub and PyPi along with extensive documentation and examples of use in various scenarios. MALPOLON offers straightforward installation, YAML-based configuration, parallel computing, multi-GPU utilization, baseline and foundational models for benchmarking, and extensive tutorials/documentation, aiming to enhance accessibility and performance scalability for ecologists and researchers. \u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u5728\u536b\u661f\u88c5\u914d\u3001\u96c6\u6210\u4e0e\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528 \u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e0e\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7684\u878d\u5408\u6709\u671b\u901a\u8fc7\u63d0\u9ad8\u7cbe\u5ea6\u3001\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u4ee5\u53ca\u5728\u6d01\u51c0\u5ba4\u73af\u5883\u4e2d\u63d0\u5347\u64cd\u4f5c\u6548\u7387\uff0c\u5f7b\u5e95\u6539\u53d8\u536b\u661f\u88c5\u914d\u3001\u96c6\u6210\u548c\u6d4b\u8bd5\uff08AIT\uff09\u6d41\u7a0b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u6b27\u6d32\u822a\u5929\u5c40\uff08ESA\uff09\u7684\u201cAI for AR in Satellite AIT\u201d\u9879\u76ee\uff0c\u8be5\u9879\u76ee\u7ed3\u5408\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAR\u7cfb\u7edf\uff0c\u4ee5\u534f\u52a9\u6280\u672f\u4eba\u5458\u8fdb\u884c\u536b\u661f\u88c5\u914d\u3002\u5229\u7528\u5fae\u8f6fHoloLens 2\u4f5c\u4e3aAR\u63a5\u53e3\uff0c\u8be5\u7cfb\u7edf\u63d0\u4f9b\u60c5\u5883\u611f\u77e5\u7684\u6307\u5bfc\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u89e3\u51b3\u4e86AIT\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7269\u4f53\u8bc6\u522b\u548c6D\u59ff\u6001\u4f30\u8ba1\u7684\u590d\u6742\u6027\u3002\u6240\u6709AI\u6a21\u578b\u7684\u51c6\u786e\u7387\u5747\u8d85\u8fc770%\uff0c\u5176\u4e2d\u68c0\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u663e\u793a\u51fa\u9ad8\u6c34\u5e73\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002\u672c\u7814\u7a76\u7684\u4e00\u4e2a\u91cd\u8981\u8d21\u732e\u5728\u4e8e\u6709\u6548\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884cAR\u5e94\u7528\u4e2d\u7684AI\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u536b\u661f\u73af\u5883\u4e2d\u83b7\u53d6\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u91cd\u5927\u6311\u6218\uff0c\u5e76\u521b\u5efa\u4e86\u7528\u4e8e\u81ea\u52a8\u6807\u6ce8\u7684\u5206\u5272\u4e00\u5207\u6a21\u578b\uff08SAMAL\uff09\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u81ea\u52a8\u6807\u6ce8\uff0c\u901f\u5ea6\u6bd4\u4eba\u5de5\u6807\u6ce8\u5feb20\u500d\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cAI\u9a71\u52a8\u7684AR\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u5173\u952e\u536b\u661f\u88c5\u914d\u4efb\u52a1\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u822a\u5929\u5de5\u4e1a\u672a\u6765\u7684\u521b\u65b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002 Alvaro Patricio PDF N/A AI-Powered Augmented Reality for Satellite Assembly, Integration and Test The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is set to transform satellite Assembly, Integration, and Testing (AIT) processes by enhancing precision, minimizing human error, and improving operational efficiency in cleanroom environments. This paper presents a technical description of the European Space Agency's (ESA) project \"AI for AR in Satellite AIT,\" which combines real-time computer vision and AR systems to assist technicians during satellite assembly. Leveraging Microsoft HoloLens 2 as the AR interface, the system delivers context-aware instructions and real-time feedback, tackling the complexities of object recognition and 6D pose estimation in AIT workflows. All AI models demonstrated over 70% accuracy, with the detection model exceeding 95% accuracy, indicating a high level of performance and reliability. A key contribution of this work lies in the effective use of synthetic data for training AI models in AR applications, addressing the significant challenges of obtaining real-world datasets in highly dynamic satellite environments, as well as the creation of the Segmented Anything Model for Automatic Labelling (SAMAL), which facilitates the automatic annotation of real data, achieving speeds up to 20 times faster than manual human annotation. The findings demonstrate the efficacy of AI-driven AR systems in automating critical satellite assembly tasks, setting a foundation for future innovations in the space industry. \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7528\u4e8e\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u7535\u5f71\u5206\u5272 \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08SSP\uff09\u5728\u4ece\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u65b9\u9762\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u53ef\u80fd\u5bf9\u81ea\u52a8\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\uff08CMR\uff09\u77ed\u8f74\u7535\u5f71\u5206\u5272\u6709\u7528\u3002\u7136\u800c\uff0c\u5173\u4e8eSSP\u5bf9\u5206\u5272\u76ca\u5904\u7684\u4e0d\u4e00\u81f4\u62a5\u544a\u4f7f\u5f97\u5c06\u5176\u5e94\u7528\u4e8eCMR\u53d8\u5f97\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30SSP\u65b9\u6cd5\u5728CMR\u7535\u5f71\u5206\u5272\u4e2d\u7684\u6548\u679c\u3002\u4e3a\u6b64\uff0c\u4f7f\u7528\u4e86296\u540d\u53d7\u8bd5\u8005\uff0890618\u5f202D\u5207\u7247\uff09\u7684\u77ed\u8f74\u7535\u5f71\u5806\u6808\u8fdb\u884c\u672a\u6807\u6ce8\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u4e86\u56db\u79cdSSP\u65b9\u6cd5\uff1aSimCLR\u3001\u4f4d\u7f6e\u5bf9\u6bd4\u5b66\u4e60\u3001DINO\u548c\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u3002\u5bf9\u6bcf\u79cdSSP\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e0d\u540c\u6570\u91cf\u7684\u53d7\u8bd5\u8005\u5b50\u96c6\u8fdb\u884c2D\u6a21\u578b\u7684\u6709\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u4e00\u4e2a2D\u57fa\u7ebf\u6a21\u578b\u3002\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e0e\u57fa\u7ebf\u6a21\u578b\u5728140\u540d\u53d7\u8bd5\u8005\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u4f7f\u75283D Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u8fdb\u884c\u6bd4\u8f83\u3002SSP\u65b9\u6cd5\u5728\u6700\u5927\u6709\u76d1\u7763\u5fae\u8c03\u5b50\u96c6\u4e0a\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u6ca1\u6709\u6027\u80fd\u63d0\u5347\uff08DSC = 0.89\uff09\u3002\u5f53\u4ec5\u670910\u540d\u53d7\u8bd5\u8005\uff08231\u5f202D\u5207\u7247\uff09\u53ef\u7528\u4e8e\u6709\u76d1\u7763\u8bad\u7ec3\u65f6\uff0c\u4f7f\u7528MIM\u7684SSP\uff08DSC = 0.86\uff09\u4f18\u4e8e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff08DSC = 0.82\uff09\u3002\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\uff0cSSP\u5bf9CMR\u7535\u5f71\u5206\u5272\u5177\u6709\u4ef7\u503c\uff0c\u4f46\u5728\u6709\u5145\u8db3\u6807\u6ce8\u6570\u636e\u65f6\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6ca1\u6709\u5e2e\u52a9\u3002\u6b64\u5916\uff0cSSP\u65b9\u6cd5\u7684\u9009\u62e9\u5f88\u91cd\u8981\u3002\u4ee3\u7801\u516c\u5f00\u5728\uff1ahttps://github.com/q-cardIA/ssp-cmr-cine-segmentation\u3002 Rob A. J. de Mooij PDF N/A Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation Self-supervised pretraining (SSP) has shown promising results in learning from large unlabeled datasets and, thus, could be useful for automated cardiovascular magnetic resonance (CMR) short-axis cine segmentation. However, inconsistent reports of the benefits of SSP for segmentation have made it difficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP methods for CMR cine segmentation.   To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were used for unlabeled pretraining with four SSP methods; SimCLR, positional contrastive learning, DINO, and masked image modeling (MIM). Subsets of varying numbers of subjects were used for supervised fine-tuning of 2D models for each SSP method, as well as to train a 2D baseline model from scratch. The fine-tuned models were compared to the baseline using the 3D Dice similarity coefficient (DSC) in a test dataset of 140 subjects.   The SSP methods showed no performance gains with the largest supervised fine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects (231 2D slices) are available for supervised training, SSP using MIM (DSC = 0.86) improves over training from scratch (DSC = 0.82).   This study found that SSP is valuable for CMR cine segmentation when labeled training data is scarce, but does not aid state-of-the-art deep learning methods when ample labeled data is available. Moreover, the choice of SSP method is important. The code is publicly available at: https://github.com/q-cardIA/ssp-cmr-cine-segmentation EfficientCrackNet\uff1a\u4e00\u79cd\u7528\u4e8e\u88c2\u7f1d\u5206\u5272\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b \u88c2\u7f1d\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u4ece\u8def\u9762\u56fe\u50cf\u4e2d\u8fdb\u884c\u68c0\u6d4b\uff0c\u7531\u4e8e\u5b58\u5728\u5f3a\u5ea6\u4e0d\u5747\u5300\u3001\u590d\u6742\u62d3\u6251\u7ed3\u6784\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u566a\u58f0\u80cc\u666f\u7b49\u56fa\u6709\u590d\u6742\u6027\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6784\u6210\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002\u81ea\u52a8\u5316\u88c2\u7f1d\u68c0\u6d4b\u5bf9\u4e8e\u7ef4\u62a4\u5efa\u7b51\u7269\u3001\u8def\u9762\u548c\u6865\u6881\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5e38\u5e38\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3001\u88c2\u7f1d\u6a21\u5f0f\u590d\u6742\u548c\u80cc\u666f\u56f0\u96be\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u68c0\u6d4b\u4e0d\u51c6\u786e\u4e14\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86EfficientCrackNet\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u53d8\u538b\u5668\uff08transformers\uff09\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u88c2\u7f1d\u5206\u5272\u3002EfficientCrackNet\u96c6\u6210\u4e86\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff08DSC\uff09\u5c42\u548cMobileViT\u5757\uff0c\u4ee5\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u3002\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u8fb9\u7f18\u63d0\u53d6\u65b9\u6cd5\uff08EEM\uff09\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u88c2\u7f1d\u8fb9\u7f18\u68c0\u6d4b\uff0c\u5e76\u4f7f\u7528\u8d85\u8f7b\u91cf\u7ea7\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff08ULSAM\uff09\u6765\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u3002\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6Crack500\u3001DeepCrack\u548cGAPs384\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEfficientCrackNet\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u540c\u65f6\u4ec5\u97000.26M\u53c2\u6570\u548c0.483 GFLOPs\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5e76\u4e3a\u5b9e\u9645\u88c2\u7f1d\u5206\u5272\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002 Abid Hasan Zim PDF N/A EfficientCrackNet: A Lightweight Model for Crack Segmentation Crack detection, particularly from pavement images, presents a formidable challenge in the domain of computer vision due to several inherent complexities such as intensity inhomogeneity, intricate topologies, low contrast, and noisy backgrounds. Automated crack detection is crucial for maintaining the structural integrity of essential infrastructures, including buildings, pavements, and bridges. Existing lightweight methods often face challenges including computational inefficiency, complex crack patterns, and difficult backgrounds, leading to inaccurate detection and impracticality for real-world applications. To address these limitations, we propose EfficientCrackNet, a lightweight hybrid model combining Convolutional Neural Networks (CNNs) and transformers for precise crack segmentation. EfficientCrackNet integrates depthwise separable convolutions (DSC) layers and MobileViT block to capture both global and local features. The model employs an Edge Extraction Method (EEM) and for efficient crack edge detection without pretraining, and Ultra-Lightweight Subspace Attention Module (ULSAM) to enhance feature extraction. Extensive experiments on three benchmark datasets Crack500, DeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superior performance compared to existing lightweight models, while requiring only 0.26M parameters, and 0.483 FLOPs (G). The proposed model offers an optimal balance between accuracy and computational efficiency, outperforming state-of-the-art lightweight models, and providing a robust and adaptable solution for real-world crack segmentation. DiffSSC\uff1a\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u5b9e\u73b0\u8bed\u4e49\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5b8c\u6210 \u611f\u77e5\u7cfb\u7edf\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u4f20\u611f\u5668\u548c\u76f8\u5e94\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u3002\u4e09\u7ef4\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u5e7f\u6cdb\u7528\u4e8e\u6355\u6349\u8f66\u8f86\u5468\u56f4\u73af\u5883\u7684\u7a00\u758f\u70b9\u4e91\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fd9\u4e9b\u70b9\u4e91\u7684\u7a00\u758f\u6027\u548c\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u6b64\u7c7b\u7cfb\u7edf\u96be\u4ee5\u611f\u77e5\u88ab\u906e\u6321\u533a\u57df\u548c\u573a\u666f\u4e2d\u7684\u95f4\u9699\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u7ed3\u5408\u4e86\u672a\u89c2\u5bdf\u5230\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u5b8c\u6574\u7684\u573a\u666f\u8868\u793a\u3002\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u53d6\u5f97\u7684\u663e\u8457\u6210\u679c\uff0c\u6211\u4eec\u63d0\u51fa\u5c06\u5176\u6269\u5c55\u5230SSC\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5206\u522b\u5728\u70b9\u548c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u566a\u58f0\u5316\u548c\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u3002\u4e3a\u4e86\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\uff0c\u6211\u4eec\u91c7\u7528\u8bed\u4e49\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5e76\u8bbe\u8ba1\u5c40\u90e8\u548c\u5168\u5c40\u6b63\u5219\u5316\u635f\u5931\u4ee5\u7a33\u5b9a\u53bb\u566a\u8fc7\u7a0b\u3002\u6211\u4eec\u5728\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728SSC\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002 Helin Cao PDF N/A DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models Perception systems play a crucial role in autonomous driving, incorporating multiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors are widely used to capture sparse point clouds of the vehicle's surroundings. However, such systems struggle to perceive occluded areas and gaps in the scene due to the sparsity of these point clouds and their lack of semantics. To address these challenges, Semantic Scene Completion (SSC) jointly predicts unobserved geometry and semantics in the scene given raw LiDAR measurements, aiming for a more complete scene representation. Building on promising results of diffusion models in image generation and super-resolution tasks, we propose their extension to SSC by implementing the noising and denoising diffusion processes in the point and semantic spaces individually. To control the generation, we employ semantic LiDAR point clouds as conditional input and design local and global regularization losses to stabilize the denoising process. We evaluate our approach on autonomous driving datasets and our approach outperforms the state-of-the-art for SSC. GSON\uff1a\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u7684\u793e\u4f1a\u5bfc\u822a\u6846\u67b6\uff0c\u91c7\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b \u968f\u7740\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u670d\u52a1\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u5b83\u4eec\u7684\u9700\u6c42\u5df2\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u5bfc\u822a\u81f3\u76ee\u7684\u5730\u3002\u5b83\u4eec\u8fd8\u5fc5\u987b\u8003\u8651\u52a8\u6001\u7684\u793e\u4f1a\u60c5\u5883\uff0c\u786e\u4fdd\u5728\u5171\u4eab\u7a7a\u95f4\u4e2d\u5bf9\u4ed6\u4eba\u8868\u73b0\u51fa\u5c0a\u91cd\u548c\u8212\u9002\uff0c\u8fd9\u5bf9\u611f\u77e5\u548c\u89c4\u5212\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7fa4\u4f53\u7684\u793e\u4f1a\u5bfc\u822a\u6846\u67b6GSON\uff0c\u901a\u8fc7\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u79fb\u52a8\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u5e76\u5229\u7528\u5468\u56f4\u7684\u793e\u4f1a\u7fa4\u4f53\u3002\u5728\u611f\u77e5\u65b9\u9762\uff0c\u6211\u4eec\u5e94\u7528\u89c6\u89c9\u63d0\u793a\u6280\u672f\uff0c\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u63d0\u53d6\u884c\u4eba\u4e4b\u95f4\u7684\u793e\u4f1a\u5173\u7cfb\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u9c81\u68d2\u7684\u884c\u4eba\u68c0\u6d4b\u548c\u8ddf\u8e2a\u6d41\u7a0b\u7ed3\u5408\uff0c\u4ee5\u7f13\u89e3LMM\u63a8\u7406\u901f\u5ea6\u4f4e\u7684\u95ee\u9898\u3002\u5728\u83b7\u5f97\u611f\u77e5\u7ed3\u679c\u540e\uff0c\u89c4\u5212\u7cfb\u7edf\u88ab\u8bbe\u8ba1\u4e3a\u907f\u514d\u7834\u574f\u5f53\u524d\u7684\u793e\u4f1a\u7ed3\u6784\u3002\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u793e\u4f1a\u7ed3\u6784\u7684\u4e2d\u95f4\u5c42\u89c4\u5212\u5668\u4f5c\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u5c40\u90e8\u8fd0\u52a8\u89c4\u5212\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u53cd\u5e94\u6027\u54cd\u5e94\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d89\u53ca\u590d\u6742\u793e\u4f1a\u7ed3\u6784\u7406\u89e3\u548c\u63a8\u7406\u7684\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u591a\u4e2a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002 Shangyi Luo PDF N/A GSON: A Group-based Social Navigation Framework with Large Multimodal Model As the number of service robots and autonomous vehicles in human-centered environments grows, their requirements go beyond simply navigating to a destination. They must also take into account dynamic social contexts and ensure respect and comfort for others in shared spaces, which poses significant challenges for perception and planning. In this paper, we present a group-based social navigation framework GSON to enable mobile robots to perceive and exploit the social group of their surroundings by leveling the visual reasoning capability of the Large Multimodal Model (LMM). For perception, we apply visual prompting techniques to zero-shot extract the social relationship among pedestrians and combine the result with a robust pedestrian detection and tracking pipeline to alleviate the problem of low inference speed of the LMM. Given the perception result, the planning system is designed to avoid disrupting the current social structure. We adopt a social structure-based mid-level planner as a bridge between global path planning and local motion planning to preserve the global context and reactive response. The proposed method is validated on real-world mobile robot navigation tasks involving complex social structure understanding and reasoning. Experimental results demonstrate the effectiveness of the system in these scenarios compared with several baselines. SKT\uff1a\u5c06\u72b6\u6001\u611f\u77e5\u7684\u5173\u952e\u70b9\u8f68\u8ff9\u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c \u81ea\u52a8\u5316\u5904\u7406\u670d\u88c5\u5bf9\u8f85\u52a9\u673a\u5668\u4eba\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u670d\u88c5\u5177\u6709\u591a\u6837\u6027\u548c\u53ef\u53d8\u5f62\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u79cd\u670d\u88c5\u7c7b\u578b\u5355\u72ec\u5efa\u6a21\uff0c\u8fd9\u9650\u5236\u4e86\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8de8\u4e0d\u540c\u670d\u88c5\u7c7b\u522b\u7684\u5173\u952e\u70b9\u9884\u6d4b\u3002\u901a\u8fc7\u89e3\u8bfb\u89c6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u7ba1\u7406\u4e0d\u540c\u7684\u670d\u88c5\u72b6\u6001\u3002\u6211\u4eec\u5229\u7528\u5148\u8fdb\u7684\u6a21\u62df\u6280\u672f\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u5f97\u65e0\u9700\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5373\u53ef\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e3a\u673a\u5668\u4eba\u670d\u88c5\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u8fd9\u9879\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86VLM\u5728\u5355\u4e00\u6846\u67b6\u5185\u7edf\u4e00\u5404\u79cd\u670d\u88c5\u5904\u7406\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728\u5bb6\u5c45\u81ea\u52a8\u5316\u548c\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002 Xin Li PDF N/A SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and adaptability. In contrast, this paper presents a unified approach using vision-language models (VLMs) to improve keypoint prediction across various garment categories. By interpreting both visual and semantic information, our model enables robots to manage different garment states with a single model. We created a large-scale synthetic dataset using advanced simulation techniques, allowing scalable training without extensive real-world data. Experimental results indicate that the VLM-based method significantly enhances keypoint detection accuracy and task success rates, providing a more flexible and general solution for robotic garment manipulation. In addition, this research also underscores the potential of VLMs to unify various garment manipulation tasks within a single framework, paving the way for broader applications in home automation and assistive robotics for future. \u5728\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e4b\u524d\u63a8\u65ad\u4eba\u7c7b\u7684\u610f\u56fe \u4e3a\u4e86\u8ba9AI\u4ee3\u7406\u80fd\u591f\u5bf9\u4eba\u7c7b\u6709\u6240\u5e2e\u52a9\uff0c\u5b83\u4eec\u5e94\u5f53\u80fd\u591f\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5b8c\u6210\u65e5\u5e38\u7684\u5408\u4f5c\u4efb\u52a1\u3002\u7136\u800c\uff0c\u771f\u5b9e\u7684\u4eba\u7c7b\u6307\u4ee4\u672c\u8eab\u5c31\u5177\u6709\u6a21\u7cca\u6027\uff0c\u56e0\u4e3a\u8bf4\u8bdd\u8005\u5047\u8bbe\u542c\u8005\u5bf9\u5176\u9690\u542b\u7684\u76ee\u6807\u548c\u610f\u56fe\u6709\u8db3\u591f\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u6807\u51c6\u7684\u8bed\u8a00\u7406\u89e3\u548c\u89c4\u5212\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u8fd9\u79cd\u6a21\u7cca\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u5c06\u4eba\u7c7b\u7684\u5185\u5728\u76ee\u6807\u5efa\u6a21\u4e3a\u73af\u5883\u4e2d\u7684\u989d\u5916\u90e8\u5206\u53ef\u89c2\u5bdf\u56e0\u7d20\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u540d\u4e3a\u201c\u9075\u5faa\u6307\u4ee4\u5e76\u8fdb\u884c\u793e\u4f1a\u548c\u5177\u8eab\u63a8\u7406\u201d\uff08Follow Instructions with Social and Embodied Reasoning, FISER\uff09\uff0c\u65e8\u5728\u66f4\u597d\u5730\u9075\u5faa\u534f\u4f5c\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002\u6211\u4eec\u7684\u6846\u67b6\u660e\u786e\u5730\u5c06\u4eba\u7c7b\u76ee\u6807\u548c\u610f\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u63a8\u65ad\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5e76\u5728\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u2014\u2014HandMeThat\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5236\u5b9a\u884c\u52a8\u8ba1\u5212\u4e4b\u524d\uff0c\u4f7f\u7528\u793e\u4f1a\u63a8\u7406\u6765\u660e\u786e\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\u7684\u65b9\u6cd5\u4f18\u4e8e\u7eaf\u7cb9\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u5c06\u5176\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5305\u62ec\u5728\u6700\u5927\u53ef\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u201c\u601d\u7ef4\u94fe\u201d\u63d0\u793a\uff0c\u53d1\u73b0FISER\u5728\u6240\u7814\u7a76\u7684\u5177\u8eab\u793e\u4f1a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u8fbe\u5230\u4e86HandMeThat\u4e0a\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002 Yanming Wan PDF N/A Infer Human's Intentions Before Following Natural Language Instructions For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-the-art on HandMeThat."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}