{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract PhysGen\uff1a\u57fa\u4e8e\u521a\u4f53\u7269\u7406\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210 \u6211\u4eec\u63d0\u51fa\u4e86PhysGen\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u5b83\u5c06\u5355\u5f20\u56fe\u50cf\u548c\u8f93\u5165\u6761\u4ef6\uff08\u4f8b\u5982\uff0c\u65bd\u52a0\u5728\u56fe\u50cf\u4e2d\u7269\u4f53\u4e0a\u7684\u529b\u548c\u626d\u77e9\uff09\u8f6c\u6362\u4e3a\u751f\u6210\u903c\u771f\u3001\u7269\u7406\u5408\u7406\u4e14\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u9891\u3002\u6211\u4eec\u7684\u5173\u952e\u6d1e\u5bdf\u662f\u5c06\u57fa\u4e8e\u6a21\u578b\u7684\u7269\u7406\u6a21\u62df\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u4fe1\u7684\u56fe\u50cf\u7a7a\u95f4\u52a8\u529b\u5b66\u3002\u6211\u4eec\u7cfb\u7edf\u7684\u5fc3\u810f\u662f\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\uff08i\uff09\u4e00\u4e2a\u56fe\u50cf\u7406\u89e3\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u56fe\u50cf\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u6750\u6599\u548c\u7269\u7406\u53c2\u6570\uff1b\uff08ii\uff09\u4e00\u4e2a\u56fe\u50cf\u7a7a\u95f4\u52a8\u529b\u5b66\u6a21\u62df\u6a21\u578b\uff0c\u5229\u7528\u521a\u4f53\u7269\u7406\u5b66\u548c\u63a8\u65ad\u7684\u53c2\u6570\u6765\u6a21\u62df\u771f\u5b9e\u884c\u4e3a\uff1b\uff08iii\uff09\u4e00\u4e2a\u57fa\u4e8e\u56fe\u50cf\u7684\u6e32\u67d3\u548c\u7ec6\u5316\u6a21\u5757\uff0c\u5229\u7528\u751f\u6210\u89c6\u9891\u6269\u6563\u6765\u751f\u6210\u5305\u542b\u6a21\u62df\u8fd0\u52a8\u7684\u903c\u771f\u89c6\u9891\u7247\u6bb5\u3002\u751f\u6210\u7684\u89c6\u9891\u5728\u7269\u7406\u548c\u5916\u89c2\u4e0a\u90fd\u5341\u5206\u903c\u771f\uff0c\u751a\u81f3\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\uff0c\u901a\u8fc7\u5b9a\u91cf\u6bd4\u8f83\u548c\u5168\u9762\u7684\u7528\u6237\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u9a71\u52a8\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u5de5\u4f5c\u7684\u5353\u8d8a\u7ed3\u679c\u3002PhysGen\u751f\u6210\u7684\u89c6\u9891\u53ef\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\uff0c\u4f8b\u5982\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u903c\u771f\u52a8\u753b\uff0c\u6216\u5141\u8bb8\u7528\u6237\u4e0e\u56fe\u50cf\u4e92\u52a8\u5e76\u521b\u5efa\u5404\u79cd\u52a8\u6001\u6548\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://stevenlsw.github.io/physgen/ Shaowei Liu PDF N/A PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation We present PhysGen, a novel image-to-video generation method that converts a single image and an input condition (e.g., force and torque applied to an object in the image) to produce a realistic, physically plausible, and temporally consistent video. Our key insight is to integrate model-based physical simulation with a data-driven video generation process, enabling plausible image-space dynamics. At the heart of our system are three core components: (i) an image understanding module that effectively captures the geometry, materials, and physical parameters of the image; (ii) an image-space dynamics simulation model that utilizes rigid-body physics and inferred parameters to simulate realistic behaviors; and (iii) an image-based rendering and refinement module that leverages generative video diffusion to produce realistic video footage featuring the simulated motion. The resulting videos are realistic in both physics and appearance and are even precisely controllable, showcasing superior results over existing data-driven image-to-video generation works through quantitative comparison and comprehensive user study. PhysGen's resulting videos can be used for various downstream applications, such as turning an image into a realistic animation or allowing users to interact with the image and create various dynamics. Project page: https://stevenlsw.github.io/physgen/ \u63a2\u7d22\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u4ee4\u724c\u526a\u679d\u6280\u672f \u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u76f8\u8f83\u4e8eTransformer\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5177\u6709\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u4f18\u52bf\uff0c\u5e76\u5df2\u88ab\u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\uff0c\u6210\u4e3a\u4e00\u79cd\u65b0\u578b\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002\u53d7\u5230\u89c6\u89c9Transformer\uff08ViTs\uff09\u4e2d\u6700\u7ec8\u9884\u6d4b\u4ec5\u57fa\u4e8e\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5b50\u96c6\u7684\u89c2\u5bdf\u542f\u53d1\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u4e00\u9879\u521b\u65b0\u6b65\u9aa4\uff0c\u901a\u8fc7\u57fa\u4e8e\u6807\u8bb0\u7684\u526a\u679d\u6765\u63d0\u9ad8\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u6a21\u578b\u7684\u6548\u7387\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528\u4e3aViTs\u8bbe\u8ba1\u7684\u73b0\u6709\u6807\u8bb0\u526a\u679d\u6280\u672f\u65e0\u6cd5\u5e26\u6765\u826f\u597d\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u7ecf\u8fc7\u5e7f\u6cdb\u7684\u5fae\u8c03\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86SSMs\u72ec\u7279\u7684\u8ba1\u7b97\u7279\u6027\uff0c\u5e76\u53d1\u73b0\u7b80\u5355\u5e94\u7528\u4f1a\u7834\u574f\u6807\u8bb0\u7684\u987a\u5e8f\u4f4d\u7f6e\u3002\u8fd9\u4e00\u6d1e\u5bdf\u4fc3\u4f7f\u6211\u4eec\u4e3a\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u6a21\u578b\u8bbe\u8ba1\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u6807\u8bb0\u526a\u679d\u65b9\u6cd5\u3002\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u4e00\u79cd\u526a\u679d\u611f\u77e5\u9690\u85cf\u72b6\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u7a33\u5b9a\u5269\u4f59\u6807\u8bb0\u7684\u90bb\u57df\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6211\u4eec\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94SSM\u6a21\u578b\u7684\u6807\u8bb0\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u6307\u5bfc\u6807\u8bb0\u526a\u679d\u3002\u901a\u8fc7\u9ad8\u6548\u7684\u5b9e\u73b0\u548c\u5b9e\u9645\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5e26\u6765\u4e86\u5b9e\u9645\u7684\u52a0\u901f\u6548\u679c\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u6700\u5c0f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5728ImageNet\u4e0a\u5b9e\u73b0\u4e8681.7%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5bf9\u526a\u679d\u540e\u7684PlainMamba-L3\u6a21\u578b\u51cf\u5c11\u4e8641.6%\u7684FLOPs\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u7406\u89e3\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u6a21\u578b\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 Zheng Zhan PDF N/A Exploring Token Pruning in Vision State Space Models State Space Models (SSMs) have the advantage of keeping linear computational complexity compared to attention modules in transformers, and have been applied to vision tasks as a new type of powerful vision foundation model. Inspired by the observations that the final prediction in vision transformers (ViTs) is only based on a subset of most informative tokens, we take the novel step of enhancing the efficiency of SSM-based vision models through token-based pruning. However, direct applications of existing token pruning techniques designed for ViTs fail to deliver good performance, even with extensive fine-tuning. To address this issue, we revisit the unique computational characteristics of SSMs and discover that naive application disrupts the sequential token positions. This insight motivates us to design a novel and general token pruning method specifically for SSM-based vision models. We first introduce a pruning-aware hidden state alignment method to stabilize the neighborhood of remaining tokens for performance enhancement. Besides, based on our detailed analysis, we propose a token importance evaluation method adapted for SSM models, to guide the token pruning. With efficient implementation and practical acceleration methods, our method brings actual speedup. Extensive experiments demonstrate that our approach can achieve significant computation reduction with minimal impact on performance across different tasks. Notably, we achieve 81.7\\% accuracy on ImageNet with a 41.6\\% reduction in the FLOPs for pruned PlainMamba-L3. Furthermore, our work provides deeper insights into understanding the behavior of SSM-based vision models for future research. ProMerge: \u65e0\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u7684\u63d0\u793a\u4e0e\u5408\u5e76 \u65e0\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u65e8\u5728\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u56fe\u50cf\u4e2d\u7684\u4e0d\u540c\u5bf9\u8c61\u5b9e\u4f8b\u8fdb\u884c\u5206\u5272\u3002\u8fd9\u4e00\u9886\u57df\u8fd1\u671f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u90e8\u5206\u539f\u56e0\u5728\u4e8e\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982DINO\uff09\u63d0\u4f9b\u7684\u4e30\u5bcc\u89c6\u89c9\u7279\u5f81\u8868\u793a\u6240\u5e26\u6765\u7684\u5f3a\u5927\u5c40\u90e8\u5bf9\u5e94\u5173\u7cfb\u3002\u6700\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5229\u7528\u81ea\u76d1\u7763\u7279\u5f81\u5c06\u56fe\u50cf\u8868\u793a\u4e3a\u56fe\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u5e7f\u4e49\u7279\u5f81\u503c\u7cfb\u7edf\uff08\u5373\u5f52\u4e00\u5316\u5206\u5272\uff09\u6765\u751f\u6210\u524d\u666f\u63a9\u7801\u3002\u5c3d\u7ba1\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u5176\u4f34\u968f\u7684\u8ba1\u7b97\u9700\u6c42\u5bfc\u81f4\u4e86\u8f83\u6162\u7684\u63a8\u7406\u901f\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Prompt and Merge\uff08ProMerge\uff09\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u81ea\u76d1\u7763\u89c6\u89c9\u7279\u5f81\u83b7\u53d6\u521d\u59cb\u7684\u8865\u4e01\u5206\u7ec4\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u7b56\u7565\u6027\u7684\u5408\u5e76\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u5206\u6bb5\u8fdb\u884c\u5904\u7406\uff0c\u540c\u65f6\u501f\u52a9\u4e00\u79cd\u590d\u6742\u7684\u57fa\u4e8e\u80cc\u666f\u7684\u63a9\u7801\u4fee\u526a\u6280\u672f\u3002ProMerge\u4e0d\u4ec5\u4ea7\u751f\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u800c\u4e14\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u76f8\u6bd4\u57fa\u4e8e\u5f52\u4e00\u5316\u5206\u5272\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u3002\u6b64\u5916\uff0c\u5f53\u4f7f\u7528\u6211\u4eec\u7684\u63a9\u7801\u9884\u6d4b\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u65f6\uff0c\u6240\u5f97\u5230\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u5b9e\u4f8b\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5f53\u524d\u9886\u5148\u7684\u975e\u76d1\u7763\u6a21\u578b\u3002 Dylan Li PDF N/A ProMerge: Prompt and Merge for Unsupervised Instance Segmentation Unsupervised instance segmentation aims to segment distinct object instances in an image without relying on human-labeled data. This field has recently seen significant advancements, partly due to the strong local correspondences afforded by rich visual feature representations from self-supervised models (e.g., DINO). Recent state-of-the-art approaches use self-supervised features to represent images as graphs and solve a generalized eigenvalue system (i.e., normalized-cut) to generate foreground masks. While effective, this strategy is limited by its attendant computational demands, leading to slow inference speeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages self-supervised visual features to obtain initial groupings of patches and applies a strategic merging to these segments, aided by a sophisticated background-based mask pruning technique. ProMerge not only yields competitive results but also offers a significant reduction in inference time compared to state-of-the-art normalized-cut-based approaches. Furthermore, when training an object detector using our mask predictions as pseudo-labels, the resulting detector surpasses the current leading unsupervised model on various challenging instance segmentation benchmarks. \u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\uff0c\u6269\u6563\u6982\u7387\u6a21\u578b\u7684$O(d/T)$\u6536\u655b\u7406\u8bba \u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u9006\u8f6c\u4e00\u4e2a\u6269\u6563\u8fc7\u7a0b\u6765\u751f\u6210\u65b0\u6570\u636e\uff0c\u8be5\u8fc7\u7a0b\u5c06\u6765\u81ea\u76ee\u6807\u5206\u5e03\u7684\u6570\u636e\u6270\u52a8\u4e3a\u566a\u58f0\uff0c\u5728\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u5c3d\u7ba1\u5b83\u4eec\u5728\u7ecf\u9a8c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u73b0\u6709\u7684\u7406\u8bba\u4fdd\u8bc1\u5f80\u5f80\u53d7\u5230\u4e25\u683c\u5047\u8bbe\u6216\u6b21\u4f18\u6536\u655b\u7387\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5728\u6700\u5c0f\u7684\u5047\u8bbe\u4e0b\u4e3a\u4e00\u79cd\u6d41\u884c\u7684\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u7684\u91c7\u6837\u5668\u5efa\u7acb\u4e86\u5feb\u901f\u6536\u655b\u7406\u8bba\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5728\u63d0\u4f9b$\\ell_{2}$\u51c6\u786e\u4f30\u8ba1\u7684\u5206\u6570\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u76ee\u6807\u5206\u5e03\u4e0e\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u603b\u53d8\u5dee\u8ddd\u79bb\u88ab\u4e0a\u754c\u4e3a$O(d/T)$\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\uff0c\u5176\u4e2d$d$\u662f\u6570\u636e\u7ef4\u5ea6\uff0c$T$\u662f\u6b65\u6570\u3002\u8fd9\u4e00\u7ed3\u679c\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u6709\u9650\u4e00\u9636\u77e9\u7684\u76ee\u6807\u5206\u5e03\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u4e0d\u4ec5\u6539\u8fdb\u4e86\u57fa\u4e8eSDE\u7684\u91c7\u6837\u5668\u7684\u73b0\u6709\u6536\u655b\u7406\u8bba\uff0c\u8fd8\u6539\u8fdb\u4e86\u53e6\u4e00\u79cd\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u7684\u91c7\u6837\u5668\u7684\u6536\u655b\u7406\u8bba\uff0c\u540c\u65f6\u5bf9\u76ee\u6807\u6570\u636e\u5206\u5e03\u548c\u5206\u6570\u4f30\u8ba1\u65bd\u52a0\u4e86\u6700\u5c0f\u7684\u5047\u8bbe\u3002\u8fd9\u4e00\u6210\u679c\u662f\u901a\u8fc7\u4e00\u7ec4\u65b0\u9896\u7684\u5206\u6790\u5de5\u5177\u5b9e\u73b0\u7684\uff0c\u8fd9\u4e9b\u5de5\u5177\u63d0\u4f9b\u4e86\u5bf9\u53cd\u5411\u8fc7\u7a0b\u4e2d\u6bcf\u4e00\u6b65\u8bef\u5dee\u4f20\u64ad\u7684\u7cbe\u7ec6\u63cf\u8ff0\u3002 Gen Li PDF N/A $O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical guarantees are often constrained by stringent assumptions or suboptimal convergence rates. In this paper, we establish a fast convergence theory for a popular SDE-based sampler under minimal assumptions. Our analysis shows that, provided $\\ell_{2}$-accurate estimates of the score functions, the total variation distance between the target and generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionality and $T$ is the number of steps. This result holds for any target distribution with finite first-order moment. To our knowledge, this improves upon existing convergence theory for both the SDE-based sampler and another ODE-based sampler, while imposing minimal assumptions on the target data distribution and score estimates. This is achieved through a novel set of analytical tools that provides a fine-grained characterization of how the error propagates at each step of the reverse process. LML\uff1a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u6570\u636e\u96c6\u4ee5\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u9884\u6d4b \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u7531\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\u5904\u7406\u3002\u4e0e\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u6e05\u6d17\u548c\u7279\u5f81\u5de5\u7a0b\u7684ML\u6a21\u578b\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7LLMs\u7b80\u5316\u4e86\u6d41\u7a0b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\uff08LML\uff09\u201d\u7684\u65b0\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u7531\u4e00\u79cd\u540d\u4e3a\u201c\u6570\u636e\u589e\u5f3a\u9884\u6d4b\uff08DAP\uff09\u201d\u7684\u65b0\u65b9\u6cd5\u9a71\u52a8\u3002\u5206\u7c7b\u8fc7\u7a0b\u901a\u8fc7LLMs\u8fdb\u884c\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u624b\u52a8\u63a2\u7d22\u548c\u7406\u89e3\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u6570\u636e\u4f5c\u4e3a\u53c2\u8003\u6765\u51b3\u5b9a\u5206\u7c7b\u3002\u8bad\u7ec3\u6570\u636e\u88ab\u603b\u7ed3\u548c\u8bc4\u4f30\uff0c\u4ee5\u786e\u5b9a\u5bfc\u81f4\u6bcf\u4e2a\u6807\u7b7e\u5206\u7c7b\u7684\u6700\u663e\u8457\u7279\u5f81\u3002\u5728DAP\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f7f\u7528\u6570\u636e\u6458\u8981\u81ea\u52a8\u521b\u5efa\u67e5\u8be2\uff0c\u7528\u4e8e\u4ece\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u76f8\u5173\u884c\u3002LLM\u4f7f\u7528\u6570\u636e\u6458\u8981\u548c\u76f8\u5173\u884c\u751f\u6210\u5206\u7c7b\uff0c\u5373\u4f7f\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u65f6\u4e5f\u80fd\u786e\u4fdd\u4ee4\u4eba\u6ee1\u610f\u7684\u51c6\u786e\u6027\u3002DAP\u4e2d\u4f7f\u7528\u6570\u636e\u6458\u8981\u548c\u76f8\u4f3c\u6570\u636e\u786e\u4fdd\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51b3\u7b56\u5236\u5b9a\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u793a\u4e2d\u4f7f\u7528\u201c\u5145\u5f53\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u201d\u7684\u8bcd\u8bed\uff0c\u4ee5\u589e\u5f3a\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5141\u8bb8\u7528\u6237\u5ba1\u67e5\u6bcf\u4e2a\u9884\u6d4b\u80cc\u540e\u7684\u903b\u8f91\u3002\u5728\u4e00\u4e9b\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u7cfb\u7edf\u7684\u51c6\u786e\u7387\u8d85\u8fc7\u4e8690%\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8d85\u8d8a\u4f20\u7edfML\u6a21\u578b\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u53ef\u5728https://github.com/Pro-GenAI/LML-DAP\u83b7\u53d6\u3002 Praneeth Vadlapati PDF N/A LML: Language Model Learning a Dataset for Data-Augmented Prediction This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called \"Language Model Learning (LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words \"Act as an Explainable Machine Learning Model\" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP RepairBench\uff1a\u7a0b\u5e8f\u4fee\u590d\u524d\u6cbf\u6a21\u578b\u7684\u6392\u884c\u699c AI\u9a71\u52a8\u7684\u7a0b\u5e8f\u4fee\u590d\u5229\u7528AI\u6a21\u578b\u901a\u8fc7\u751f\u6210\u8865\u4e01\u6765\u4fee\u590d\u6709\u7f3a\u9677\u7684\u8f6f\u4ef6\u3002AI\u6280\u672f\u7684\u8fc5\u901f\u8fdb\u6b65\u65e0\u7591\u4f1a\u5f71\u54cd\u7a0b\u5e8f\u4fee\u590d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u7136\u800c\uff0c\u8981\u628a\u63e1\u8fd9\u4e00\u8fdb\u5c55\uff0c\u9700\u8981\u9891\u7e41\u4e14\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u63d0\u51fa\u4e86RepairBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8eAI\u9a71\u52a8\u7a0b\u5e8f\u4fee\u590d\u7684\u65b0\u578b\u6392\u884c\u699c\u3002RepairBench\u7684\u5173\u952e\u7279\u70b9\u662f\uff1a1) \u57fa\u4e8e\u6267\u884c\uff1a\u6240\u6709\u8865\u4e01\u90fd\u4f1a\u9488\u5bf9\u6d4b\u8bd5\u5957\u4ef6\u8fdb\u884c\u7f16\u8bd1\u548c\u6267\u884c\uff1b2) \u5b83\u4ee5\u9891\u7e41\u4e14\u6807\u51c6\u5316\u7684\u65b9\u5f0f\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u3002RepairBench\u5229\u7528\u4e24\u4e2a\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\u2014\u2014Defects4J\u548cGitBug-Java\uff0c\u6765\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86RepairBench\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u968f\u7740\u65b0\u524d\u6cbf\u6a21\u578b\u7684\u53d1\u5e03\uff0c\u6211\u4eec\u5c06\u66f4\u65b0\u6392\u884c\u699c\u3002 Andr\u00e9 Silva PDF N/A RepairBench: Leaderboard of Frontier Models for Program Repair AI-driven program repair uses AI models to repair buggy software by producing patches. Rapid advancements in AI surely impact state-of-the-art performance of program repair. Yet, grasping this progress requires frequent and standardized evaluations. We propose RepairBench, a novel leaderboard for AI-driven program repair. The key characteristics of RepairBench are: 1) it is execution-based: all patches are compiled and executed against a test suite, 2) it assesses frontier models in a frequent and standardized way. RepairBench leverages two high-quality benchmarks, Defects4J and GitBug-Java, to evaluate frontier models against real-world program repair tasks. We publicly release the evaluation framework of RepairBench. We will update the leaderboard as new frontier models are released. \u5149\u8c31\u5c0f\u6ce2\u4e22\u5f03\uff1a\u5c0f\u6ce2\u57df\u4e2d\u7684\u6b63\u5219\u5316 \u6b63\u5219\u5316\u6280\u672f\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fc7\u62df\u5408\u7684\u4e00\u4e2a\u539f\u56e0\u662f\u7f51\u7edc\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u590d\u6742\u7684\u534f\u540c\u9002\u5e94\uff0c\u8fd9\u4f7f\u5f97CNN\u4f9d\u8d56\u4e8e\u5b83\u4eec\u7684\u8054\u5408\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u9f13\u52b1\u6bcf\u4e2a\u90e8\u5206\u72ec\u7acb\u5b66\u4e60\u6709\u7528\u7684\u7279\u5f81\u8868\u793a\u3002\u9891\u57df\u64cd\u4f5c\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u9891\u7387\u5206\u89e3\u6765\u4fee\u6539\u5177\u6709\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u6570\u636e\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u8c31\u5c0f\u6ce2\u4e22\u5f03\uff08Spectral Wavelet Dropout, SWD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a1D-SWD\u548c2D-SWD\u3002\u8fd9\u4e9b\u53d8\u4f53\u901a\u8fc7\u968f\u673a\u4e22\u5f03\u7279\u5f81\u56fe\u79bb\u6563\u5c0f\u6ce2\u5206\u89e3\u4e2d\u7684\u7ec6\u8282\u9891\u7387\u5e26\uff0c\u6765\u63d0\u9ad8CNN\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u7684\u8c31\u201c\u5085\u91cc\u53f6\u201d\u4e22\u5f03\uff08Spectral \"Fourier\" Dropout, 2D-SFD\uff09\u6709\u6240\u4e0d\u540c\uff0c\u540e\u8005\u662f\u5728\u5085\u91cc\u53f6\u57df\u4e2d\u6d88\u9664\u7cfb\u6570\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSWD\u4ec5\u9700\u8981\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u800cSFD\u5219\u9700\u8981\u4e24\u4e2a\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5b9e\u73b0\u4e00\u7ef4\u7248\u672c\u7684\u8c31\u201c\u5085\u91cc\u53f6\u201d\u4e22\u5f03\uff081D-SFD\uff09\u6765\u6269\u5c55\u6587\u732e\uff0c\u4e3a\u5168\u9762\u6bd4\u8f83\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5728CIFAR-10/100\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c1D\u548c2D SWD\u53d8\u4f53\u76f8\u5bf9\u4e8e1D-SFD\u548c2D-SFD\u90fd\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c1D-SWD\u76f8\u6bd41D/2D-SFD\u5177\u6709\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5728Pascal VOC\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWD\u53d8\u4f53\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e861D-SFD\u548c2D-SFD\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002 Rinor Cakaj PDF N/A Spectral Wavelet Dropout: Regularization in the Wavelet Domain Regularization techniques help prevent overfitting and therefore improve the ability of convolutional neural networks (CNNs) to generalize. One reason for overfitting is the complex co-adaptations among different parts of the network, which make the CNN dependent on their joint response rather than encouraging each part to learn a useful feature representation independently. Frequency domain manipulation is a powerful strategy for modifying data that has temporal and spatial coherence by utilizing frequency decomposition. This work introduces Spectral Wavelet Dropout (SWD), a novel regularization method that includes two variants: 1D-SWD and 2D-SWD. These variants improve CNN generalization by randomly dropping detailed frequency bands in the discrete wavelet decomposition of feature maps. Our approach distinguishes itself from the pre-existing Spectral \"Fourier\" Dropout (2D-SFD), which eliminates coefficients in the Fourier domain. Notably, SWD requires only a single hyperparameter, unlike the two required by SFD. We also extend the literature by implementing a one-dimensional version of Spectral \"Fourier\" Dropout (1D-SFD), setting the stage for a comprehensive comparison. Our evaluation shows that both 1D and 2D SWD variants have competitive performance on CIFAR-10/100 benchmarks relative to both 1D-SFD and 2D-SFD. Specifically, 1D-SWD has a significantly lower computational complexity compared to 1D/2D-SFD. In the Pascal VOC Object Detection benchmark, SWD variants surpass 1D-SFD and 2D-SFD in performance and demonstrate lower computational complexity during training. \u5b9e\u73b0\u9664\u6cd5\u5f52\u4e00\u5316\u7684\u9012\u5f52\u795e\u7ecf\u7535\u8def\u7684\u65e0\u6761\u4ef6\u7a33\u5b9a\u6027 \u5728\u9012\u5f52\u795e\u7ecf\u6a21\u578b\u4e2d\uff0c\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5f00\u53d1\u80fd\u591f\u65e0\u7f1d\u8bad\u7ec3\u7684\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u795e\u7ecf\u52a8\u529b\u5b66\u6a21\u578b\u65f6\u3002\u4f20\u7edf\u7684\u76ae\u5c42\u7535\u8def\u6a21\u578b\u7531\u4e8e\u52a8\u529b\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u7684\u975e\u7ebf\u6027\u800c\u81ed\u540d\u662d\u8457\uff0c\u96be\u4ee5\u8bad\u7ec3\uff0c\u5bfc\u81f4\u4f18\u5316\u95ee\u9898\u5e26\u6709\u96be\u4ee5\u65bd\u52a0\u7684\u975e\u7ebf\u6027\u7a33\u5b9a\u6027\u7ea6\u675f\u3002\u76f8\u53cd\uff0c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u5728\u6d89\u53ca\u5e8f\u5217\u6570\u636e\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u751f\u7269\u5b66\u5408\u7406\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u52a8\u6001\u5206\u88c2\u5f52\u4e00\u5316\uff08DN\uff09\u4e0eORGaNICs\u7684\u7a33\u5b9a\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\u3002ORGaNICs\u662f\u4e00\u79cd\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u9012\u5f52\u76ae\u5c42\u7535\u8def\u6a21\u578b\uff0c\u80fd\u591f\u52a8\u6001\u5b9e\u73b0DN\uff0c\u5e76\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u6a21\u62df\u5e7f\u6cdb\u7684\u795e\u7ecf\u751f\u7406\u73b0\u8c61\u3002\u901a\u8fc7\u4f7f\u7528\u674e\u96c5\u666e\u8bfa\u592b\u95f4\u63a5\u65b9\u6cd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5f53\u9012\u5f52\u6743\u91cd\u77e9\u9635\u4e3a\u5355\u4f4d\u77e9\u9635\u65f6\uff0c\u4efb\u610f\u7ef4\u5ea6\u7684ORGaNICs\u7535\u8def\u5177\u6709\u65e0\u6761\u4ef6\u7684\u5c40\u90e8\u7a33\u5b9a\u6027\u8fd9\u4e00\u663e\u8457\u7279\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06ORGaNICs\u4e0e\u8026\u5408\u963b\u5c3c\u8c10\u632f\u5b50\u7cfb\u7edf\u8054\u7cfb\u8d77\u6765\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u63a8\u5bfc\u51fa\u7535\u8def\u7684\u80fd\u91cf\u51fd\u6570\uff0c\u63d0\u4f9b\u7535\u8def\u53ca\u5355\u4e2a\u795e\u7ecf\u5143\u6240\u8ffd\u6c42\u7684\u89c4\u8303\u6027\u539f\u5219\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u4e00\u822c\u7684\u9012\u5f52\u6743\u91cd\u77e9\u9635\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e8c\u7ef4\u6a21\u578b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u8bc1\u660e\u5728\u9ad8\u7ef4\u5ea6\u4e0b\u7a33\u5b9a\u6027\u4ecd\u7136\u6210\u7acb\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u7531\u4e8eORGaNICs\u7684\u5185\u5728\u7a33\u5b9a\u6027\u7279\u6027\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5e38\u6570\uff0c\u53ef\u4ee5\u89e3\u51b3\u68af\u5ea6\u7206\u70b8\u3001\u6d88\u5931\u548c\u632f\u8361\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u68af\u5ea6\u88c1\u526a/\u7f29\u653e\u3002\u901a\u8fc7\u5728RNN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u53d1\u73b0ORGaNICs\u5728\u9759\u6001\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u795e\u7ecf\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5728\u5e8f\u5217\u4efb\u52a1\u4e2d\u4e0eLSTM\u8868\u73b0\u76f8\u5f53\u3002 Shivang Rawat PDF N/A Unconditional stability of a recurrent neural circuit implementing divisive normalization Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ORGaNICs, a biologically plausible recurrent cortical circuit model that dynamically achieves DN and has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks. \u901a\u8fc7\u58f0\u97f3\u5efa\u7acb\u4fe1\u4efb\uff1a\u8bed\u8c03\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u8bed\u97f3\u52a9\u624b\u5438\u5f15\u529b\u7684\u611f\u77e5 \u8bed\u97f3\u52a9\u624b\uff08VAs\uff09\u5728\u5904\u7406\u7b80\u5355\u4efb\u52a1\u65f6\u5e7f\u53d7\u6b22\u8fce\uff0c\u4f46\u7528\u6237\u5f80\u5f80\u5bf9\u4f7f\u7528\u5b83\u4eec\u8fdb\u884c\u590d\u6742\u7684\u5728\u7ebf\u8d2d\u7269\u7b49\u6d3b\u52a8\u6301\u8c28\u614e\u6001\u5ea6\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u8bed\u97f3\u52a9\u624b\u7684\u8bed\u97f3\u7279\u5f81\uff0c\u5982\u8bed\u97f3\u52a9\u624b\u7684\u8bed\u97f3\u8bed\u8c03\uff0c\u662f\u5426\u80fd\u8ba9\u7528\u6237\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u611f\u89c9\u5b83\u4eec\u66f4\u5177\u5438\u5f15\u529b\u548c\u53ef\u4fe1\u5ea6\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u97f3\u52a9\u624b\u7684\u8bed\u97f3\u8bed\u8c03\u5bf9\u5176\u88ab\u611f\u77e5\u7684\u5438\u5f15\u529b\u548c\u53ef\u4fe1\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002\u5b9e\u9a8c\u4e2d\u7684\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u88ab\u5177\u6709\u79ef\u6781\u6216\u4e2d\u6027\u8bed\u8c03\u7684\u8bed\u97f3\u52a9\u624b\u6240\u5438\u5f15\uff0c\u5e76\u6700\u7ec8\u4fe1\u4efb\u90a3\u4e9b\u4ed6\u4eec\u8ba4\u4e3a\u66f4\u5177\u5438\u5f15\u529b\u7684\u8bed\u97f3\u52a9\u624b\u3002\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bed\u97f3\uff0c\u7ed3\u5408\u591a\u79cd\u8bed\u97f3\u8bed\u8c03\uff0c\u53ef\u4ee5\u589e\u5f3a\u8bed\u97f3\u52a9\u624b\u88ab\u611f\u77e5\u7684\u53ef\u4fe1\u5ea6\u3002 Sabid Bin Habib Pias PDF N/A Building Trust Through Voice: How Vocal Tone Impacts User Perception of Attractiveness of Voice Assistants Voice Assistants (VAs) are popular for simple tasks, but users are often hesitant to use them for complex activities like online shopping. We explored whether the vocal characteristics like the VA's vocal tone, can make VAs perceived as more attractive and trustworthy to users for complex tasks. Our findings show that the tone of the VA voice significantly impacts its perceived attractiveness and trustworthiness. Participants in our experiment were more likely to be attracted to VAs with positive or neutral tones and ultimately trusted the VAs they found more attractive. We conclude that VA's perceived trustworthiness can be enhanced through thoughtful voice design, incorporating a variety of vocal tones. \u4ece\u79d2\u5230\u5c0f\u65f6\uff1a\u5728\u7efc\u5408\u957f\u89c6\u9891\u7406\u89e3\u4e0a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7ed3\u5408\u5728\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u6700\u8fd1\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u8868\u73b0\uff0c\u5229\u7528\u5176\u56fa\u6709\u7684\u80fd\u529b\u6765\u7406\u89e3\u548c\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6587\u672c\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u3002\u9274\u4e8e\u89c6\u89c9\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MM-LLMs\uff09\u5728\u7406\u89e3\u548c\u5904\u7406\u56fe\u50cf\u3001\u77ed\u89c6\u9891\u548c\u957f\u89c6\u9891\u65f6\u8868\u73b0\u51fa\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7684\u5dee\u5f02\u3002\u6211\u4eec\u7684\u8bba\u6587\u4e13\u6ce8\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u4e0e\u9759\u6001\u56fe\u50cf\u548c\u77ed\u89c6\u9891\u7406\u89e3\u76f8\u6bd4\u6240\u9762\u4e34\u7684\u663e\u8457\u5dee\u5f02\u548c\u72ec\u7279\u6311\u6218\u3002\u4e0e\u9759\u6001\u56fe\u50cf\u4e0d\u540c\uff0c\u77ed\u89c6\u9891\u5305\u542b\u5177\u6709\u7a7a\u95f4\u548c\u4e8b\u4ef6\u5185\u65f6\u95f4\u4fe1\u606f\u7684\u8fde\u7eed\u5e27\uff0c\u800c\u957f\u89c6\u9891\u5219\u7531\u5177\u6709\u4e8b\u4ef6\u95f4\u548c\u957f\u671f\u65f6\u95f4\u4fe1\u606f\u7684\u591a\u4e8b\u4ef6\u7ec4\u6210\u3002\u5728\u8fd9\u7bc7\u7efc\u8ff0\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u8ffd\u6eaf\u548c\u603b\u7ed3MM-LLMs\u4ece\u56fe\u50cf\u7406\u89e3\u5230\u957f\u89c6\u9891\u7406\u89e3\u7684\u8fdb\u5c55\u3002\u6211\u4eec\u56de\u987e\u4e86\u5404\u79cd\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5f3a\u8c03\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u66f4\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u7ec6\u8282\u3001\u52a8\u6001\u4e8b\u4ef6\u548c\u957f\u671f\u4f9d\u8d56\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u8be6\u7ec6\u603b\u7ed3\u4e86MM-LLMs\u5728\u7406\u89e3\u548c\u5904\u7406\u957f\u89c6\u9891\u65b9\u9762\u7684\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u8fdb\u5c55\u3002\u6700\u540e\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u73b0\u6709MM-LLMs\u5728\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86MM-LLMs\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6f5c\u5728\u672a\u6765\u65b9\u5411\u3002 Heqing Zou PDF N/A From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding The integration of Large Language Models (LLMs) with visual encoders has recently shown promising performance in visual understanding tasks, leveraging their inherent capability to comprehend and generate human-like text for visual reasoning. Given the diverse nature of visual data, MultiModal Large Language Models (MM-LLMs) exhibit variations in model designing and training for understanding images, short videos, and long videos. Our paper focuses on the substantial differences and unique challenges posed by long video understanding compared to static image and short video understanding. Unlike static images, short videos encompass sequential frames with both spatial and within-event temporal information, while long videos consist of multiple events with between-event and long-term temporal information. In this survey, we aim to trace and summarize the advancements of MM-LLMs from image understanding to long video understanding. We review the differences among various visual understanding tasks and highlight the challenges in long video understanding, including more fine-grained spatiotemporal details, dynamic events, and long-term dependencies. We then provide a detailed summary of the advancements in MM-LLMs in terms of model design and training methodologies for understanding long videos. Finally, we compare the performance of existing MM-LLMs on video understanding benchmarks of various lengths and discuss potential future directions for MM-LLMs in long video understanding."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}