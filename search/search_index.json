{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u6301\u7eed\u6539\u8fdb\u79fb\u52a8\u64cd\u4f5c\u4e0e\u81ea\u4e3b\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60 \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u81ea\u4e3b\u7684\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u64cd\u4f5c\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5927\u91cf\u4eea\u5668\u6216\u4eba\u5de5\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u7b56\u7565\u3002\u8fd9\u4e00\u6846\u67b6\u7684\u5b9e\u73b0\u5f97\u76ca\u4e8e\u4ee5\u4e0b\u4e09\u70b9\uff1a1) \u4efb\u52a1\u76f8\u5173\u81ea\u4e3b\u6027\uff0c\u5b83\u5f15\u5bfc\u63a2\u7d22\u671d\u5411\u7269\u4f53\u4ea4\u4e92\uff0c\u5e76\u9632\u6b62\u5728\u76ee\u6807\u72b6\u6001\u9644\u8fd1\u505c\u6ede\uff1b2) \u901a\u8fc7\u5229\u7528\u884c\u4e3a\u5148\u9a8c\u4e2d\u7684\u57fa\u672c\u4efb\u52a1\u77e5\u8bc6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\uff1b3) \u5236\u5b9a\u7ed3\u5408\u4eba\u7c7b\u53ef\u89e3\u91ca\u8bed\u4e49\u4fe1\u606f\u4e0e\u4f4e\u5c42\u6b21\u3001\u7ec6\u7c92\u5ea6\u89c2\u5bdf\u7684\u901a\u7528\u5956\u52b1\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u4f7fSpot\u673a\u5668\u4eba\u80fd\u591f\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u8868\u73b0\uff0c\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523080%\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e863-4\u4e2a\u767e\u5206\u70b9\u3002\u76f8\u5173\u89c6\u9891\u53ef\u5728https://continual-mobile-manip.github.io/\u67e5\u770b\u3002 Russell Mendonca PDF N/A Continuously Improving Mobile Manipulation with Autonomous Real-World RL We present a fully autonomous real-world RL framework for mobile manipulation that can learn policies without extensive instrumentation or human supervision. This is enabled by 1) task-relevant autonomy, which guides exploration towards object interactions and prevents stagnation near goal states, 2) efficient policy learning by leveraging basic task knowledge in behavior priors, and 3) formulating generic rewards that combine human-interpretable semantic information with low-level, fine-grained observations. We demonstrate that our approach allows Spot robots to continually improve their performance on a set of four challenging mobile manipulation tasks, obtaining an average success rate of 80% across tasks, a 3-4 improvement over existing approaches. Videos can be found at https://continual-mobile-manip.github.io/ MM1.5\uff1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u65b9\u6cd5\u3001\u5206\u6790\u4e0e\u6d1e\u5bdf \u6211\u4eec\u63d0\u51fa\u4e86MM1.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bb6\u65cf\uff0c\u65e8\u5728\u589e\u5f3a\u5728\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u50cf\u7406\u89e3\u3001\u89c6\u89c9\u6307\u79f0\u548c\u5b9a\u4f4d\u4ee5\u53ca\u591a\u56fe\u50cf\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u57fa\u4e8eMM1\u67b6\u6784\uff0cMM1.5\u91c7\u7528\u4e86\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u5728\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u751f\u547d\u5468\u671f\u4e2d\u591a\u6837\u6570\u636e\u6df7\u5408\u7684\u5f71\u54cd\u3002\u8fd9\u5305\u62ec\u9ad8\u8d28\u91cf\u7684OCR\u6570\u636e\u548c\u7528\u4e8e\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u5408\u6210\u5b57\u5e55\uff0c\u4ee5\u53ca\u4e3a\u76d1\u7763\u5fae\u8c03\u4f18\u5316\u7684\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u6df7\u5408\u3002\u6211\u4eec\u7684\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4ece1B\u523030B\u4e0d\u7b49\uff0c\u6db5\u76d6\u4e86\u5bc6\u96c6\u6a21\u578b\u548c\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u53d8\u4f53\uff0c\u5e76\u5c55\u793a\u4e86\u5373\u4f7f\u5728\u8f83\u5c0f\u89c4\u6a21\uff081B\u548c3B\uff09\u4e0b\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u7b5b\u9009\u548c\u8bad\u7ec3\u7b56\u7565\u4e5f\u80fd\u5e26\u6765\u5f3a\u5927\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u4e13\u95e8\u7684\u53d8\u4f53\uff1aMM1.5-Video\uff0c\u4e13\u4e3a\u89c6\u9891\u7406\u89e3\u8bbe\u8ba1\uff0c\u4ee5\u53caMM1.5-UI\uff0c\u4e13\u4e3a\u79fb\u52a8UI\u7406\u89e3\u5b9a\u5236\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u5173\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u548c\u51b3\u7b56\u7684\u8be6\u7ec6\u89c1\u89e3\uff0c\u8fd9\u4e9b\u89c1\u89e3\u6784\u6210\u4e86\u6211\u4eec\u6700\u7ec8\u8bbe\u8ba1\u7684\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u5728MLLM\u5f00\u53d1\u4e2d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6307\u5bfc\u3002 Haotian Zhang PDF N/A MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. Our models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, we introduce two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, we provide detailed insights into the training processes and decisions that inform our final designs, offering valuable guidance for future research in MLLM development. SpaceMesh\uff1a\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u6d41\u5f62\u66f2\u9762\u7f51\u683c\u7684\u8fde\u7eed\u8868\u793a \u7f51\u683c\u5728\u89c6\u89c9\u8ba1\u7b97\u548c\u6a21\u62df\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u7136\u800c\u5927\u591a\u6570\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u4ec5\u95f4\u63a5\u8868\u793a\u7f51\u683c\uff0c\u4f8b\u5982\u4f5c\u4e3a\u6807\u91cf\u573a\u7684\u6c34\u5e73\u96c6\u6216\u6a21\u677f\u7684\u53d8\u5f62\uff0c\u6216\u4f5c\u4e3a\u7f3a\u4e4f\u5c40\u90e8\u7ed3\u6784\u7684\u65e0\u5e8f\u4e09\u89d2\u5f62\u6c64\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6848\uff0c\u76f4\u63a5\u751f\u6210\u5177\u6709\u590d\u6742\u8fde\u901a\u6027\u7684\u6d41\u5f62\u591a\u8fb9\u5f62\u7f51\u683c\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5728\u6bcf\u4e2a\u7f51\u683c\u9876\u70b9\u5b9a\u4e49\u4e00\u4e2a\u8fde\u7eed\u7684\u6f5c\u5728\u8fde\u901a\u6027\u7a7a\u95f4\uff0c\u8fd9\u9690\u542b\u4e86\u79bb\u6563\u7f51\u683c\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u9876\u70b9\u5d4c\u5165\u5728\u534a\u8fb9\u7f51\u683c\u8868\u793a\u4e2d\u751f\u6210\u5faa\u73af\u90bb\u5c45\u5173\u7cfb\uff0c\u8fd9\u4fdd\u8bc1\u4e86\u8fb9\u7f18\u6d41\u5f62\u6027\u5e76\u80fd\u591f\u8868\u793a\u4e00\u822c\u7684\u591a\u8fb9\u5f62\u7f51\u683c\u3002\u8fd9\u79cd\u8868\u793a\u975e\u5e38\u9002\u5408\u673a\u5668\u5b66\u4e60\u548c\u968f\u673a\u4f18\u5316\uff0c\u4e0d\u53d7\u8fde\u901a\u6027\u6216\u62d3\u6251\u7684\u9650\u5236\u3002\u6211\u4eec\u9996\u5148\u63a2\u7d22\u4e86\u8fd9\u79cd\u8868\u793a\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u7136\u540e\u4f7f\u7528\u5b83\u6765\u62df\u5408\u6765\u81ea\u5927\u578b\u6570\u636e\u96c6\u7684\u7f51\u683c\u5206\u5e03\u3002\u751f\u6210\u7684\u6a21\u578b\u751f\u6210\u7684\u7f51\u683c\u5177\u6709\u4ece\u6570\u636e\u96c6\u7fa4\u4f53\u4e2d\u5b66\u4e60\u5230\u7684\u9576\u5d4c\u7ed3\u6784\uff0c\u7ec6\u8282\u7b80\u6d01\u4e14\u7f51\u683c\u5143\u7d20\u8d28\u91cf\u9ad8\u3002\u5728\u5e94\u7528\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u4ece\u751f\u6210\u6a21\u578b\u4e2d\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\uff0c\u8fd8\u76f4\u63a5\u5b9e\u73b0\u4e86\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\u7684\u51e0\u4f55\u5904\u7406\u4efb\u52a1\uff0c\u5982\u7f51\u683c\u4fee\u590d\u3002 Tianchang Shen PDF N/A SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes Meshes are ubiquitous in visual computing and simulation, yet most existing machine learning techniques represent meshes only indirectly, e.g. as the level set of a scalar field or deformation of a template, or as a disordered triangle soup lacking local structure. This work presents a scheme to directly generate manifold, polygonal meshes of complex connectivity as the output of a neural network. Our key innovation is to define a continuous latent connectivity space at each mesh vertex, which implies the discrete mesh. In particular, our vertex embeddings generate cyclic neighbor relationships in a halfedge mesh representation, which gives a guarantee of edge-manifoldness and the ability to represent general polygonal meshes. This representation is well-suited to machine learning and stochastic optimization, without restriction on connectivity or topology. We first explore the basic properties of this representation, then use it to fit distributions of meshes from large datasets. The resulting models generate diverse meshes with tessellation structure learned from the dataset population, with concise details and high-quality mesh elements. In applications, this approach not only yields high-quality outputs from generative models, but also enables directly learning challenging geometry processing tasks such as mesh repair. LaMMA-P\uff1a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684PDDL\u89c4\u5212\u5668\u7684\u53ef\u6cdb\u5316\u591a\u667a\u80fd\u4f53\u957f\u671f\u4efb\u52a1\u5206\u914d\u4e0e\u89c4\u5212 \u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5177\u5907\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5730\u5c06\u4eba\u7c7b\u6307\u4ee4\u8f6c\u5316\u4e3a\u7b80\u5355\u673a\u5668\u4eba\u4efb\u52a1\u7684\u8be6\u7ec6\u8ba1\u5212\u3002\u7136\u800c\uff0c\u5904\u7406\u957f\u65f6\u95f4\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u534f\u4f5c\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b50\u4efb\u52a1\u8bc6\u522b\u548c\u5206\u914d\u65b9\u9762\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53PDDL\u89c4\u5212\u5668\uff08LaMMA-P\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002LaMMA-P\u7ed3\u5408\u4e86LMs\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f20\u7edf\u542f\u53d1\u5f0f\u641c\u7d22\u89c4\u5212\u5668\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u548c\u9ad8\u6548\u6027\uff0c\u540c\u65f6\u5728\u4efb\u52a1\u95f4\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u521b\u5efa\u4e86MAT-THOR\uff0c\u8fd9\u662f\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u57fa\u4e8eAI2-THOR\u73af\u5883\uff0c\u5305\u542b\u4e86\u5177\u6709\u4e24\u79cd\u4e0d\u540c\u590d\u6742\u7a0b\u5ea6\u7684\u5bb6\u5c45\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLaMMA-P\u7684\u6210\u529f\u7387\u6bd4\u73b0\u6709\u7684\u57fa\u4e8eLM\u7684\u591a\u667a\u80fd\u4f53\u89c4\u5212\u5668\u9ad8\u51fa105%\uff0c\u6548\u7387\u9ad8\u51fa36%\u3002\u672c\u5de5\u4f5c\u7684\u5b9e\u9a8c\u89c6\u9891\u3001\u4ee3\u7801\u3001\u6570\u636e\u96c6\u4ee5\u53ca\u6bcf\u4e2a\u6a21\u5757\u4e2d\u4f7f\u7528\u7684\u8be6\u7ec6\u63d0\u793a\u5747\u53ef\u901a\u8fc7\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\uff1ahttps://lamma-p.github.io\u3002 Xiaopan Zhang PDF N/A LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner Language models (LMs) possess a strong capability to comprehend natural language, making them effective in translating human instructions into detailed plans for simple robot tasks. Nevertheless, it remains a significant challenge to handle long-horizon tasks, especially in subtask identification and allocation for cooperative heterogeneous robot teams. To address this issue, we propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel multi-agent task planning framework that achieves state-of-the-art performance on long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning capability and the traditional heuristic search planner to achieve a high success rate and efficiency while demonstrating strong generalization across tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that features household tasks with two different levels of complexity based on the AI2-THOR environment. The experimental results demonstrate that LaMMA-P achieves a 105% higher success rate and 36% higher efficiency than existing LM-based multi-agent planners. The experimental videos, code, and datasets of this work as well as the detailed prompts used in each module are available at https://lamma-p.github.io. \u76d1\u7763\u591a\u6a21\u6001\u88c2\u53d8\u5b66\u4e60 \u4ece\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u53ef\u4ee5\u5229\u7528\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u63d0\u9ad8\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5728\u9ad8\u7ef4\u6570\u636e\u96c6\u4e2d\u8003\u8651\u7279\u5f81\u76f8\u5173\u6027\u7684\u5e38\u7528\u7b56\u7565\u662f\u6f5c\u5728\u53d8\u91cf\u65b9\u6cd5\u3002\u5df2\u7ecf\u63d0\u51fa\u4e86\u51e0\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u6f5c\u5728\u53d8\u91cf\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u4e13\u6ce8\u4e8e\u63d0\u53d6\u6240\u6709\u6a21\u6001\u4e4b\u95f4\u7684\u5171\u4eab\u6210\u5206\uff0c\u8981\u4e48\u540c\u65f6\u63d0\u53d6\u5171\u4eab\u6210\u5206\u548c\u6bcf\u4e2a\u6a21\u6001\u7279\u6709\u7684\u4e2a\u4f53\u6210\u5206\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u88c2\u53d8\u5b66\u4e60\uff08Multi-Modal Fission Learning, MMFL\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u540c\u65f6\u8bc6\u522b\u591a\u6a21\u6001\u6570\u636e\u96c6\u7279\u5f81\u4e2d\u6f5c\u5728\u7684\u5168\u5c40\u8054\u5408\u3001\u90e8\u5206\u8054\u5408\u548c\u4e2a\u4f53\u6210\u5206\u3002\u4e0e\u73b0\u6709\u7684\u6f5c\u5728\u53d8\u91cf\u65b9\u6cd5\u4e0d\u540c\uff0cMMFL\u5229\u7528\u54cd\u5e94\u53d8\u91cf\u7684\u76d1\u7763\u6765\u8bc6\u522b\u5177\u6709\u9884\u6d4b\u6027\u7684\u6f5c\u5728\u6210\u5206\uff0c\u5e76\u4e14\u81ea\u7136\u5730\u6269\u5c55\u4ee5\u6574\u5408\u4e0d\u5b8c\u6574\u7684\u591a\u6a21\u6001\u6570\u636e\u3002\u901a\u8fc7\u6a21\u62df\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u660e\u4e86MMFL\u5728\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u5404\u79cd\u73b0\u6709\u7684\u591a\u6a21\u6001\u7b97\u6cd5\u3002\u6211\u4eec\u5c06MMFL\u5e94\u7528\u4e8e\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u4f7f\u7528\u6765\u81ea\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u795e\u7ecf\u5f71\u50cf\u5b66\u5021\u8bae\uff08Alzheimer's Disease Neuroimaging Initiative, ADNI\uff09\u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u9884\u6d4b\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cMMFL\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u5e76\u66f4\u597d\u5730\u6d1e\u5bdf\u4e86\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u76f8\u5173\u6027\u3002 Lingchao Mao PDF N/A Supervised Multi-Modal Fission Learning Learning from multimodal datasets can leverage complementary information and improve performance in prediction tasks. A commonly used strategy to account for feature correlations in high-dimensional datasets is the latent variable approach. Several latent variable methods have been proposed for multimodal datasets. However, these methods either focus on extracting the shared component across all modalities or on extracting both a shared component and individual components specific to each modality. To address this gap, we propose a Multi-Modal Fission Learning (MMFL) model that simultaneously identifies globally joint, partially joint, and individual components underlying the features of multimodal datasets. Unlike existing latent variable methods, MMFL uses supervision from the response variable to identify predictive latent components and has a natural extension for incorporating incomplete multimodal data. Through simulation studies, we demonstrate that MMFL outperforms various existing multimodal algorithms in both complete and incomplete modality settings. We applied MMFL to a real-world case study for early prediction of Alzheimers Disease using multimodal neuroimaging and genomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset. MMFL provided more accurate predictions and better insights into within- and across-modality correlations compared to existing methods. Maia-2\uff1a\u4e00\u4e2a\u5728\u68cb\u7c7b\u6e38\u620f\u4e2d\u5b9e\u73b0\u4eba\u673a\u5bf9\u9f50\u7684\u7edf\u4e00\u6a21\u578b \u8d8a\u6765\u8d8a\u591a\u7684\u9886\u57df\u4e2d\uff0c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u4eba\u7c7b\u7684\u80fd\u529b\uff0c\u8fd8\u80fd\u51c6\u786e\u5730\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u3002\u8fd9\u4e3a\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\u901a\u8fc7\u66f4\u6613\u7406\u89e3\u7684AI\u4f19\u4f34\u548c\u66f4\u6df1\u5165\u7684\u4eba\u7c7b\u51b3\u7b56\u6d1e\u5bdf\u529b\u5b9e\u73b0\u7b97\u6cd5\u9a71\u52a8\u7684\u6559\u5b66\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002\u7136\u800c\uff0c\u8981\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5173\u952e\u5728\u4e8e\u8fde\u8d2f\u5730\u6a21\u62df\u4eba\u7c7b\u5728\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u4e0a\u7684\u884c\u4e3a\u3002\u56fd\u9645\u8c61\u68cb\u662f\u4e00\u4e2a\u7406\u60f3\u7684\u7814\u7a76\u7cfb\u7edf\uff0c\u7528\u4e8e\u63a2\u7d22\u8fd9\u79cd\u4eba\u673a\u534f\u540c\uff0c\u56e0\u4e3a\u5b83\u6709\u7740\u4e30\u5bcc\u7684\u5386\u53f2\uff0c\u662fAI\u7814\u7a76\u7684\u5173\u952e\u8bd5\u9a8c\u573a\uff0c\u62e5\u6709\u50cfAlphaZero\u8fd9\u6837\u6210\u719f\u7684\u8d85\u4eba\u7c7bAI\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u56fd\u9645\u8c61\u68cb\u8bc4\u7ea7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6280\u80fd\u7684\u7cbe\u786e\u6d4b\u91cf\u3002\u4ee5\u5f80\u5728\u6a21\u62df\u56fd\u9645\u8c61\u68cb\u4e2d\u4eba\u7c7b\u51b3\u7b56\u7684\u5de5\u4f5c\u4f7f\u7528\u5b8c\u5168\u72ec\u7acb\u7684\u6a21\u578b\u6765\u6355\u6349\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u4e0a\u7684\u4eba\u7c7b\u98ce\u683c\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u7f3a\u4e4f\u9002\u5e94\u4eba\u7c7b\u5168\u9762\u8fdb\u6b65\u7684\u80fd\u529b\u8fde\u8d2f\u6027\uff0c\u5e76\u6700\u7ec8\u5728\u4f5c\u4e3aAI\u4f19\u4f34\u548c\u6559\u5b66\u5de5\u5177\u7684\u6709\u6548\u6027\u4e0a\u53d7\u5230\u9650\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4eba\u673a\u534f\u540c\u5efa\u6a21\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u56fd\u9645\u8c61\u68cb\u4e2d\u8fde\u8d2f\u5730\u6355\u6349\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u4e0a\u7684\u4eba\u7c7b\u98ce\u683c\uff0c\u5e76\u76f4\u63a5\u6355\u6349\u4eba\u4eec\u5982\u4f55\u6539\u8fdb\u3002\u8ba4\u8bc6\u5230\u4eba\u7c7b\u5b66\u4e60\u7684\u590d\u6742\u975e\u7ebf\u6027\u7279\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6280\u80fd\u611f\u77e5\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u52a8\u6001\u5730\u5c06\u73a9\u5bb6\u7684\u4f18\u52bf\u4e0e\u7f16\u7801\u7684\u68cb\u5c40\u4f4d\u7f6e\u6574\u5408\uff0c\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u654f\u611f\u5730\u53cd\u6620\u73a9\u5bb6\u6280\u80fd\u7684\u6f14\u53d8\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86AI\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u5728\u5404\u79cd\u4e13\u4e1a\u6c34\u5e73\u4e0a\u7684\u534f\u540c\u6027\uff0c\u4e3a\u6df1\u5165\u6d1e\u5bdf\u4eba\u7c7b\u51b3\u7b56\u548cAI\u5f15\u5bfc\u7684\u6559\u5b66\u5de5\u5177\u94fa\u5e73\u4e86\u9053\u8def\u3002 Zhenwei Tang PDF N/A Maia-2: A Unified Model for Human-AI Alignment in Chess There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making. Critical to achieving this goal, however, is coherently modeling human behavior at various skill levels. Chess is an ideal model system for conducting research into this kind of human-AI alignment, with its rich history as a pivotal testbed for AI research, mature superhuman AI systems like AlphaZero, and precise measurements of skill via chess rating systems. Previous work in modeling human decision-making in chess uses completely independent models to capture human style at different skill levels, meaning they lack coherence in their ability to adapt to the full spectrum of human improvement and are ultimately limited in their effectiveness as AI partners and teaching tools. In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players' strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill. Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools. \u5b9e\u9645\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\uff1a\u73b0\u8c61\u3001\u673a\u5236\u4e0e\u7f13\u89e3 \u4ee3\u7801\u751f\u6210\u65e8\u5728\u6839\u636e\u8f93\u5165\u9700\u6c42\u81ea\u52a8\u751f\u6210\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002\u8fd1\u671f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65b9\u6cd5\u5c55\u793a\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\uff0c\u5e76\u5f7b\u5e95\u6539\u53d8\u4e86\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u3002\u5c3d\u7ba1\u6027\u80fd\u4ee4\u4eba\u9f13\u821e\uff0c\u4f46LLMs\u5728\u751f\u6210\u5185\u5bb9\u65f6\u5e38\u5e38\u51fa\u73b0\u5e7b\u89c9\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9700\u8981\u5904\u7406\u590d\u6742\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u4ee3\u7801\u751f\u6210\u573a\u666f\u4e2d\u3002\u867d\u7136\u5148\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u5206\u6790\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u4ec5\u9650\u4e8e\u72ec\u7acb\u51fd\u6570\u7684\u751f\u6210\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u5728\u66f4\u5b9e\u9645\u548c\u590d\u6742\u7684\u5f00\u53d1\u4e0a\u4e0b\u6587\u4e2d\u7684\u4ed3\u5e93\u7ea7\u751f\u6210\u573a\u666f\u4e0b\uff0cLLM\u5e7b\u89c9\u7684\u73b0\u8c61\u3001\u673a\u5236\u53ca\u5176\u7f13\u89e3\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u624b\u52a8\u68c0\u67e5\u4e86\u516d\u4e2a\u4e3b\u6d41LLMs\u7684\u4ee3\u7801\u751f\u6210\u7ed3\u679c\uff0c\u5efa\u7acb\u4e86LLM\u751f\u6210\u4ee3\u7801\u7684\u5e7b\u89c9\u5206\u7c7b\u3002\u63a5\u7740\uff0c\u6211\u4eec\u8be6\u7ec6\u9610\u8ff0\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u5206\u6790\u4e86\u5176\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u7684\u5206\u5e03\u3002\u7136\u540e\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5e7b\u89c9\u7684\u539f\u56e0\uff0c\u5e76\u786e\u5b9a\u4e86\u56db\u4e2a\u53ef\u80fd\u5bfc\u81f4\u5e7b\u89c9\u7684\u56e0\u7d20\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRAG\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u7814\u7a76\u7684LLMs\u4e2d\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6709\u6548\u6027\u3002\u5305\u542b\u4ee3\u7801\u3001\u6570\u636e\u548c\u5b9e\u9a8c\u7ed3\u679c\u7684\u590d\u5236\u5305\u53ef\u5728https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination\u83b7\u53d6\u3002 Ziyao Zhang PDF N/A LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependencies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs. The replication package including code, data, and experimental results is available at https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination \u7f57\u6bd4\u00b7\u5df4\u7279\u52d2\uff1a\u4e0e\u5bb6\u7528\u673a\u5668\u4eba\u52a9\u624b\u7684\u8fdc\u7a0b\u591a\u6a21\u6001\u4e92\u52a8 \u672c\u6587\u4ecb\u7ecd\u4e86Robi Butler\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bb6\u5ead\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u4e0e\u8fdc\u7a0b\u7528\u6237\u8fdb\u884c\u591a\u6a21\u6001\u4e92\u52a8\u3002\u57fa\u4e8e\u5148\u8fdb\u7684\u901a\u4fe1\u63a5\u53e3\uff0cRobi Butler\u5141\u8bb8\u7528\u6237\u76d1\u63a7\u673a\u5668\u4eba\u7684\u72b6\u6001\uff0c\u53d1\u9001\u6587\u672c\u6216\u8bed\u97f3\u6307\u4ee4\uff0c\u5e76\u901a\u8fc7\u624b\u52bf\u6307\u5411\u9009\u62e9\u76ee\u6807\u7269\u4f53\u3002\u6211\u4eec\u7cfb\u7edf\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684\u9ad8\u7ea7\u884c\u4e3a\u6a21\u5757\uff0c\u5b83\u80fd\u591f\u89e3\u6790\u591a\u6a21\u6001\u6307\u4ee4\u4ee5\u751f\u6210\u884c\u52a8\u8ba1\u5212\u3002\u8fd9\u4e9b\u8ba1\u5212\u7531\u4e00\u7ec4\u5f00\u653e\u8bcd\u6c47\u539f\u8bed\u7ec4\u6210\uff0c\u8fd9\u4e9b\u539f\u8bed\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u652f\u6301\uff0c\u80fd\u591f\u5904\u7406\u6587\u672c\u548c\u6307\u5411\u67e5\u8be2\u3002\u4e0a\u8ff0\u7ec4\u4ef6\u7684\u6574\u5408\u4f7f\u5f97Robi Butler\u80fd\u591f\u4ee5\u96f6\u6837\u672c\u7684\u65b9\u5f0f\u5c06\u8fdc\u7a0b\u591a\u6a21\u6001\u6307\u4ee4\u4e0e\u73b0\u5b9e\u5bb6\u5ead\u73af\u5883\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u7cfb\u5217\u6d89\u53ca\u8fdc\u7a0b\u7528\u6237\u63d0\u4f9b\u591a\u6a21\u6001\u6307\u4ee4\u7684\u65e5\u5e38\u5bb6\u5ead\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8fdb\u884c\u4e86\u4e00\u9879\u7528\u6237\u7814\u7a76\uff0c\u5206\u6790\u4e86\u591a\u6a21\u6001\u4e92\u52a8\u5728\u8fdc\u7a0b\u4eba\u673a\u4e92\u52a8\u4e2d\u5bf9\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u4e86\u6f5c\u5728\u7684\u6539\u8fdb\u63aa\u65bd\u3002 Anxing Xiao PDF N/A Robi Butler: Remote Multimodal Interactions with Household Robot Assistant In this paper, we introduce Robi Butler, a novel household robotic system that enables multimodal interactions with remote users. Building on the advanced communication interfaces, Robi Butler allows users to monitor the robot's status, send text or voice instructions, and select target objects by hand pointing. At the core of our system is a high-level behavior module, powered by Large Language Models (LLMs), that interprets multimodal instructions to generate action plans. These plans are composed of a set of open vocabulary primitives supported by Vision Language Models (VLMs) that handle both text and pointing queries. The integration of the above components allows Robi Butler to ground remote multimodal instructions in the real-world home environment in a zero-shot manner. We demonstrate the effectiveness and efficiency of this system using a variety of daily household tasks that involve remote users giving multimodal instructions. Additionally, we conducted a user study to analyze how multimodal interactions affect efficiency and user experience during remote human-robot interaction and discuss the potential improvements. \u9000\u706b\u6d41\u751f\u6210\u6a21\u578b\uff1a\u9762\u5411\u9ad8\u7ef4\u548c\u591a\u6a21\u6001\u5206\u5e03\u7684\u91c7\u6837 \u4ece\u9ad8\u7ef4\u3001\u591a\u6a21\u6001\u5206\u5e03\u4e2d\u91c7\u6837\u4ecd\u7136\u662f\u7edf\u8ba1\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u57fa\u4e8e\u7269\u7406\u7684\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u4e2d\u7684\u4e00\u4e2a\u6839\u672c\u6027\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9000\u706b\u6d41\uff08Annealing Flow, AF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4ece\u9ad8\u7ef4\u548c\u591a\u6a21\u6001\u5206\u5e03\u4e2d\u91c7\u6837\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5b66\u4e60\u4e00\u4e2a\u7531\u9000\u706b\u5f15\u5bfc\u7684\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u8fd0\u8f93\u6620\u5c04\uff0c\u5c06\u6837\u672c\u4ece\u6613\u4e8e\u91c7\u6837\u7684\u5206\u5e03\u8fc7\u6e21\u5230\u76ee\u6807\u5206\u5e03\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5bf9\u6a21\u5f0f\u7684\u6709\u6548\u63a2\u7d22\u3002\u4e0e\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cAF\u7684\u8bad\u7ec3\u4e0d\u4f9d\u8d56\u4e8e\u76ee\u6807\u5206\u5e03\u7684\u6837\u672c\u3002AF\u786e\u4fdd\u4e86\u6a21\u5f0f\u63a2\u7d22\u7684\u6709\u6548\u6027\u548c\u5e73\u8861\u6027\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u5927\u5c0f\u548c\u7ef4\u5ea6\u7684\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5e76\u907f\u514d\u4e86\u4f4e\u6548\u7684\u6df7\u5408\u65f6\u95f4\u3002\u6211\u4eec\u901a\u8fc7\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\uff0c\u5c55\u793a\u4e86AF\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5f3a\u8c03\u4e86AF\u5728\u91c7\u6837\u6700\u4e0d\u5229\u5206\u5e03\u65b9\u9762\u7684\u6f5c\u529b\u3002 Dongze Wu PDF N/A Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions Sampling from high-dimensional, multi-modal distributions remains a fundamental challenge across domains such as statistical Bayesian inference and physics-based machine learning. In this paper, we propose Annealing Flow (AF), a continuous normalizing flow-based approach designed to sample from high-dimensional and multi-modal distributions. The key idea is to learn a continuous normalizing flow-based transport map, guided by annealing, to transition samples from an easy-to-sample distribution to the target distribution, facilitating effective exploration of modes in high-dimensional spaces. Unlike many existing methods, AF training does not rely on samples from the target distribution. AF ensures effective and balanced mode exploration, achieves linear complexity in sample size and dimensions, and circumvents inefficient mixing times. We demonstrate the superior performance of AF compared to state-of-the-art methods through extensive experiments on various challenging distributions and real-world datasets, particularly in high-dimensional and multi-modal settings. We also highlight the potential of AF for sampling the least favorable distributions. \u6269\u5c55\u672c\u4f53\u611f\u53d7-\u89c6\u89c9\u5b66\u4e60\u4e0e\u5f02\u6784\u9884\u8bad\u7ec3\u53d8\u538b\u5668 \u5f53\u524d\u8bad\u7ec3\u901a\u7528\u673a\u5668\u4eba\u6a21\u578b\u7684\u4e00\u4e2a\u969c\u788d\u662f\u5f02\u8d28\u6027\u3002\u4ee5\u5f80\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u6536\u96c6\u6570\u636e\u4ee5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u4e00\u4e2a\u5177\u4f53\u5f62\u6001\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5728\u4e0d\u540c\u5f62\u6001\u548c\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u4e0a\u8fdb\u884c\u5f02\u8d28\u6027\u9884\u8bad\u7ec3\u6765\u5b66\u4e60\u7b56\u7565\u8868\u793a\u7684\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u5f02\u8d28\u6027\u9884\u8bad\u7ec3\u53d8\u538b\u5668\uff08HPT\uff09\uff0c\u5b83\u9884\u8bad\u7ec3\u4e00\u4e2a\u5927\u578b\u3001\u53ef\u5171\u4eab\u7684\u7b56\u7565\u795e\u7ecf\u7f51\u7edc\u4e3b\u5e72\uff0c\u4ee5\u5b66\u4e60\u4e0e\u4efb\u52a1\u548c\u5f62\u6001\u65e0\u5173\u7684\u5171\u4eab\u8868\u793a\u3002\u8fd9\u79cd\u901a\u7528\u67b6\u6784\u5c06\u6765\u81ea\u4e0d\u540c\u5f62\u6001\u7684\u5177\u4f53\u672c\u4f53\u611f\u53d7\u548c\u89c6\u89c9\u8f93\u5165\u5bf9\u9f50\u4e3a\u4e00\u7cfb\u5217\u77ed\u7684\u6807\u8bb0\uff0c\u7136\u540e\u5904\u7406\u8fd9\u4e9b\u6807\u8bb0\u4ee5\u6620\u5c04\u5230\u4e0d\u540c\u4efb\u52a1\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002\u5229\u7528\u6700\u8fd1\u7684\u5927\u89c4\u6a21\u591a\u5f62\u6001\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6570\u636e\u96c6\u4ee5\u53ca\u6a21\u62df\u3001\u90e8\u7f72\u7684\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u8de8\u5f02\u8d28\u6027\u7684\u7b56\u7565\u9884\u8bad\u7ec3\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u8bad\u7ec3\u76ee\u6807\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u6d89\u53ca\u591a\u8fbe52\u4e2a\u6570\u636e\u96c6\u3002HPT\u5728\u591a\u4e2a\u6a21\u62df\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\uff0c\u5e76\u5c06\u5fae\u8c03\u7b56\u7565\u7684\u6027\u80fd\u63d0\u5347\u4e86\u8d85\u8fc720%\u3002\u6709\u5173\u4ee3\u7801\u548c\u89c6\u9891\uff0c\u8bf7\u53c2\u89c1\u9879\u76ee\u7f51\u7ad9\uff08https://liruiw.github.io/hpt/\uff09\u3002 Lirui Wang PDF N/A Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (https://liruiw.github.io/hpt/) for code and videos."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}