{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u8fde\u63a5\u7247\u6bb5\u4e0e\u8bed\u4e49\uff1a\u4e00\u79cd\u7406\u89e3\u957f\u7bc7\u89c6\u9891\u7684\u65b0\u6846\u67b6 \u5c3d\u7ba1\u73b0\u6709\u7814\u7a76\u5e38\u5c06\u957f\u89c6\u9891\u89c6\u4e3a\u6269\u5c55\u7684\u77ed\u89c6\u9891\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u8ba4\u77e5\u7684\u65b0\u65b9\u6cd5\u3002\u672c\u6587\u4ecb\u7ecd\u4e86BREASE\uff1a\u4e00\u79cd\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u60c5\u666f\u8bb0\u5fc6\u7684\u79ef\u7d2f\u6765\u6355\u6349\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u7ed3\u5408\u89c6\u9891\u4e2d\u5206\u6563\u7684\u8bed\u4e49\u77e5\u8bc6\u5bf9\u5176\u8fdb\u884c\u5f3a\u5316\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u505a\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u8d21\u732e\uff1a\u9996\u5148\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u60c5\u666f\u538b\u7f29\u5668\uff08ECO\uff09\uff0c\u5b83\u80fd\u9ad8\u6548\u5730\u4ece\u5fae\u89c2\u5230\u534a\u5b8f\u89c2\u5c42\u9762\u805a\u5408\u5173\u952e\u8868\u5f81\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u4e49\u68c0\u7d22\u5668\uff08SeTR\uff09\uff0c\u901a\u8fc7\u805a\u7126\u4e8e\u66f4\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u8fd9\u4e9b\u805a\u5408\u8868\u5f81\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u964d\u4f4e\u7279\u5f81\u7ef4\u5ea6\u540c\u65f6\u4fdd\u7559\u76f8\u5173\u7684\u5b8f\u89c2\u7ea7\u522b\u4fe1\u606f\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBREASE\u5728\u96f6\u6837\u672c\u548c\u5b8c\u5168\u76d1\u7763\u8bbe\u7f6e\u4e0b\u7684\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\u548c\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://joslefaure.github.io/assets/html/hermes.html\u3002 Gueter Josmy Faure PDF N/A Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html. SelectTTS\uff1a\u57fa\u4e8e\u79bb\u6563\u5355\u5143\u5e27\u9009\u62e9\u7684\u8bed\u97f3\u5408\u6210\u6280\u672f\uff0c\u53ef\u6a21\u62df\u4efb\u4f55\u4eba\u7684\u58f0\u97f3 \u5408\u6210\u672a\u89c1\u8bf4\u8bdd\u8005\u7684\u58f0\u97f3\u662f\u591a\u8bf4\u8bdd\u8005\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u4e2d\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\u3002\u5927\u591a\u6570\u591a\u8bf4\u8bdd\u8005TTS\u6a21\u578b\u4f9d\u8d56\u4e8e\u5728\u8bad\u7ec3\u671f\u95f4\u901a\u8fc7\u8bf4\u8bdd\u8005\u8c03\u8282\u6765\u5efa\u6a21\u8bf4\u8bdd\u8005\u7279\u5f81\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u672a\u89c1\u8bf4\u8bdd\u8005\u5c5e\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u9700\u8981\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u8fd9\u4f7f\u5f97\u518d\u73b0\u7ed3\u679c\u548c\u6539\u8fdb\u7ed3\u679c\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u4eec\u63d0\u51fa\u4e86SelectTTS\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4ece\u76ee\u6807\u8bf4\u8bdd\u8005\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5e27\uff0c\u5e76\u4f7f\u7528\u5e27\u7ea7\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7279\u5f81\u8fdb\u884c\u89e3\u7801\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u672a\u89c1\u8bf4\u8bdd\u8005\u7684\u8bf4\u8bdd\u8005\u7279\u5f81\uff0c\u5e76\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u6307\u6807\u4e0a\u4e0e\u5176\u4ed6\u591a\u8bf4\u8bdd\u8005TTS\u6846\u67b6\u53d6\u5f97\u53ef\u6bd4\u7684\u7ed3\u679c\u3002\u901a\u8fc7SelectTTS\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4ece\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u4e2d\u9009\u62e9\u5e27\u662f\u5b9e\u73b0\u672a\u89c1\u8bf4\u8bdd\u8005\u6cdb\u5316\u7684\u76f4\u63a5\u65b9\u5f0f\uff0c\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u8f83\u4f4e\u3002\u6211\u4eec\u5728\u8bf4\u8bdd\u8005\u76f8\u4f3c\u6027\u6027\u80fd\u4e0a\u4f18\u4e8eSOTA\u57fa\u7ebfXTTS-v2\u548cVALL-E\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u4e868\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u4e86270\u500d\u3002 Ismail Rasim Ulgen PDF N/A SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u591a\u8bf4\u8bdd\u4eba\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6027\u80fd \u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u8bc6\u522b\u6765\u81ea\u591a\u4e2a\u8bf4\u8bdd\u8005\u7684\u91cd\u53e0\u8bed\u97f3\u662f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u4e4b\u4e00\u3002\u5e8f\u5217\u5316\u8f93\u51fa\u8bad\u7ec3\uff08SOT\uff09\u662f\u4e00\u79cd\u7ecf\u5178\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u8bf4\u8bdd\u8005ASR\u95ee\u9898\uff0c\u5176\u601d\u8def\u662f\u6839\u636e\u591a\u4e2a\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u53d1\u5c04\u65f6\u95f4\u5c06\u4ed6\u4eec\u7684\u8f6c\u5f55\u6587\u672c\u4e32\u8054\u8d77\u6765\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u6e90\u81ea\u5bf9\u8bdd\u4e2d\u591a\u4e2a\u76f8\u5173\u8bdd\u8bed\u4e32\u8054\u7684SOT\u98ce\u683c\u8f6c\u5f55\u6587\u672c\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5bf9\u957f\u4e0a\u4e0b\u6587\u7684\u5efa\u6a21\u3002\u56e0\u6b64\uff0c\u4e0e\u4e3b\u8981\u5f3a\u8c03\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff08AED\uff09\u67b6\u6784\u4e2d\u7f16\u7801\u5668\u6027\u80fd\u7684\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e76\u501f\u52a9\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u53ef\u80fd\u66f4\u9002\u5408\u8fd9\u79cd\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684SOT\u65b9\u6cd5\u7528\u4e8e\u591a\u8bf4\u8bdd\u8005ASR\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u97f3\u7f16\u7801\u5668\u548cLLM\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u7b56\u7565\u5bf9\u591a\u8bf4\u8bdd\u8005\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u96c6LibriMix\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u4e8eAED\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6AMI\u7684\u8bc4\u4f30\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u4f7f\u75281000\u500d\u4ee5\u4e0a\u76d1\u7763\u6570\u636e\u8bad\u7ec3\u7684AED\u6a21\u578b\u3002 Mohan Shi PDF N/A Advancing Multi-talker ASR Performance with Large Language Models Recognizing overlapping speech from multiple speakers in conversational scenarios is one of the most challenging problem for automatic speech recognition (ASR). Serialized output training (SOT) is a classic method to address multi-talker ASR, with the idea of concatenating transcriptions from multiple speakers according to the emission times of their speech for training. However, SOT-style transcriptions, derived from concatenating multiple related utterances in a conversation, depend significantly on modeling long contexts. Therefore, compared to traditional methods that primarily emphasize encoder performance in attention-based encoder-decoder (AED) architectures, a novel approach utilizing large language models (LLMs) that leverages the capabilities of pre-trained decoders may be better suited for such complex and challenging scenarios. In this paper, we propose an LLM-based SOT approach for multi-talker ASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on multi-talker dataset using appropriate strategies. Experimental results demonstrate that our approach surpasses traditional AED-based methods on the simulated dataset LibriMix and achieves state-of-the-art performance on the evaluation set of the real-world dataset AMI, outperforming the AED model trained with 1000 times more supervised data in previous works. \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d \u89c6\u9891\u52a8\u4f5c\u5b9a\u4f4d\u65e8\u5728\u4ece\u957f\u89c6\u9891\u4e2d\u627e\u51fa\u7279\u5b9a\u52a8\u4f5c\u7684\u65f6\u95f4\u70b9\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5bf9\u89c6\u9891\u8fdb\u884c\u6807\u6ce8\uff0c\u5e26\u6765\u4e86\u53ef\u89c2\u7684\u52b3\u52a8\u529b\u6210\u672c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5b66\u4e60\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u65b0\u5174\u7684\u73b0\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3002\u6311\u6218\u5728\u4e8e\uff0cVLM\u65e2\u4e0d\u662f\u8bbe\u8ba1\u6765\u5904\u7406\u957f\u89c6\u9891\u7684\uff0c\u4e5f\u4e0d\u662f\u4e13\u4e3a\u5bfb\u627e\u52a8\u4f5c\u800c\u5b9a\u5236\u7684\u3002\u6211\u4eec\u901a\u8fc7\u6269\u5c55\u4e00\u79cd\u8fed\u4ee3\u7684\u89c6\u89c9\u63d0\u793a\u6280\u672f\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5c06\u89c6\u9891\u5e27\u91c7\u6837\u6210\u5e26\u6709\u5e27\u7d22\u5f15\u6807\u7b7e\u7684\u62fc\u63a5\u56fe\u50cf\uff0c\u8ba9VLM\u731c\u6d4b\u88ab\u8ba4\u4e3a\u662f\u52a8\u4f5c\u5f00\u59cb/\u7ed3\u675f\u6700\u63a5\u8fd1\u7684\u5e27\u3002\u901a\u8fc7\u7f29\u5c0f\u91c7\u6837\u65f6\u95f4\u7a97\u53e3\u6765\u8fed\u4ee3\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u6700\u7ec8\u627e\u5230\u52a8\u4f5c\u5f00\u59cb\u548c\u7ed3\u675f\u7684\u7279\u5b9a\u5e27\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u79cd\u91c7\u6837\u6280\u672f\u80fd\u4ea7\u751f\u5408\u7406\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86VLM\u5728\u7406\u89e3\u89c6\u9891\u65b9\u9762\u7684\u5b9e\u9645\u6269\u5c55\u3002\u793a\u4f8b\u4ee3\u7801\u53ef\u5728 https://microsoft.github.io/VLM-Video-Action-Localization/ \u83b7\u53d6\u3002 Naoki Wake PDF N/A Open-vocabulary Temporal Action Localization using VLMs Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. A sample code is available at https://microsoft.github.io/VLM-Video-Action-Localization/. \u83b7\u53d6\u529f\u80fd\u542f\u53d1\uff1a\u5e94\u7528\u5546\u5e97\u4e0e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5 \u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\uff0c\u53d7\u5e94\u7528\u5546\u5e97\uff08AppStore\uff09\u542f\u53d1\u7684\u9700\u6c42\u83b7\u53d6\u5df2\u88ab\u8bc1\u660e\u975e\u5e38\u6709\u76ca\u3002\u5f00\u53d1\u8005\u901a\u5e38\u4f1a\u7814\u7a76\u7ade\u4e89\u5bf9\u624b\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u6536\u96c6\u65b0\u529f\u80fd\u7684\u7075\u611f\u3002\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u542f\u53d1\u7684\u9700\u6c42\u83b7\u53d6\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002LLM\u53ef\u4ee5\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u65b0\u529f\u80fd\u521b\u610f\u7684\u7075\u611f\u3002\u5c3d\u7ba1\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u4eba\u4eec\u5bf9\u5176\u5dee\u5f02\u7684\u4e86\u89e3\u751a\u5c11\u3002\u6211\u4eec\u62a5\u544a\u4e86\u4e00\u9879\u6bd4\u8f83\u7814\u7a76\uff0c\u5bf9\u6bd4\u4e86\u57fa\u4e8eAppStore\u548cLLM\u7684\u65b9\u6cd5\u5728\u5c06\u529f\u80fd\u7ec6\u5316\u4e3a\u5b50\u529f\u80fd\u65b9\u9762\u7684\u6548\u679c\u3002\u901a\u8fc7\u624b\u52a8\u5206\u6790\u4ece\u8fd9\u4e24\u79cd\u65b9\u6cd5\u63a8\u8350\u76841200\u4e2a\u5b50\u529f\u80fd\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u5b83\u4eec\u7684\u4f18\u70b9\u3001\u6311\u6218\u548c\u5173\u952e\u5dee\u5f02\u3002\u867d\u7136\u8fd9\u4e24\u79cd\u65b9\u6cd5\u63a8\u8350\u7684\u5b50\u529f\u80fd\u90fd\u5177\u6709\u9ad8\u5ea6\u76f8\u5173\u6027\u548c\u6e05\u6670\u7684\u63cf\u8ff0\uff0c\u4f46LLM\u5728\u5904\u7406\u65b0\u9896\u3001\u672a\u89c1\u8fc7\u7684\u5e94\u7528\u8303\u56f4\u65b9\u9762\u4f3c\u4e4e\u66f4\u5177\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u4e00\u4e9b\u63a8\u8350\u7684\u7279\u6027\u53ef\u80fd\u662f\u865a\u6784\u7684\uff0c\u53ef\u884c\u6027\u4e0d\u660e\u786e\uff0c\u8fd9\u8868\u660e\u5728\u9700\u6c42\u83b7\u53d6\u8fc7\u7a0b\u4e2d\u4eba\u7c7b\u5206\u6790\u5e08\u7684\u91cd\u8981\u6027\u3002 Jialiang Wei PDF N/A Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach Over the past decade, app store (AppStore)-inspired requirements elicitation has proven to be highly beneficial. Developers often explore competitors' apps to gather inspiration for new features. With the advance of Generative AI, recent studies have demonstrated the potential of large language model (LLM)-inspired requirements elicitation. LLMs can assist in this process by providing inspiration for new feature ideas. While both approaches are gaining popularity in practice, there is a lack of insight into their differences. We report on a comparative study between AppStore- and LLM-based approaches for refining features into sub-features. By manually analyzing 1,200 sub-features recommended from both approaches, we identified their benefits, challenges, and key differences. While both approaches recommend highly relevant sub-features with clear descriptions, LLMs seem more powerful particularly concerning novel unseen app scopes. Moreover, some recommended features are imaginary with unclear feasibility, which suggests the importance of a human-analyst in the elicitation loop. \u63a2\u7d22\u89e3\u91ca\u5185\u5bb9\u4e0e\u683c\u5f0f\u5bf9\u7528\u6237\u7406\u89e3\u548c\u4fe1\u4efb\u7684\u5f71\u54cd \u8fd1\u5e74\u6765\uff0c\u591a\u79cd\u65b9\u6cd5\u88ab\u63d0\u51fa\u7528\u4e8e\u89e3\u91ca\u201c\u9ed1\u7bb1\u201dAI\u6a21\u578b\u7684\u8f93\u51fa\u7ed3\u679c\u3002\u7136\u800c\uff0c\u7528\u6237\u662f\u5426\u771f\u6b63\u7406\u89e3\u5e76\u4fe1\u4efb\u8fd9\u4e9b\u89e3\u91ca\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u805a\u7126\u4e8e\u4e00\u79cd\u8bc4\u4f30\u764c\u75c7\u98ce\u9669\u7684\u56de\u5f52\u5de5\u5177\u7684\u89e3\u91ca\uff0c\u5e76\u63a2\u8ba8\u4e86\u89e3\u91ca\u5185\u5bb9\u548c\u683c\u5f0f\u5bf9\u7528\u6237\u7406\u89e3\u5ea6\u548c\u4fe1\u4efb\u5ea6\u7684\u5f71\u54cd\u3002\u5728\u5185\u5bb9\u65b9\u9762\uff0c\u6211\u4eec\u5b9e\u9a8c\u4e86\u4e24\u79cd\u89e3\u91ca\u65b9\u6cd5\uff1a\u6d41\u884c\u7684SHAP\uff0c\u57fa\u4e8e\u535a\u5f08\u8bba\u6982\u5ff5\uff0c\u53ef\u80fd\u5bf9\u666e\u901a\u7528\u6237\u6765\u8bf4\u8f83\u4e3a\u590d\u6742\uff1b\u4ee5\u53ca\u57fa\u4e8e\u7279\u5f81\u906e\u6321\u7684occlusion-1\uff0c\u53ef\u80fd\u66f4\u6613\u4e8e\u7406\u89e3\u3002\u5728\u683c\u5f0f\u65b9\u9762\uff0c\u6211\u4eec\u5c06SHAP\u89e3\u91ca\u4ee5\u56fe\u8868\u5f62\u5f0f\uff08SC\uff09\u5448\u73b0\uff0c\u8fd9\u662f\u5e38\u89c4\u505a\u6cd5\uff0c\u800cocclusion-1\u89e3\u91ca\u5219\u540c\u65f6\u4ee5\u56fe\u8868\uff08OC\uff09\u548c\u6587\u672c\uff08OT\uff09\u5f62\u5f0f\u5c55\u793a\uff0c\u5176\u7b80\u5355\u6027\u4e5f\u9002\u5408\u8fd9\u79cd\u5448\u73b0\u65b9\u5f0f\u3002\u5b9e\u9a8c\u6d89\u53ca\u7528\u6237\u7814\u7a76\uff0c\u5411\u4e24\u7c7b\u4e0d\u540c\u4e13\u4e1a\u6c34\u5e73\u7684\u53c2\u4e0e\u8005\uff08\u666e\u901a\u4eba\u7fa4\u548c\u5177\u6709\u4e00\u5b9a\u533b\u5b66\u80cc\u666f\u7684\u4eba\uff09\u8be2\u95ee\u4ed6\u4eec\u5bf9\u56de\u5f52\u5de5\u5177\u8f93\u51fa\u89e3\u91ca\u7684\u4e3b\u89c2\u548c\u5ba2\u89c2\u7406\u89e3\u53ca\u4fe1\u4efb\u7a0b\u5ea6\u3002\u5728\u4e24\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u57fa\u4e8e\u5185\u5bb9\u6bd4\u8f83\u65f6\uff0c\u7528\u6237\u5728\u4e3b\u89c2\u7406\u89e3\u5ea6\u548c\u4fe1\u4efb\u5ea6\u4e0a\u666e\u904d\u504f\u597docclusion-1\u89e3\u91ca\u800c\u975eSHAP\u89e3\u91ca\u3002\u7136\u800c\uff0c\u5728\u63a7\u5236\u683c\u5f0f\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\u65f6\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4ec5\u663e\u793a\u51faOT\u89e3\u91ca\u4f18\u4e8eSC\u89e3\u91ca\uff0c\u8fd9\u8868\u660eocclusion-1\u5bf9SHAP\u89e3\u91ca\u7684\u4f18\u52bf\u53ef\u80fd\u6e90\u4e8e\u7528\u6237\u5bf9\u6587\u672c\u89e3\u91ca\u7684\u504f\u597d\u800c\u975e\u56fe\u8868\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u5ba2\u89c2\u7406\u89e3\u5ea6\u65b9\u9762\u672a\u53d1\u73b0\u89e3\u91ca\u7c7b\u578b\u95f4\u7684\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u603b\u4f53\u800c\u8a00\uff0c\u9009\u62e9\u89e3\u91ca\u7684\u5185\u5bb9\u548c\u683c\u5f0f\u9700\u8c28\u614e\u8003\u8651\uff0c\u56e0\u4e3a\u5728\u67d0\u4e9b\u60c5\u5883\u4e0b\uff0c\u683c\u5f0f\u800c\u975e\u5185\u5bb9\u53ef\u80fd\u5bf9\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u8d77\u5173\u952e\u4f5c\u7528\u3002 Antonio Rago PDF N/A Exploring the Effect of Explanation Content and Format on User Comprehension and Trust In recent years, various methods have been introduced for explaining the outputs of \"black-box\" AI models. However, it is not well understood whether users actually comprehend and trust these explanations. In this paper, we focus on explanations for a regression tool for assessing cancer risk and examine the effect of the explanations' content and format on the user-centric metrics of comprehension and trust. Regarding content, we experiment with two explanation methods: the popular SHAP, based on game-theoretic notions and thus potentially complex for everyday users to comprehend, and occlusion-1, based on feature occlusion which may be more comprehensible. Regarding format, we present SHAP explanations as charts (SC), as is conventional, and occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature also lends itself. The experiments amount to user studies questioning participants, with two different levels of expertise (the general population and those with some medical training), on their subjective and objective comprehension of and trust in explanations for the outputs of the regression tool. In both studies we found a clear preference in terms of subjective comprehension and trust for occlusion-1 over SHAP explanations in general, when comparing based on content. However, direct comparisons of explanations when controlling for format only revealed evidence for OT over SC explanations in most cases, suggesting that the dominance of occlusion-1 over SHAP explanations may be driven by a preference for text over charts as explanations. Finally, we found no evidence of a difference between the explanation types in terms of objective comprehension. Thus overall, the choice of the content and format of explanations needs careful attention, since in some contexts format, rather than content, may play the critical role in improving user experience. \u516c\u5e73\u611f\u77e5\u56fe\u6a21\u578b\u4f30\u8ba1 \u672c\u6587\u63a2\u8ba8\u4e86\u56fe\u6a21\u578b\uff08\u5c24\u5176\u662f\u9ad8\u65af\u6a21\u578b\u3001\u534f\u65b9\u5dee\u6a21\u578b\u548c\u4f0a\u8f9b\u6a21\u578b\uff09\u4f30\u8ba1\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u7406\u89e3\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u590d\u6742\u5173\u7cfb\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u6807\u51c6\u7684\u56fe\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u6709\u504f\u5dee\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u57fa\u7840\u6570\u636e\u6d89\u53ca\u654f\u611f\u7279\u5f81\u6216\u53d7\u4fdd\u62a4\u7fa4\u4f53\u65f6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u4e0e\u53d7\u4fdd\u62a4\u5c5e\u6027\u76f8\u5173\u7684\u56fe\u6a21\u578b\u4f30\u8ba1\u4e2d\u7684\u504f\u5dee\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6210\u5bf9\u56fe\u5dee\u5f02\u8bef\u5dee\u548c\u5b9a\u5236\u635f\u5931\u51fd\u6570\u6574\u5408\u5230\u4e00\u4e2a\u975e\u5149\u6ed1\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u529b\u6c42\u5728\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u95f4\u5b9e\u73b0\u516c\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u5bf9\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u504f\u5dee\uff0c\u4e14\u4e0d\u4f1a\u524a\u5f31\u56fe\u6a21\u578b\u7684\u6027\u80fd\u3002 Zhuoping Zhou PDF N/A Fairness-Aware Estimation of Graphical Models This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance. \u795e\u7ecf\u5207\u7ebf\u7cfb\u7efc\u7684\u6301\u7eed\u5b66\u4e60 \u6301\u7eed\u5b66\u4e60\u7684\u4e00\u79cd\u81ea\u7136\u7b56\u7565\u662f\u6743\u8861\u4e00\u7ec4\u56fa\u5b9a\u51fd\u6570\u7684\u8d1d\u53f6\u65af\u96c6\u6210\u3002\u8fd9\u8868\u660e\uff0c\u5982\u679c\u4e00\u4e2a\uff08\u5355\u4e00\u7684\uff09\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3a\u4e00\u4e2a\u96c6\u6210\uff0c\u90a3\u4e48\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u9057\u5fd8\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u53ef\u80fd\u6027\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5177\u6709N\u4e2a\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3aN\u4e2a\u5206\u7c7b\u5668\u7684\u52a0\u6743\u96c6\u6210\uff0c\u5e76\u4e14\u5728\u61d2\u60f0\u673a\u5236\u7684\u6781\u9650\u4e0b\uff0c\u8fd9\u4e9b\u5206\u7c7b\u5668\u5728\u6574\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u662f\u56fa\u5b9a\u7684\u3002\u6211\u4eec\u79f0\u8fd9\u4e9b\u5206\u7c7b\u5668\u4e3a\u795e\u7ecf\u5207\u7ebf\u4e13\u5bb6\uff0c\u5e76\u8bc1\u660e\u5b83\u4eec\u8f93\u51fa\u7684\u6807\u7b7e\u6982\u7387\u5206\u5e03\u662f\u6709\u6548\u7684\u3002\u7136\u540e\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u6bcf\u4e2a\u4e13\u5bb6\u5728\u7ed9\u5b9a\u8fc7\u53bb\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u4f3c\u7136\u548c\u540e\u9a8c\u6982\u7387\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u4e9b\u4e13\u5bb6\u7684\u540e\u9a8c\u66f4\u65b0\u7b49\u540c\u4e8e\u7f51\u7edc\u6743\u91cd\u7684\u7f29\u653e\u548c\u6295\u5f71\u5f62\u5f0f\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u3002\u5728\u61d2\u60f0\u673a\u5236\u4e4b\u5916\uff0c\u7f51\u7edc\u53ef\u4ee5\u88ab\u89c6\u4e3a\u968f\u65f6\u95f4\u6539\u8fdb\u7684\u9002\u5e94\u6027\u4e13\u5bb6\u7684\u96c6\u6210\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\uff0c\u5373\u4f5c\u4e3a\u4e13\u5bb6\u7684\u8d1d\u53f6\u65af\u96c6\u6210\uff0c\u4e3a\u7406\u89e3\u548c\u7f13\u89e3\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684\u6846\u67b6\u3002 Ari S. Benjamin PDF N/A Continual learning with the neural tangent ensemble A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We term these classifiers the neural tangent experts and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly, we learn that the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings. \u975e\u51f8\u4e24\u9636\u6bb5\u968f\u673a\u4f18\u5316\u95ee\u9898\u7684\u8d1d\u53f6\u65af\u4f18\u5316 \u8d1d\u53f6\u65af\u4f18\u5316\u662f\u4e00\u79cd\u9ad8\u6548\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6210\u672c\u9ad8\u6602\u3001\u9ed1\u7bb1\u4f18\u5316\u95ee\u9898\u3002\u968f\u673a\u89c4\u5212\u5173\u6ce8\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4f18\u5316\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u5173\u6ce8\u7684\u662f\u5e73\u5747\u6027\u80fd\u3002\u5728\u4e24\u9636\u6bb5\u95ee\u9898\u7684\u7b2c\u4e00\u9636\u6bb5\uff0c\u5fc5\u987b\u5728\u9762\u5bf9\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u65f6\u505a\u51fa\u201c\u6b64\u65f6\u6b64\u5730\u201d\u7684\u51b3\u7b56\uff1b\u800c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5219\u5728\u4e0d\u786e\u5b9a\u6027\u89e3\u51b3\u540e\u505a\u51fa\u201c\u89c2\u671b\u7b49\u5f85\u201d\u7684\u51b3\u7b56\u3002\u8bb8\u591a\u968f\u673a\u89c4\u5212\u65b9\u6cd5\u5047\u8bbe\u76ee\u6807\u51fd\u6570\u6613\u4e8e\u8bc4\u4f30\u4e14\u4e3a\u7ebf\u6027\u6216\u51f8\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5e94\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6765\u89e3\u51b3\u8bc4\u4f30\u6210\u672c\u9ad8\u6602\u7684\u975e\u51f8\u4e24\u9636\u6bb5\u968f\u673a\u89c4\u5212\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u68af\u5ea6\u7684\u91c7\u96c6\u51fd\u6570\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u7b2c\u4e00\u9636\u6bb5\u548c\u7b2c\u4e8c\u9636\u6bb5\u7684\u53d8\u91cf\uff0c\u5efa\u7acb\u4e86\u6e10\u8fd1\u4e00\u81f4\u6027\u7684\u4fdd\u8bc1\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4e0e\u53e6\u4e00\u79cd\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u76f8\u5f53\u7684\u5b9e\u8bc1\u7ed3\u679c\uff0c\u540e\u8005\u5728\u4e24\u79cd\u53d8\u91cf\u7c7b\u578b\u4e4b\u95f4\u4ea4\u66ff\u5176\u5173\u6ce8\u70b9\uff0c\u5e76\u4e14\u4f18\u4e8e\u6807\u51c6\u7684\u3001\u6734\u7d20\u7684\u4e24\u6b65\u57fa\u51c6\u65b9\u6cd5\u3002\u6211\u4eec\u8868\u660e\uff0c\u53d8\u91cf\u7c7b\u578b\u4e4b\u95f4\u5728\u7ef4\u5ea6\u548c\u5927\u5c3a\u5ea6\u4e0a\u7684\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u4e24\u6b65\u7b97\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u8054\u5408\u548c\u4ea4\u66ff\u91c7\u96c6\u51fd\u6570\u5728\u6240\u6709\u6d4b\u8bd5\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\u3002\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u4f8b\u5b50\u4e0a\u8fdb\u884c\u3002 Jack M. Buckingham PDF N/A Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization Problems Bayesian optimization is a sample-efficient method for solving expensive, black-box optimization problems. Stochastic programming concerns optimization under uncertainty where, typically, average performance is the quantity of interest. In the first stage of a two-stage problem, here-and-now decisions must be made in the face of this uncertainty, while in the second stage, wait-and-see decisions are made after the uncertainty has been resolved. Many methods in stochastic programming assume that the objective is cheap to evaluate and linear or convex. In this work, we apply Bayesian optimization to solve non-convex, two-stage stochastic programs which are expensive to evaluate. We formulate a knowledge-gradient-based acquisition function to jointly optimize the first- and second-stage variables, establish a guarantee of asymptotic consistency and provide a computationally efficient approximation. We demonstrate comparable empirical results to an alternative we formulate which alternates its focus between the two variable types, and superior empirical results over the standard, naive, two-step benchmark. We show that differences in the dimension and length scales between the variable types can lead to inefficiencies of the two-step algorithm, while the joint and alternating acquisition functions perform well in all problems tested. Experiments are conducted on both synthetic and real-world examples. LASSO-MOGAT\uff1a\u4e00\u79cd\u7528\u4e8e\u764c\u75c7\u5206\u7c7b\u7684\u591a\u7ec4\u5b66\u56fe\u6ce8\u610f\u529b\u6846\u67b6 \u5c06\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e\u5206\u6790\u57fa\u56e0\u8868\u8fbe\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u8fd1\u5e74\u6765\u5728\u764c\u75c7\u7814\u7a76\u4e2d\u5d2d\u9732\u5934\u89d2\uff0c\u6210\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u624b\u6bb5\uff0c\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u764c\u75c7\u53d1\u5c55\u548c\u8fdb\u5c55\u80cc\u540e\u5206\u5b50\u673a\u5236\u7684\u7406\u89e3\u3002\u7ed3\u5408\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u4e0e\u5176\u4ed6\u7c7b\u578b\u7684\u7ec4\u5b66\u6570\u636e\uff0c\u5df2\u88ab\u4f17\u591a\u7814\u7a76\u8bc1\u5b9e\u80fd\u63d0\u5347\u764c\u75c7\u5206\u7c7b\u7684\u6548\u679c\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u5c55\uff0c\u6709\u6548\u6574\u5408\u9ad8\u7ef4\u591a\u7ec4\u5b66\u6570\u636e\u5e76\u6355\u6349\u4e0d\u540c\u751f\u7269\u5c42\u7ea7\u95f4\u7684\u590d\u6742\u5173\u7cfb\u4ecd\u9887\u5177\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86LASSO-MOGAT\uff08LASSO-\u591a\u7ec4\u5b66\u95e8\u63a7\u6ce8\u610f\u529b\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4fe1\u4f7fRNA\u3001\u5faeRNA\u53caDNA\u7532\u57fa\u5316\u6570\u636e\uff0c\u7528\u4ee5\u5206\u7c7b31\u79cd\u764c\u75c7\u7c7b\u578b\u3002\u901a\u8fc7\u5229\u7528LIMMA\u8fdb\u884c\u5dee\u5f02\u8868\u8fbe\u5206\u6790\u53caLASSO\u56de\u5f52\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u501f\u52a9\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GATs\uff09\u7eb3\u5165\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\uff08PPI\uff09\u7f51\u7edc\uff0cLASSO-MOGAT\u6709\u6548\u5730\u6355\u6349\u4e86\u591a\u7ec4\u5b66\u6570\u636e\u4e2d\u7684\u590d\u6742\u5173\u7cfb\u3002\u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u53ef\u9760\u6027\u548c\u63d0\u4f9b\u6df1\u5165\u764c\u75c7\u5206\u5b50\u673a\u5236\u5168\u9762\u89c1\u89e3\u7684\u80fd\u529b\u3002\u57fa\u4e8e\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7684\u56fe\u6ce8\u610f\u529b\u67b6\u6784\u6240\u8ba1\u7b97\u7684\u56fe\u4e2d\u8fb9\u7684\u6ce8\u610f\u529b\u7cfb\u6570\uff0c\u88ab\u8bc1\u5b9e\u6709\u52a9\u4e8e\u8bc6\u522b\u591a\u7ec4\u5b66\u6570\u636e\u4e2d\u764c\u75c7\u5206\u7c7b\u7684\u534f\u540c\u6548\u5e94\u3002 Fadi Alharbi PDF N/A LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer Classification The application of machine learning methods to analyze changes in gene expression patterns has recently emerged as a powerful approach in cancer research, enhancing our understanding of the molecular mechanisms underpinning cancer development and progression. Combining gene expression data with other types of omics data has been reported by numerous works to improve cancer classification outcomes. Despite these advances, effectively integrating high-dimensional multi-omics data and capturing the complex relationships across different biological layers remains challenging. This paper introduces LASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep learning framework that integrates messenger RNA, microRNA, and DNA methylation data to classify 31 cancer types. Utilizing differential expression analysis with LIMMA and LASSO regression for feature selection, and leveraging Graph Attention Networks (GATs) to incorporate protein-protein interaction (PPI) networks, LASSO-MOGAT effectively captures intricate relationships within multi-omics data. Experimental validation using five-fold cross-validation demonstrates the method's precision, reliability, and capacity for providing comprehensive insights into cancer molecular mechanisms. The computation of attention coefficients for the edges in the graph by the proposed graph-attention architecture based on protein-protein interactions proved beneficial for identifying synergies in multi-omics data for cancer classification."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}